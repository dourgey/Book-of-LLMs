# ç¬¬14ç«  é™„å½•

## é™„å½•Aï¼šå¸¸ç”¨æ•°å­¦ç¬¦å·ä¸æ¦‚å¿µ

æœ¬ä¹¦æ—¨åœ¨è®²è§£å¤§æ¨¡å‹çš„åŸç†ä¸å®è·µï¼Œè™½ç„¶æˆ‘ä»¬å°½é‡é¿å…è¿‡äºå¤æ‚çš„æ•°å­¦æ¨å¯¼ï¼Œä½†ç†è§£ä¸€äº›æ ¸å¿ƒçš„æ•°å­¦ç¬¦å·å’Œæ¦‚å¿µå¯¹äºæ·±å…¥æŒæ¡ç›¸å…³ç®—æ³•ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶ã€ä¼˜åŒ–è¿‡ç¨‹ã€è¯„ä¼°æŒ‡æ ‡ç­‰ï¼‰è‡³å…³é‡è¦ã€‚æœ¬é™„å½•æ—¨åœ¨ä¸ºç†Ÿæ‚‰åŸºç¡€åº”ç”¨æ•°å­¦ï¼ˆå¾®ç§¯åˆ†ã€çº¿æ€§ä»£æ•°ã€æ¦‚ç‡ç»Ÿè®¡ï¼‰çš„è¯»è€…æä¾›ä¸€ä¸ªå¿«é€Ÿå‚è€ƒï¼Œæ¢³ç†æœ¬ä¹¦ä¸­å¯èƒ½é‡åˆ°æˆ–ä¸å¤§æ¨¡å‹é¢†åŸŸç´§å¯†ç›¸å…³çš„å¸¸ç”¨æ•°å­¦ç¬¦å·å’ŒåŸºæœ¬æ¦‚å¿µã€‚

**A.1 çº¿æ€§ä»£æ•° (Linear Algebra)**

çº¿æ€§ä»£æ•°æ˜¯æè¿°å’Œæ“ä½œå‘é‡ã€çŸ©é˜µã€å¼ é‡ç­‰æ•°æ®ç»“æ„çš„åŸºç¡€ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯å¤„ç†é«˜ç»´æ•°æ®å’Œå‚æ•°ï¼‰çš„æ•°å­¦åŸºçŸ³ã€‚

* **æ ‡é‡ (Scalar):** å•ä¸ªæ•°å€¼ã€‚é€šå¸¸ç”¨å°å†™æ–œä½“å­—æ¯è¡¨ç¤ºï¼Œå¦‚ `s`, `Î»`, `Î±`ã€‚
* **å‘é‡ (Vector):** ä¸€ç»´æ•°ç»„ï¼Œè¡¨ç¤ºç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹æˆ–æ–¹å‘ã€‚é€šå¸¸ç”¨å°å†™ç²—ä½“å­—æ¯è¡¨ç¤ºï¼Œå¦‚ **v**, **x**, **b**ã€‚ä¾‹å¦‚ï¼Œ`x = [xâ‚, xâ‚‚, ..., x_n]^T` è¡¨ç¤ºä¸€ä¸ª n ç»´åˆ—å‘é‡ï¼ˆé»˜è®¤æ˜¯åˆ—å‘é‡ï¼Œ`^T` è¡¨ç¤ºè½¬ç½®ï¼‰ã€‚
* **çŸ©é˜µ (Matrix):** äºŒç»´æ•°ç»„ã€‚é€šå¸¸ç”¨å¤§å†™ç²—ä½“å­—æ¯è¡¨ç¤ºï¼Œå¦‚ **W**, **A**, **X**ã€‚`A âˆˆ â„^(mÃ—n)` è¡¨ç¤ºä¸€ä¸ªåŒ…å« m è¡Œ n åˆ—å®æ•°çš„çŸ©é˜µã€‚`Aáµ¢â±¼` æˆ– `[A]áµ¢â±¼` è¡¨ç¤ºçŸ©é˜µ A çš„ç¬¬ i è¡Œç¬¬ j åˆ—çš„å…ƒç´ ã€‚
* **å¼ é‡ (Tensor):** å¤šç»´æ•°ç»„ï¼Œæ˜¯æ ‡é‡ã€å‘é‡ã€çŸ©é˜µçš„æ¨å¹¿ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªä¸‰é˜¶å¼ é‡å¯ä»¥è¡¨ç¤ºä¸€ä¸ªæ•°æ®ç«‹æ–¹ä½“ï¼ˆå¦‚ RGB å›¾åƒï¼‰ã€‚åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ PyTorch, TensorFlowï¼‰ä¸­ï¼Œå¼ é‡æ˜¯æœ€åŸºæœ¬çš„æ•°æ®ç»“æ„ã€‚è¡¨ç¤ºæ³•ä¸ç»Ÿä¸€ï¼Œæœ‰æ—¶ç”¨å¤§å†™ç²—ä½“ **X** æˆ–å¤§å†™ä¹¦æ³•ä½“/å¾·å¼å­—ä½“ `ğ’³`ã€‚
* **è½¬ç½® (Transpose):** äº¤æ¢çŸ©é˜µçš„è¡Œå’Œåˆ—ã€‚çŸ©é˜µ **A** çš„è½¬ç½®è®°ä½œ **A**^Tã€‚å‘é‡ **x** çš„è½¬ç½® **x**^T å°†åˆ—å‘é‡å˜ä¸ºè¡Œå‘é‡ã€‚
* **å‘é‡ç‚¹ç§¯ (Dot Product / Inner Product):** ä¸¤ä¸ªç»´åº¦ç›¸åŒçš„å‘é‡ **x** å’Œ **y** çš„ç‚¹ç§¯å®šä¹‰ä¸º `x Â· y = x^T y = Î£áµ¢ xáµ¢ yáµ¢`ã€‚ç»“æœæ˜¯ä¸€ä¸ªæ ‡é‡ã€‚ç‚¹ç§¯å¯ä»¥è¡¡é‡ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦æˆ–æŠ•å½±å…³ç³»ï¼ˆ`x Â· y = ||x|| ||y|| cos Î¸`ï¼Œå…¶ä¸­ Î¸ æ˜¯å‘é‡å¤¹è§’ï¼‰ã€‚**æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ QK^T å°±æ˜¯è®¡ç®—æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡çš„ç‚¹ç§¯ç›¸ä¼¼åº¦ã€‚**
* **çŸ©é˜µä¹˜æ³• (Matrix Multiplication):** çŸ©é˜µ **A** âˆˆ â„^(mÃ—n) å’ŒçŸ©é˜µ **B** âˆˆ â„^(nÃ—p) çš„ä¹˜ç§¯ **C** = **AB** âˆˆ â„^(mÃ—p)ï¼Œå…¶ä¸­ `Cáµ¢â±¼ = Î£â‚– Aáµ¢â‚– Bâ‚–â±¼`ã€‚æ³¨æ„çŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³äº¤æ¢å¾‹ï¼ˆ**AB â‰  BA**ï¼‰ã€‚**ç¥ç»ç½‘ç»œä¸­çš„çº¿æ€§å˜æ¢å±‚æœ¬è´¨ä¸Šå°±æ˜¯è¾“å…¥ä¹˜ä»¥æƒé‡çŸ©é˜µã€‚** åœ¨ PyTorch ä¸­å¸¸ç”¨ `@` è¿ç®—ç¬¦æˆ– `torch.matmul()`ã€‚
* **å…ƒç´ ä¹˜ç§¯ (Element-wise Product / Hadamard Product):** ä¸¤ä¸ªç»´åº¦ç›¸åŒçš„çŸ©é˜µ **A** å’Œ **B** çš„å…ƒç´ ä¹˜ç§¯ **C** = **A** âŠ™ **B**ï¼Œå…¶ä¸­ `Cáµ¢â±¼ = Aáµ¢â±¼ Báµ¢â±¼`ã€‚**ä¸€äº›æ¿€æ´»å‡½æ•°æˆ–é—¨æ§æœºåˆ¶ä¸­ä¼šç”¨åˆ°ã€‚**
* **èŒƒæ•° (Norm):** è¡¡é‡å‘é‡æˆ–çŸ©é˜µå¤§å°ï¼ˆé•¿åº¦æˆ–å¹…åº¦ï¼‰çš„å‡½æ•°ã€‚
  * **Lâ‚ èŒƒæ•°:** `||x||â‚ = Î£áµ¢ |xáµ¢|` (å‘é‡å…ƒç´ ç»å¯¹å€¼ä¹‹å’Œ)ã€‚ç”¨äº L1 æ­£åˆ™åŒ–ï¼Œå€¾å‘äºäº§ç”Ÿç¨€ç–è§£ã€‚
  * **Lâ‚‚ èŒƒæ•° (æ¬§å‡ é‡Œå¾—èŒƒæ•° Euclidean Norm):** `||x||â‚‚ = sqrt(Î£áµ¢ xáµ¢Â²) = sqrt(x^T x)` (å‘é‡å…ƒç´ çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹)ã€‚æœ€å¸¸ç”¨çš„èŒƒæ•°ï¼Œè¡¨ç¤ºå‘é‡çš„æ¬§æ°è·ç¦»ã€‚ç”¨äº L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰å’Œæ¢¯åº¦è£å‰ªã€‚é€šå¸¸ `||x||` ä¸å¸¦ä¸‹æ ‡æ—¶æŒ‡ Lâ‚‚ èŒƒæ•°ã€‚
  * **Frobenius èŒƒæ•°:** çŸ©é˜µçš„ Lâ‚‚ èŒƒæ•°ï¼Œ`||A||_F = sqrt(Î£áµ¢ Î£â±¼ Aáµ¢â±¼Â²) `ã€‚
* **å•ä½çŸ©é˜µ (Identity Matrix):** ä¸»å¯¹è§’çº¿å…ƒç´ ä¸º 1ï¼Œå…¶ä½™å…ƒç´ ä¸º 0 çš„æ–¹é˜µï¼Œè®°ä½œ **I**ã€‚æ»¡è¶³ **AI** = **IA** = **A**ã€‚
* **å¯¹è§’çŸ©é˜µ (Diagonal Matrix):** åªæœ‰ä¸»å¯¹è§’çº¿å…ƒç´ éé›¶çš„æ–¹é˜µï¼Œ`diag(v)` è¡¨ç¤ºç”±å‘é‡ **v** çš„å…ƒç´ æ„æˆçš„å¯¹è§’çŸ©é˜µã€‚
* **ç‰¹å¾å€¼ä¸ç‰¹å¾å‘é‡ (Eigenvalues & Eigenvectors):** å¯¹äºæ–¹é˜µ **A**ï¼Œå¦‚æœå­˜åœ¨æ ‡é‡ Î» å’Œéé›¶å‘é‡ **v** ä½¿å¾— **Av** = Î»**v**ï¼Œåˆ™ Î» ç§°ä¸º **A** çš„ç‰¹å¾å€¼ï¼Œ**v** ç§°ä¸ºå¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚æè¿°äº†çŸ©é˜µåœ¨çº¿æ€§å˜æ¢ä¸­ä¿æŒæ–¹å‘ä¸å˜çš„å‘é‡å’Œå¯¹åº”çš„ç¼©æ”¾å› å­ã€‚
* **å¥‡å¼‚å€¼åˆ†è§£ (Singular Value Decomposition, SVD):** ä»»ä½•å®æ•°çŸ©é˜µ **A** âˆˆ â„^(mÃ—n) å¯ä»¥åˆ†è§£ä¸º **A** = **U Î£ V**^Tã€‚å…¶ä¸­ **U** âˆˆ â„^(mÃ—m) å’Œ **V** âˆˆ â„^(nÃ—n) æ˜¯æ­£äº¤çŸ©é˜µï¼ˆåˆ—å‘é‡ç›¸äº’æ­£äº¤ä¸”é•¿åº¦ä¸º 1ï¼‰ï¼Œ**Î£** âˆˆ â„^(mÃ—n) æ˜¯å¯¹è§’çŸ©é˜µï¼ˆå¯¹è§’å…ƒç´ ç§°ä¸ºå¥‡å¼‚å€¼ï¼Œéè´Ÿä¸”é™åºæ’åˆ—ï¼‰ã€‚SVD æ­ç¤ºäº†çŸ©é˜µçš„å†…åœ¨ç»“æ„ã€‚**ä½ç§©è¿‘ä¼¼ï¼ˆä¿ç•™æœ€å¤§çš„ k ä¸ªå¥‡å¼‚å€¼ï¼‰ä¸ LoRA æŠ€æœ¯ä¸­ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼å‚æ•°æ›´æ–°çš„æ€æƒ³æœ‰å…³ã€‚**

**A.2 å¾®ç§¯åˆ† (Calculus)**

å¾®ç§¯åˆ†æä¾›äº†æè¿°å˜åŒ–ç‡å’Œç´¯ç§¯æ•ˆåº”çš„å·¥å…·ï¼Œæ˜¯ç†è§£ç¥ç»ç½‘ç»œä¼˜åŒ–ï¼ˆæ¢¯åº¦ä¸‹é™ã€åå‘ä¼ æ’­ï¼‰çš„åŸºç¡€ã€‚

* **å¯¼æ•° (Derivative):** å‡½æ•° `f(x)` åœ¨æŸç‚¹ `x` çš„ç¬æ—¶å˜åŒ–ç‡ï¼Œè®°ä½œ `f'(x)` æˆ– `df/dx`ã€‚
* **åå¯¼æ•° (Partial Derivative):** å¤šå…ƒå‡½æ•° `f(xâ‚, xâ‚‚, ..., x_n)` å¯¹å…¶ä¸­ä¸€ä¸ªå˜é‡ `xáµ¢` çš„å¯¼æ•°ï¼ŒåŒæ—¶ä¿æŒå…¶ä»–å˜é‡ä¸å˜ï¼Œè®°ä½œ `âˆ‚f/âˆ‚xáµ¢`ã€‚
* **æ¢¯åº¦ (Gradient):** å¤šå…ƒå‡½æ•° `f(x)`ï¼ˆå…¶ä¸­ **x** æ˜¯å‘é‡ï¼‰åœ¨æŸç‚¹ **x** çš„æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ï¼ŒåŒ…å«äº†å‡½æ•°å¯¹æ¯ä¸ªå˜é‡çš„åå¯¼æ•°ï¼Œè®°ä½œ `âˆ‡f(x)` æˆ– `âˆ‡â‚“f(x)`ã€‚`âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚x_n]^T`ã€‚**æ¢¯åº¦æŒ‡å‘å‡½æ•°å€¼å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚** åœ¨ä¼˜åŒ–ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘ `-âˆ‡f(x)` æ›´æ–°å‚æ•°ä»¥æœ€å°åŒ–å‡½æ•°å€¼ï¼ˆå¦‚æŸå¤±å‡½æ•°ï¼‰ã€‚
* **é“¾å¼æ³•åˆ™ (Chain Rule):** ç”¨äºè®¡ç®—å¤åˆå‡½æ•°çš„å¯¼æ•°ã€‚å¦‚æœ `z = g(y)` ä¸” `y = f(x)`ï¼Œåˆ™ `dz/dx = (dz/dy) * (dy/dx)`ã€‚å¯¹äºå¤šå˜é‡å‡½æ•°ï¼Œé“¾å¼æ³•åˆ™æ¶‰åŠé›…å¯æ¯”çŸ©é˜µçš„ä¹˜ç§¯ã€‚**åå‘ä¼ æ’­ç®—æ³•æœ¬è´¨ä¸Šå°±æ˜¯åº”ç”¨é“¾å¼æ³•åˆ™æ¥é«˜æ•ˆè®¡ç®—æŸå¤±å‡½æ•°å¯¹ç½‘ç»œä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ã€‚**
* **é›…å¯æ¯”çŸ©é˜µ (Jacobian Matrix):** ä¸€ä¸ªå‘é‡å€¼å‡½æ•° **f**: â„â¿ â†’ â„áµ çš„ä¸€é˜¶åå¯¼æ•°çŸ©é˜µã€‚`Jáµ¢â±¼ = âˆ‚fáµ¢/âˆ‚xâ±¼`ã€‚
* **æµ·æ£®çŸ©é˜µ (Hessian Matrix):** ä¸€ä¸ªæ ‡é‡å€¼å‡½æ•° `f`: â„â¿ â†’ â„ çš„äºŒé˜¶åå¯¼æ•°çŸ©é˜µã€‚`Háµ¢â±¼ = âˆ‚Â²f / (âˆ‚xáµ¢ âˆ‚xâ±¼)`ã€‚æè¿°äº†å‡½æ•°çš„å±€éƒ¨æ›²ç‡ï¼Œå¯ç”¨äºäºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼ˆå¦‚ç‰›é¡¿æ³•ï¼‰ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­å› è®¡ç®—é‡è¿‡å¤§è¾ƒå°‘ç›´æ¥ä½¿ç”¨ã€‚

**A.3 æ¦‚ç‡ä¸ç»Ÿè®¡ (Probability & Statistics)**

æ¦‚ç‡è®ºæä¾›äº†å¤„ç†ä¸ç¡®å®šæ€§çš„æ¡†æ¶ï¼Œç»Ÿè®¡å­¦åˆ™å…³æ³¨ä»æ•°æ®ä¸­å­¦ä¹ å’Œæ¨æ–­ã€‚è¿™å¯¹äºç†è§£è¯­è¨€æ¨¡å‹ï¼ˆæœ¬è´¨ä¸Šæ˜¯æ¦‚ç‡æ¨¡å‹ï¼‰ã€æŸå¤±å‡½æ•°ã€è¯„ä¼°æŒ‡æ ‡å’Œæ•°æ®åˆ†å¸ƒè‡³å…³é‡è¦ã€‚

* **éšæœºå˜é‡ (Random Variable):** å…¶å€¼æ˜¯éšæœºç°è±¡ç»“æœçš„å˜é‡ã€‚é€šå¸¸ç”¨å¤§å†™å­—æ¯è¡¨ç¤ºï¼Œå¦‚ `X`, `Y`ã€‚
* **æ¦‚ç‡åˆ†å¸ƒ (Probability Distribution):** æè¿°éšæœºå˜é‡å–ä¸åŒå€¼çš„å¯èƒ½æ€§ã€‚
  * **æ¦‚ç‡è´¨é‡å‡½æ•° (Probability Mass Function, PMF):** å¯¹äºç¦»æ•£éšæœºå˜é‡ `X`ï¼Œ`P(X=x)` è¡¨ç¤ºå–å€¼ä¸º `x` çš„æ¦‚ç‡ã€‚æ»¡è¶³ `Î£â‚“ P(X=x) = 1`ã€‚
  * **æ¦‚ç‡å¯†åº¦å‡½æ•° (Probability Density Function, PDF):** å¯¹äºè¿ç»­éšæœºå˜é‡ `X`ï¼Œ`p(x)` è¡¨ç¤ºå˜é‡åœ¨ `x` é™„è¿‘çš„æ¦‚ç‡å¯†åº¦ã€‚æ»¡è¶³ `âˆ« p(x) dx = 1`ã€‚æ¦‚ç‡é€šè¿‡ç§¯åˆ†è®¡ç®— `P(a â‰¤ X â‰¤ b) = âˆ«[a,b] p(x) dx`ã€‚
* **æœŸæœ› (Expected Value):** éšæœºå˜é‡ `X` çš„å¹³å‡å€¼ï¼Œè®°ä½œ `E[X]` æˆ– `Î¼`ã€‚
  * ç¦»æ•£: `E[X] = Î£â‚“ x P(X=x)`ã€‚
  * è¿ç»­: `E[X] = âˆ« x p(x) dx`ã€‚
* **æ–¹å·® (Variance):** è¡¡é‡éšæœºå˜é‡å–å€¼ä¸å…¶æœŸæœ›å€¼çš„åç¦»ç¨‹åº¦ï¼ˆåˆ†æ•£ç¨‹åº¦ï¼‰ï¼Œè®°ä½œ `Var(X)` æˆ– `ÏƒÂ²`ã€‚`Var(X) = E[(X - E[X])Â²] = E[XÂ²] - (E[X])Â²`ã€‚
* **æ ‡å‡†å·® (Standard Deviation):** æ–¹å·®çš„å¹³æ–¹æ ¹ï¼Œè®°ä½œ `Std(X)` æˆ– `Ïƒ`ã€‚ä¸åŸå§‹å˜é‡å…·æœ‰ç›¸åŒçš„å•ä½ã€‚
* **æ¡ä»¶æ¦‚ç‡ (Conditional Probability):** äº‹ä»¶ B å·²ç»å‘ç”Ÿçš„æ¡ä»¶ä¸‹ï¼Œäº‹ä»¶ A å‘ç”Ÿçš„æ¦‚ç‡ï¼Œè®°ä½œ `P(A|B)`ã€‚`P(A|B) = P(A âˆ© B) / P(B)` (å‡è®¾ `P(B) > 0`)ã€‚**è¯­è¨€æ¨¡å‹å°±æ˜¯è®¡ç®—ä¸‹ä¸€ä¸ªè¯åœ¨ç»™å®šå‰é¢è¯åºåˆ—æ¡ä»¶ä¸‹çš„æ¦‚ç‡ `P(táµ¢ | tâ‚...táµ¢â‚‹â‚)`ã€‚**
* **è´å¶æ–¯å®šç† (Bayes' Theorem):** æè¿°äº†åœ¨å·²çŸ¥æŸäº›æ¡ä»¶ä¸‹ï¼Œå¯¹äº‹ä»¶æ¦‚ç‡çš„ä¿¡å¿µæ›´æ–°ã€‚`P(A|B) = [P(B|A) * P(A)] / P(B)`ã€‚å…¶ä¸­ `P(A)` æ˜¯å…ˆéªŒæ¦‚ç‡ï¼Œ`P(A|B)` æ˜¯åéªŒæ¦‚ç‡ï¼Œ`P(B|A)` æ˜¯ä¼¼ç„¶åº¦ã€‚
* **å¸¸ç”¨åˆ†å¸ƒ:**
  * **æ­£æ€åˆ†å¸ƒ (Normal Distribution) / é«˜æ–¯åˆ†å¸ƒ (Gaussian Distribution):** `N(Î¼, ÏƒÂ²)`ï¼Œç”±å‡å€¼ `Î¼` å’Œæ–¹å·® `ÏƒÂ²` å®šä¹‰çš„é’Ÿå½¢æ›²çº¿ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­éå¸¸å¸¸è§ï¼ˆå¦‚æƒé‡åˆå§‹åŒ–ã€å™ªå£°å»ºæ¨¡ï¼‰ã€‚
  * **ä¼¯åŠªåˆ©åˆ†å¸ƒ (Bernoulli Distribution):** å•æ¬¡è¯•éªŒï¼Œç»“æœåªæœ‰ä¸¤ç§ï¼ˆå¦‚æˆåŠŸ/å¤±è´¥ï¼Œ1/0ï¼‰ï¼ŒæˆåŠŸæ¦‚ç‡ä¸º pã€‚
  * **åˆ†ç±»åˆ†å¸ƒ (Categorical Distribution):** å•æ¬¡è¯•éªŒï¼Œç»“æœæœ‰ K ç§å¯èƒ½ï¼Œæ¯ç§ç»“æœæœ‰ä¸€ä¸ªæ¦‚ç‡ `páµ¢`ï¼Œ`Î£ páµ¢ = 1`ã€‚**è¯­è¨€æ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„è¾“å‡ºå°±æ˜¯ä¸€ä¸ªè¯æ±‡è¡¨å¤§å°çš„åˆ†ç±»åˆ†å¸ƒã€‚**

**A.4 ä¿¡æ¯è®º (Information Theory)**

ä¿¡æ¯è®ºæä¾›äº†é‡åŒ–ä¿¡æ¯ã€ä¸ç¡®å®šæ€§å’Œåˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æ–¹æ³•ï¼Œä¸æ¦‚ç‡è®ºç´§å¯†ç›¸å…³ï¼Œå°¤å…¶åœ¨ç†è§£æŸå¤±å‡½æ•°å’Œæ¨¡å‹è¯„ä¼°ä¸­å¾ˆæœ‰ç”¨ã€‚

* **ç†µ (Entropy):** è¡¡é‡ä¸€ä¸ªéšæœºå˜é‡æˆ–æ¦‚ç‡åˆ†å¸ƒçš„ä¸ç¡®å®šæ€§ï¼ˆæˆ–åŒ…å«çš„å¹³å‡ä¿¡æ¯é‡ï¼‰ã€‚å¯¹äºç¦»æ•£éšæœºå˜é‡ `X`ï¼Œå…¶ç†µ `H(X)` å®šä¹‰ä¸ºï¼š`H(X) = - Î£â‚“ P(X=x) log P(X=x)` (é€šå¸¸ä½¿ç”¨ log base 2 å¾—åˆ°å•ä½æ¯”ç‰¹ bitsï¼Œæˆ–è‡ªç„¶å¯¹æ•° ln å¾—åˆ°å•ä½å¥ˆç‰¹ nats)ã€‚åˆ†å¸ƒè¶Šå‡åŒ€ï¼Œç†µè¶Šå¤§ï¼›åˆ†å¸ƒè¶Šé›†ä¸­ï¼Œç†µè¶Šå°ã€‚
* **äº¤å‰ç†µ (Cross-Entropy):** è¡¡é‡ä½¿ç”¨åŸºäºæ¦‚ç‡åˆ†å¸ƒ `Q` çš„ç¼–ç æ–¹å¼æ¥ç¼–ç æ¥è‡ªæ¦‚ç‡åˆ†å¸ƒ `P` çš„æ ·æœ¬æ‰€éœ€çš„å¹³å‡æ¯”ç‰¹æ•°ï¼ˆæˆ–å¥ˆç‰¹æ•°ï¼‰ã€‚å¯¹äºç¦»æ•£åˆ†å¸ƒ `P` å’Œ `Q`ï¼Œäº¤å‰ç†µ `H(P, Q)` å®šä¹‰ä¸ºï¼š`H(P, Q) = - Î£â‚“ P(x) log Q(x)`ã€‚**åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œäº¤å‰ç†µå¸¸è¢«ç”¨ä½œæŸå¤±å‡½æ•°ï¼Œå…¶ä¸­ P æ˜¯çœŸå®æ ‡ç­¾çš„åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯ One-hotï¼‰ï¼ŒQ æ˜¯æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒã€‚æœ€å°åŒ–äº¤å‰ç†µæŸå¤±æ—¨åœ¨è®©æ¨¡å‹é¢„æµ‹çš„åˆ†å¸ƒ Q å°½å¯èƒ½æ¥è¿‘çœŸå®åˆ†å¸ƒ Pã€‚**
* **KL æ•£åº¦ (Kullback-Leibler Divergence):** ä¹Ÿç§°ä¸ºç›¸å¯¹ç†µ (Relative Entropy)ã€‚è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ `P` å’Œ `Q` ä¹‹é—´çš„å·®å¼‚ï¼ˆæˆ–è€…è¯´ï¼Œç”¨åˆ†å¸ƒ Q æ¥è¿‘ä¼¼åˆ†å¸ƒ P æ—¶ä¸¢å¤±çš„ä¿¡æ¯é‡ï¼‰ã€‚`D_KL(P || Q) = Î£â‚“ P(x) log (P(x) / Q(x)) = Î£â‚“ P(x) log P(x) - Î£â‚“ P(x) log Q(x) = H(P, Q) - H(P)`ã€‚
  * KL æ•£åº¦æ€»æ˜¯éè´Ÿçš„ (`D_KL â‰¥ 0`)ï¼Œå½“ä¸”ä»…å½“ `P=Q` æ—¶ä¸º 0ã€‚
  * å®ƒ**ä¸æ˜¯å¯¹ç§°çš„** (`D_KL(P || Q) â‰  D_KL(Q || P)`)ï¼Œå› æ­¤ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„â€œè·ç¦»â€ã€‚
  * **åœ¨ RLHF ä¸­ï¼ŒKL æ•£åº¦ç”¨äºæƒ©ç½šå½“å‰ç­–ç•¥æ¨¡å‹åç¦»åŸå§‹ SFT æ¨¡å‹çš„ç¨‹åº¦ã€‚**

**A.5 å…¶ä»–å¸¸ç”¨å‡½æ•°ä¸æ¦‚å¿µ**

* **Sigmoid å‡½æ•°:** `Ïƒ(x) = 1 / (1 + exp(-x))`ã€‚å°†å®æ•°å‹ç¼©åˆ° (0, 1) åŒºé—´ï¼Œå¸¸ç”¨äºè¡¨ç¤ºæ¦‚ç‡æˆ–é—¨æ§å€¼ã€‚
* **Softmax å‡½æ•°:** `softmax(z)áµ¢ = exp(záµ¢) / Î£â±¼ exp(zâ±¼)`ã€‚å°†ä¸€ä¸ªåŒ…å« K ä¸ªå®æ•°åˆ†æ•°çš„å‘é‡ **z** è½¬æ¢ä¸ºä¸€ä¸ª K ç»´çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ‰€æœ‰å…ƒç´ åœ¨ (0, 1) ä¹‹é—´ï¼Œä¸”å’Œä¸º 1ï¼‰ã€‚**å¹¿æ³›ç”¨äºå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚ï¼ˆåŒ…æ‹¬è¯­è¨€æ¨¡å‹çš„è¯æ±‡è¡¨é¢„æµ‹ï¼‰ã€‚**
* **å¯¹æ•° (Logarithm):** `log` (é€šå¸¸æŒ‡ `logâ‚â‚€` æˆ– `logâ‚‚`), `ln` (è‡ªç„¶å¯¹æ•°ï¼Œåº•ä¸º `e`)ã€‚åœ¨ä¿¡æ¯è®ºå’ŒæŸå¤±å‡½æ•°è®¡ç®—ä¸­å¸¸ç”¨ï¼ˆå¦‚å¯¹æ•°ä¼¼ç„¶ã€äº¤å‰ç†µï¼‰ã€‚å¯¹æ•°å¯ä»¥å°†ä¹˜æ³•è½¬æ¢ä¸ºåŠ æ³•ï¼Œå¹¶å°†æ¦‚ç‡å€¼ï¼ˆé€šå¸¸å°äº1ï¼‰è½¬æ¢ä¸ºè´Ÿæ•°ï¼Œä¾¿äºè®¡ç®—å’Œä¼˜åŒ–ã€‚
* **æŒ‡æ•°å‡½æ•° (Exponential Function):** `exp(x)` æˆ– `e^x`ã€‚æ˜¯è‡ªç„¶å¯¹æ•°çš„åå‡½æ•°ã€‚
* **æœ€å¤§å€¼å‡½æ•° (max):** `max(a, b)` è¿”å› a å’Œ b ä¸­çš„è¾ƒå¤§å€¼ã€‚`argmax f(x)` è¿”å›ä½¿å‡½æ•° `f(x)` å–æœ€å¤§å€¼çš„ `x` å€¼ã€‚
* **æœ€å°å€¼å‡½æ•° (min):** `min(a, b)` è¿”å› a å’Œ b ä¸­çš„è¾ƒå°å€¼ã€‚`argmin f(x)` è¿”å›ä½¿å‡½æ•° `f(x)` å–æœ€å°å€¼çš„ `x` å€¼ã€‚
* **å¤§ O ç¬¦å· (Big O Notation):** ç”¨äºæè¿°ç®—æ³•çš„**æ¸è¿‘å¤æ‚åº¦**ï¼ˆå½“è¾“å…¥è§„æ¨¡è¶‹äºæ— ç©·å¤§æ—¶ï¼Œè®¡ç®—æ—¶é—´æˆ–ç©ºé—´èµ„æºçš„å¢é•¿ç‡ï¼‰ã€‚ä¾‹å¦‚ï¼š
  * `O(1)`: å¸¸æ•°æ—¶é—´ã€‚
  * `O(log N)`: å¯¹æ•°æ—¶é—´ã€‚
  * `O(N)`: çº¿æ€§æ—¶é—´ã€‚
  * `O(N log N)`: çº¿æ€§å¯¹æ•°æ—¶é—´ã€‚
  * `O(NÂ²)`: å¹³æ–¹æ—¶é—´ã€‚**æ ‡å‡†è‡ªæ³¨æ„åŠ›çš„å¤æ‚åº¦æ˜¯ O(NÂ² * d)ï¼Œå…¶ä¸­ N æ˜¯åºåˆ—é•¿åº¦ã€‚**
  * `O(2^N)`: æŒ‡æ•°æ—¶é—´ã€‚

ç†Ÿæ‚‰è¿™äº›ç¬¦å·å’ŒåŸºæœ¬æ¦‚å¿µï¼Œå°†æœ‰åŠ©äºä½ æ›´é¡ºç•…åœ°é˜…è¯»æœ¬ä¹¦çš„æŠ€æœ¯ç« èŠ‚ä»¥åŠæ›´å¹¿æ³›çš„æ·±åº¦å­¦ä¹ å’Œ AI æ–‡çŒ®ã€‚å¦‚æœé‡åˆ°ä¸ç†Ÿæ‚‰çš„æœ¯è¯­ï¼Œå¯ä»¥éšæ—¶å›é¡¾æœ¬é™„å½•ã€‚

---

## é™„å½•Bï¼šPythonä¸PyTorchå¿«é€Ÿå…¥é—¨

æœ¬ä¹¦çš„æ¡ˆä¾‹ä»£ç ä¸»è¦ä½¿ç”¨ Python è¯­è¨€å’Œ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚è™½ç„¶æœ¬ä¹¦å‡è®¾è¯»è€…å…·å¤‡ä¸€å®šçš„ Python ç¼–ç¨‹åŸºç¡€å’Œæ·±åº¦å­¦ä¹ çŸ¥è¯†ï¼Œä½†æœ¬é™„å½•æ—¨åœ¨ä¸ºéœ€è¦å¿«é€Ÿå›é¡¾æˆ–åˆšæ¥è§¦ PyTorch çš„è¯»è€…æä¾›ä¸€ä¸ªç®€æ˜çš„å…¥é—¨æŒ‡å—ã€‚æˆ‘ä»¬å°†é‡ç‚¹ä»‹ç» Python çš„åŸºç¡€ç‰¹æ€§ä»¥åŠ PyTorch çš„æ ¸å¿ƒæ¦‚å¿µå’Œå¸¸ç”¨æ“ä½œï¼Œè¿™äº›å¯¹äºç†è§£å’Œè¿è¡Œæœ¬ä¹¦ä¸­çš„ä»£ç è‡³å…³é‡è¦ã€‚

**B.1 Python åŸºç¡€å›é¡¾**

Python æ˜¯ä¸€ç§è§£é‡Šå‹ã€äº¤äº’å¼ã€é¢å‘å¯¹è±¡çš„é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶è¯­æ³•ç®€æ´ã€æ˜“è¯»æ€§å¼ºå’Œä¸°å¯Œçš„åº“ç”Ÿæ€è€Œé—»åï¼Œæˆä¸ºç§‘å­¦è®¡ç®—ã€æ•°æ®åˆ†æå’Œäººå·¥æ™ºèƒ½é¢†åŸŸçš„é¦–é€‰è¯­è¨€ä¹‹ä¸€ã€‚

**B.1.1 åŸºæœ¬æ•°æ®ç±»å‹ä¸ç»“æ„**

* **æ•°å­— (Numbers):** æ•´æ•° (`int`), æµ®ç‚¹æ•° (`float`), å¤æ•° (`complex`)ã€‚
  ```python
  a = 10       # int
  b = 3.14     # float
  c = 2 + 3j   # complex
  ```
* **å­—ç¬¦ä¸² (String):** ç”¨å•å¼•å· `' '` æˆ–åŒå¼•å· `" "` æ‹¬èµ·æ¥çš„å­—ç¬¦åºåˆ—ã€‚æ”¯æŒç´¢å¼•ã€åˆ‡ç‰‡ã€æ‹¼æ¥ (`+`)ã€é‡å¤ (`*`) ä»¥åŠå„ç§æ–¹æ³•ï¼ˆå¦‚ `.strip()`, `.split()`, `.join()`, `.format()`, `.lower()`, `.upper()`ï¼‰ã€‚
  ```python
  s1 = 'Hello'
  s2 = "World"
  s3 = s1 + ', ' + s2 + '!' # 'Hello, World!'
  print(s3[0])        # 'H'
  print(s3[7:12])     # 'World'
  print(f"Formatted: {s1}") # f-string (Python 3.6+)
  ```
* **åˆ—è¡¨ (List):** æœ‰åºã€å¯å˜çš„å…ƒç´ åºåˆ—ï¼Œç”¨æ–¹æ‹¬å· `[ ]` å®šä¹‰ã€‚æ”¯æŒç´¢å¼•ã€åˆ‡ç‰‡ã€æ·»åŠ  (`.append()`, `.insert()`, `+`)ã€åˆ é™¤ (`.remove()`, `del`, `.pop()`) ç­‰æ“ä½œã€‚
  ```python
  my_list = [1, "apple", 3.14, [5, 6]]
  my_list.append("new")
  print(my_list[1])   # 'apple'
  print(my_list[-1])  # 'new'
  print(my_list[1:3]) # ['apple', 3.14]
  ```
* **å…ƒç»„ (Tuple):** æœ‰åºã€**ä¸å¯å˜**çš„å…ƒç´ åºåˆ—ï¼Œç”¨åœ†æ‹¬å· `( )` å®šä¹‰ã€‚é€šå¸¸ç”¨äºè¡¨ç¤ºå›ºå®šé›†åˆæˆ–å‡½æ•°è¿”å›å¤šä¸ªå€¼ã€‚æ”¯æŒç´¢å¼•å’Œåˆ‡ç‰‡ã€‚
  ```python
  my_tuple = (1, "banana", 42)
  print(my_tuple[0]) # 1
  # my_tuple[0] = 5 # TypeError: 'tuple' object does not support item assignment
  ```
* **å­—å…¸ (Dictionary):** æ— åºï¼ˆPython 3.7+ æœ‰åºï¼‰çš„é”®å€¼å¯¹ (`key: value`) é›†åˆï¼Œç”¨èŠ±æ‹¬å· `{ }` å®šä¹‰ã€‚é”®å¿…é¡»æ˜¯å”¯ä¸€çš„ã€ä¸å¯å˜ç±»å‹ï¼ˆé€šå¸¸æ˜¯å­—ç¬¦ä¸²æˆ–æ•°å­—ï¼‰ã€‚é€šè¿‡é”®è®¿é—®å€¼ã€‚æ”¯æŒæ·»åŠ ã€ä¿®æ”¹ã€åˆ é™¤é”®å€¼å¯¹ã€‚
  ```python
  my_dict = {"name": "Alice", "age": 30, "city": "New York"}
  print(my_dict["age"]) # 30
  my_dict["email"] = "alice@example.com" # æ·»åŠ æ–°é”®å€¼å¯¹
  del my_dict["city"]
  print(my_dict.keys())   # dict_keys(['name', 'age', 'email'])
  print(my_dict.values()) # dict_values(['Alice', 30, 'alice@example.com'])
  print(my_dict.items())  # dict_items([('name', 'Alice'), ('age', 30), ('email', 'alice@example.com')])
  ```
* **é›†åˆ (Set):** æ— åºã€ä¸é‡å¤çš„å…ƒç´ é›†åˆï¼Œç”¨èŠ±æ‹¬å· `{ }` å®šä¹‰ï¼ˆç©ºé›†åˆç”¨ `set()`ï¼‰ã€‚ä¸»è¦ç”¨äºæˆå‘˜æµ‹è¯•å’Œå»é‡ï¼Œæ”¯æŒäº¤é›† (`&`)ã€å¹¶é›† (`|`)ã€å·®é›† (`-`) ç­‰æ“ä½œã€‚
  ```python
  my_set = {1, 2, 2, 3, 4, 4}
  print(my_set) # {1, 2, 3, 4}
  print(3 in my_set) # True
  ```

**B.1.2 æ§åˆ¶æµ**

* **æ¡ä»¶è¯­å¥ (if-elif-else):**
  ```python
  x = 10
  if x > 15:
      print("x is large")
  elif x > 5:
      print("x is medium")
  else:
      print("x is small")
  ```
* **å¾ªç¯è¯­å¥ (for, while):**
  * `for` å¾ªç¯ç”¨äºéå†å¯è¿­ä»£å¯¹è±¡ï¼ˆåˆ—è¡¨ã€å…ƒç»„ã€å­—ç¬¦ä¸²ã€å­—å…¸ã€é›†åˆç­‰ï¼‰ã€‚
    ```python
    numbers = [1, 2, 3]
    for num in numbers:
        print(num * num) # 1, 4, 9

    for key, value in my_dict.items():
        print(f"{key}: {value}")

    for i in range(5): # è¿­ä»£ 0 åˆ° 4
        print(i)
    ```
  * `while` å¾ªç¯åœ¨æ¡ä»¶ä¸ºçœŸæ—¶é‡å¤æ‰§è¡Œã€‚
    ```python
    count = 0
    while count < 3:
        print(f"Count is {count}")
        count += 1
    ```
  * `break` è·³å‡ºå½“å‰å¾ªç¯ï¼Œ`continue` è·³è¿‡æœ¬æ¬¡è¿­ä»£è¿›å…¥ä¸‹ä¸€æ¬¡ã€‚

**B.1.3 å‡½æ•° (Functions)**

* ä½¿ç”¨ `def` å…³é”®å­—å®šä¹‰å‡½æ•°ã€‚å¯ä»¥æœ‰å‚æ•°å’Œè¿”å›å€¼ã€‚
  ```python
  def greet(name, greeting="Hello"):
      """è¿™æ˜¯ä¸€ä¸ªæ–‡æ¡£å­—ç¬¦ä¸² (docstring)ï¼Œç”¨äºè§£é‡Šå‡½æ•°åŠŸèƒ½ã€‚"""
      message = f"{greeting}, {name}!"
      return message

  result = greet("Bob") # ä½¿ç”¨é»˜è®¤ greeting
  print(result) # 'Hello, Bob!'
  result2 = greet("Charlie", greeting="Hi")
  print(result2) # 'Hi, Charlie!'
  ```
* **Lambda å‡½æ•°:** å®šä¹‰ç®€å•çš„åŒ¿åå‡½æ•°ã€‚
  ```python
  square = lambda x: x * x
  print(square(5)) # 25
  ```

**B.1.4 ç±»ä¸å¯¹è±¡ (Classes & Objects)**

* Python æ˜¯é¢å‘å¯¹è±¡çš„è¯­è¨€ã€‚ä½¿ç”¨ `class` å…³é”®å­—å®šä¹‰ç±»ï¼Œç±»æ˜¯åˆ›å»ºå¯¹è±¡çš„è“å›¾ã€‚
* `__init__` æ–¹æ³•æ˜¯æ„é€ å‡½æ•°ï¼Œåœ¨åˆ›å»ºå¯¹è±¡æ—¶è‡ªåŠ¨è°ƒç”¨ï¼Œç”¨äºåˆå§‹åŒ–å¯¹è±¡çš„å±æ€§ã€‚
* `self` å‚æ•°ä»£è¡¨å¯¹è±¡å®ä¾‹æœ¬èº«ã€‚
  ```python
  class Dog:
      def __init__(self, name, breed):
          self.name = name
          self.breed = breed
          self.tricks = []

      def add_trick(self, trick):
          self.tricks.append(trick)

      def bark(self):
          return f"{self.name} says Woof!"

  my_dog = Dog("Buddy", "Golden Retriever")
  my_dog.add_trick("fetch")
  print(my_dog.name) # Buddy
  print(my_dog.bark()) # Buddy says Woof!
  print(my_dog.tricks) # ['fetch']
  ```

**B.1.5 æ¨¡å—ä¸åŒ… (Modules & Packages)**

* **æ¨¡å— (.py æ–‡ä»¶):** å°†ç›¸å…³çš„ä»£ç ç»„ç»‡åœ¨ä¸€ä¸ª `.py` æ–‡ä»¶ä¸­ã€‚ä½¿ç”¨ `import` è¯­å¥å¯¼å…¥æ¨¡å—çš„åŠŸèƒ½ã€‚
  ```python
  # math_utils.py
  PI = 3.14159
  def add(a, b): return a + b

  # main.py
  import math_utils
  print(math_utils.PI)
  print(math_utils.add(5, 3))

  from math_utils import PI, add # æˆ–è€…åªå¯¼å…¥ç‰¹å®šéƒ¨åˆ†
  print(PI)
  print(add(2, 4))

  import math_utils as mu # ä½¿ç”¨åˆ«å
  print(mu.add(1, 1))
  ```
* **åŒ… (Package):** åŒ…å«å¤šä¸ªæ¨¡å—çš„ç›®å½•ï¼Œé€šå¸¸åŒ…å«ä¸€ä¸ª `__init__.py` æ–‡ä»¶ï¼ˆå¯ä»¥ä¸ºç©ºï¼‰ã€‚å…è®¸æ›´å¤æ‚çš„ä»£ç ç»„ç»‡ã€‚
* **å¸¸ç”¨åº“:** Python å¼ºå¤§çš„ç”Ÿæ€ç³»ç»Ÿä½“ç°åœ¨å…¶ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ä¸Šï¼Œå¦‚ï¼š
  * **NumPy:** ç”¨äºé«˜æ€§èƒ½ç§‘å­¦è®¡ç®—å’Œæ•°ç»„æ“ä½œã€‚
  * **Pandas:** ç”¨äºæ•°æ®å¤„ç†å’Œåˆ†æï¼ˆDataFrameï¼‰ã€‚
  * **Matplotlib / Seaborn:** ç”¨äºæ•°æ®å¯è§†åŒ–ã€‚
  * **Scikit-learn:** ç”¨äºé€šç”¨æœºå™¨å­¦ä¹ ç®—æ³•å’Œå·¥å…·ã€‚
  * **Requests:** ç”¨äºå‘é€ HTTP è¯·æ±‚ã€‚
  * **NLTK / spaCy:** ç”¨äºä¼ ç»Ÿ NLP ä»»åŠ¡ã€‚
  * **Hugging Face Transformers / Datasets / Tokenizers / Evaluate / PEFT / TRL:** ï¼ˆæœ¬ä¹¦å¤§é‡ä½¿ç”¨ï¼‰ç”¨äº Transformer æ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯ã€è¯„ä¼°ã€é«˜æ•ˆå¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ ã€‚
  * **PyTorch / TensorFlow / JAX:** æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

**B.1.6 è™šæ‹Ÿç¯å¢ƒ**

* å¦‚ç¬¬ 2.3.1 èŠ‚æ‰€è¿°ï¼Œä½¿ç”¨ `conda` æˆ– `venv` åˆ›å»ºè™šæ‹Ÿç¯å¢ƒæ¥éš”ç¦»ä¸åŒé¡¹ç›®çš„ä¾èµ–ï¼Œæ˜¯ Python å¼€å‘çš„æœ€ä½³å®è·µã€‚

**B.2 PyTorch æ ¸å¿ƒæ¦‚å¿µ**

PyTorch æ˜¯ä¸€ä¸ªåŸºäº Python çš„å¼€æºæœºå™¨å­¦ä¹ åº“ï¼Œä»¥å…¶çµæ´»æ€§ï¼ˆåŠ¨æ€è®¡ç®—å›¾ï¼‰ã€æ˜“ç”¨æ€§å’Œå¼ºå¤§çš„ GPU åŠ é€Ÿèƒ½åŠ›è€Œå¤‡å—æ¬¢è¿ã€‚

**B.2.1 Tensors (å¼ é‡)**

* **æ ¸å¿ƒæ•°æ®ç»“æ„:** ç±»ä¼¼äº NumPy çš„ `ndarray`ï¼Œä½†å¯ä»¥åœ¨ GPU ä¸Šè®¡ç®—ã€‚æ˜¯ PyTorch ä¸­æ‰€æœ‰æ•°æ®ï¼ˆè¾“å…¥ã€è¾“å‡ºã€å‚æ•°ï¼‰çš„åŸºæœ¬è¡¨ç¤ºã€‚
* **åˆ›å»º Tensor:**
  ```python
  import torch

  # ä» Python åˆ—è¡¨åˆ›å»º
  data = [[1, 2], [3, 4]]
  x_data = torch.tensor(data)
  print("Tensor from list:\n", x_data)

  # åˆ›å»ºç‰¹å®šå½¢çŠ¶å’Œç±»å‹çš„ Tensor
  x_zeros = torch.zeros(2, 3) # å…¨ 0 å¼ é‡
  x_ones = torch.ones(2, 3, dtype=torch.float32) # å…¨ 1 å¼ é‡ (æŒ‡å®šç±»å‹)
  x_rand = torch.rand(2, 3) # [0, 1) å‡åŒ€åˆ†å¸ƒéšæœºæ•°
  x_randn = torch.randn(2, 3) # æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°
  print("\nZeros:\n", x_zeros)
  print("Ones (float):\n", x_ones)
  print("Rand:\n", x_rand)
  print("Randn:\n", x_randn)

  # è·å– Tensor å±æ€§
  print("\nTensor properties:")
  print("Shape:", x_data.shape)       # torch.Size([2, 2])
  print("Data type:", x_data.dtype)   # torch.int64 (é»˜è®¤)
  print("Device:", x_data.device)     # cpu (é»˜è®¤)
  ```
* **Tensor æ“ä½œ:** æ”¯æŒå„ç§æ•°å­¦è¿ç®—ã€ç´¢å¼•ã€åˆ‡ç‰‡ã€å½¢çŠ¶å˜æ¢ç­‰ï¼Œè¯­æ³•ç±»ä¼¼ NumPyã€‚
  ```python
  x = torch.tensor([[1., 2.], [3., 4.]])
  y = torch.tensor([[5., 6.], [7., 8.]])

  # å…ƒç´ åŠ æ³•
  print("x + y:\n", x + y)
  # çŸ©é˜µä¹˜æ³•
  print("x @ y:\n", x @ y) # æˆ–è€… torch.matmul(x, y)

  # ç´¢å¼•å’Œåˆ‡ç‰‡
  print("First row:", x[0])      # tensor([1., 2.])
  print("First column:", x[:, 0]) # tensor([1., 3.])
  print("Element at (1, 1):", x[1, 1]) # tensor(4.)

  # å½¢çŠ¶å˜æ¢
  z = torch.randn(2, 3)
  print("Original shape:", z.shape)
  z_reshaped = z.view(3, 2) # æ”¹å˜è§†å›¾ï¼Œå…±äº«æ•°æ®
  print("Reshaped view:", z_reshaped.shape)
  z_flattened = z.flatten() # å±•å¹³ä¸ºä¸€ç»´
  print("Flattened:", z_flattened.shape)
  z_unsqueezed = z.unsqueeze(0) # åœ¨ç¬¬ 0 ç»´å¢åŠ ä¸€ä¸ªç»´åº¦
  print("Unsqueezed:", z_unsqueezed.shape) # torch.Size([1, 2, 3])
  ```
* **ä¸ NumPy è½¬æ¢:**
  ```python
  import numpy as np

  # Torch Tensor -> NumPy Array
  a = torch.ones(5)
  b = a.numpy() # å…±äº«å†…å­˜ (CPU Tensor)
  print(type(b)) # <class 'numpy.ndarray'>

  # NumPy Array -> Torch Tensor
  c = np.zeros(5)
  d = torch.from_numpy(c) # å…±äº«å†…å­˜
  print(type(d)) # <class 'torch.Tensor'>
  ```
* **GPU åŠ é€Ÿ:**
  ```python
  # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
  if torch.cuda.is_available():
      device = torch.device("cuda")
      print(f"GPU is available: {torch.cuda.get_device_name(0)}")
  else:
      device = torch.device("cpu")
      print("GPU not available, using CPU.")

  # å°† Tensor ç§»åŠ¨åˆ° GPU
  x_gpu = torch.randn(3, 3, device=device) # åˆ›å»ºæ—¶æŒ‡å®šè®¾å¤‡
  y_cpu = torch.ones(3, 3)
  y_gpu = y_cpu.to(device) # ä½¿ç”¨ .to() æ–¹æ³•ç§»åŠ¨

  print("x_gpu device:", x_gpu.device)
  print("y_gpu device:", y_gpu.device)

  # GPU ä¸Šçš„è¿ç®—
  z_gpu = x_gpu + y_gpu
  print("Result on GPU device:", z_gpu.device)

  # å°†ç»“æœç§»å› CPU (ä¾‹å¦‚ç”¨äºæ‰“å°æˆ–ä¸ NumPy äº¤äº’)
  z_cpu = z_gpu.to("cpu")
  print("Result back on CPU device:", z_cpu.device)
  ```

  **æ³¨æ„:** åªæœ‰åœ¨åŒä¸€è®¾å¤‡ä¸Šçš„ Tensor æ‰èƒ½ç›´æ¥è¿›è¡Œè¿ç®—ã€‚

**B.2.2 Autograd: è‡ªåŠ¨å¾®åˆ†**

* **æ ¸å¿ƒåŠŸèƒ½:** PyTorch ä½¿ç”¨ `autograd` å¼•æ“è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼Œè¿™æ˜¯è®­ç»ƒç¥ç»ç½‘ç»œçš„åŸºç¡€ã€‚
* **`requires_grad`:** å¦‚æœä¸€ä¸ª Tensor éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆä¾‹å¦‚æ¨¡å‹å‚æ•°æˆ–éœ€è¦æ¢¯åº¦çš„ä¸­é—´å˜é‡ï¼‰ï¼Œéœ€è¦å°†å…¶ `requires_grad` å±æ€§è®¾ç½®ä¸º `True`ã€‚
* **è®¡ç®—å›¾:** PyTorch ä¼šåŠ¨æ€æ„å»ºè®¡ç®—å›¾ï¼Œè®°å½•æ‰€æœ‰æ¶‰åŠ `requires_grad=True` çš„ Tensor çš„æ“ä½œã€‚
* **`.backward()`:** å½“åœ¨ä¸€ä¸ªæ ‡é‡å€¼ï¼ˆé€šå¸¸æ˜¯æŸå¤± `loss`ï¼‰ä¸Šè°ƒç”¨ `.backward()` æ—¶ï¼ŒPyTorch ä¼šæ²¿ç€è®¡ç®—å›¾åå‘ä¼ æ’­ï¼Œè®¡ç®—å›¾ä¸­æ‰€æœ‰ `requires_grad=True` çš„å¶å­èŠ‚ç‚¹ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹å‚æ•°ï¼‰ç›¸å¯¹äºè¯¥æ ‡é‡å€¼çš„æ¢¯åº¦ã€‚
* **`.grad`:** è®¡ç®—å‡ºçš„æ¢¯åº¦ä¼šç´¯ç§¯å­˜å‚¨åœ¨å¯¹åº” Tensor çš„ `.grad` å±æ€§ä¸­ã€‚
* **`torch.no_grad()`:** åœ¨è¿›è¡Œæ¨ç†æˆ–ä¸éœ€è¦è®¡ç®—æ¢¯åº¦çš„ä»£ç å—ï¼ˆå¦‚è¯„ä¼°æ¨¡å‹ï¼‰æ—¶ï¼Œä½¿ç”¨ `with torch.no_grad():` ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯ä»¥ä¸´æ—¶ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—ã€‚

```python
# åˆ›å»ºéœ€è¦æ¢¯åº¦çš„ Tensor (å¶å­èŠ‚ç‚¹)
w = torch.randn(5, 3, requires_grad=True)
x = torch.randn(1, 5) # è¾“å…¥é€šå¸¸ä¸éœ€è¦æ¢¯åº¦
b = torch.randn(1, 3, requires_grad=True)

# å‰å‘è®¡ç®—
y_pred = x @ w + b # y_pred ä¾èµ–äº w å’Œ bï¼Œå…¶ requires_grad=True
loss = torch.mean((y_pred - torch.ones(1, 3))**2) # è®¡ç®— MSE æŸå¤± (æ ‡é‡)

print("Loss:", loss.item()) # .item() è·å–æ ‡é‡å€¼
print("w.grad before backward:", w.grad) # None
print("b.grad before backward:", b.grad) # None

# åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
loss.backward()

# æŸ¥çœ‹æ¢¯åº¦ (dL/dw, dL/db)
print("\nw.grad after backward:\n", w.grad)
print("b.grad after backward:\n", b.grad)
# æ³¨æ„: x.grad ä»ç„¶æ˜¯ Noneï¼Œå› ä¸ºå®ƒ requires_grad=False

# --- æ¢¯åº¦æ¸…é›¶ ---
# æ¢¯åº¦æ˜¯ç´¯åŠ çš„ï¼Œåœ¨æ¯æ¬¡ä¼˜åŒ–å™¨æ›´æ–°å‰éœ€è¦æ¸…é›¶
w.grad.zero_()
b.grad.zero_()
print("\nAfter zero_grad():")
print("w.grad:", w.grad)
print("b.grad:", b.grad)

# --- ç¦ç”¨æ¢¯åº¦è®¡ç®— ---
print("\nRequires grad for w:", w.requires_grad) # True
with torch.no_grad():
    y_no_grad = x @ w + b
    print("Inside no_grad, y requires_grad:", y_no_grad.requires_grad) # False
print("Outside no_grad, y requires_grad:", y_pred.requires_grad) # True (ä¹‹å‰çš„ y_pred)
```

**B.2.3 `nn.Module`: æ„å»ºç¥ç»ç½‘ç»œ**

* **åŸºç±»:** æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå±‚ã€æ•´ä¸ªæ¨¡å‹ï¼‰éƒ½åº”ç»§æ‰¿ `torch.nn.Module`ã€‚
* **`__init__()`:** æ„é€ å‡½æ•°ï¼Œç”¨äºå®šä¹‰æ¨¡å‹åŒ…å«çš„å±‚ï¼ˆå¦‚ `nn.Linear`, `nn.Conv2d`, `nn.LSTM`, `nn.TransformerEncoderLayer`, `nn.LayerNorm`, `nn.Dropout`, `nn.Embedding` ç­‰ï¼‰ã€‚è¿™äº›å±‚æœ¬èº«ä¹Ÿæ˜¯ `nn.Module` çš„å®ä¾‹ï¼Œå®ƒä»¬ä¼šè‡ªåŠ¨æ³¨å†Œå…¶å†…éƒ¨çš„å¯å­¦ä¹ å‚æ•°ã€‚
* **`forward()`:** å®šä¹‰æ•°æ®åœ¨æ¨¡å‹ä¸­çš„å‰å‘ä¼ æ’­é€»è¾‘ã€‚
* **å‚æ•°ç®¡ç†:** `nn.Module` ä¼šè‡ªåŠ¨è·Ÿè¸ªå…¶åŒ…å«çš„æ‰€æœ‰å­æ¨¡å—çš„å‚æ•°ã€‚å¯ä»¥é€šè¿‡ `model.parameters()` è·å–æ¨¡å‹æ‰€æœ‰å¯å­¦ä¹ å‚æ•°çš„è¿­ä»£å™¨ï¼Œä¼ é€’ç»™ä¼˜åŒ–å™¨ã€‚
* **çŠ¶æ€å­—å…¸:** `model.state_dict()` è¿”å›ä¸€ä¸ªåŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°å’ŒæŒä¹…åŒ–ç¼“å†²åŒºï¼ˆBuffersï¼‰çŠ¶æ€çš„å­—å…¸ï¼Œç”¨äºä¿å­˜å’ŒåŠ è½½æ¨¡å‹ã€‚

```python
import torch.nn as nn
import torch.nn.functional as F # å‡½æ•°å¼æ¥å£

class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleMLP, self).__init__() # å¿…é¡»è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        # å®šä¹‰å±‚
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.activation = nn.ReLU() # ä½¿ç”¨ Module å½¢å¼çš„æ¿€æ´»
        self.layer2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­
        x = self.layer1(x)
        x = self.activation(x)
        # ä¹Ÿå¯ä»¥ä½¿ç”¨ F.relu(x) å‡½æ•°å¼æ¥å£
        x = self.layer2(x)
        return x

# å®ä¾‹åŒ–æ¨¡å‹
input_dim = 10
hidden_dim = 20
output_dim = 5
model = SimpleMLP(input_dim, hidden_dim, output_dim)
print("Model Architecture:\n", model)

# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print("\nModel Parameters:")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name, param.shape)

# æ¨¡æ‹Ÿè¾“å…¥å’Œè¾“å‡º
dummy_input = torch.randn(4, input_dim) # batch_size=4
output = model(dummy_input)
print("\nOutput shape:", output.shape) # torch.Size([4, 5])

# ä¿å­˜å’ŒåŠ è½½æ¨¡å‹çŠ¶æ€
# torch.save(model.state_dict(), 'mlp_model.pth')
# loaded_model = SimpleMLP(input_dim, hidden_dim, output_dim)
# loaded_model.load_state_dict(torch.load('mlp_model.pth'))
# loaded_model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ (ä¼šå…³é—­ dropout, batchnorm ç­‰)
```

**B.2.4 `torch.optim`: ä¼˜åŒ–å™¨**

* åŒ…å«å„ç§ä¼˜åŒ–ç®—æ³•çš„å®ç°ï¼Œç”¨äºæ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
* **å¸¸ç”¨ä¼˜åŒ–å™¨:** `optim.SGD`, `optim.Adam`, `optim.AdamW` (æ¨èç”¨äº Transformer)ã€‚
* **ä½¿ç”¨æµç¨‹:**
  1. åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹ï¼Œä¼ å…¥æ¨¡å‹å‚æ•°å’Œå­¦ä¹ ç‡ç­‰è¶…å‚æ•°ã€‚
  2. åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼š
     * è°ƒç”¨ `optimizer.zero_grad()` æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦ã€‚
     * è®¡ç®—æŸå¤± `loss`ã€‚
     * è°ƒç”¨ `loss.backward()` è®¡ç®—æ¢¯åº¦ã€‚
     * è°ƒç”¨ `optimizer.step()` æ‰§è¡Œå‚æ•°æ›´æ–°ã€‚

```python
import torch.optim as optim

# åˆ›å»ºä¼˜åŒ–å™¨ (ä½¿ç”¨ AdamW)
optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)

# --- æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤ ---
# (å‡è®¾å·²ç»æœ‰äº† output å’Œ target_labels)
# target_labels = torch.randn(4, output_dim)
# criterion = nn.MSELoss() # å‡è®¾ä½¿ç”¨ MSE æŸå¤±

# optimizer.zero_grad()       # æ¸…é›¶æ¢¯åº¦
# loss = criterion(output, target_labels)
# print("Calculated loss:", loss.item())
# loss.backward()           # è®¡ç®—æ¢¯åº¦
# optimizer.step()            # æ›´æ–°å‚æ•°

# print("Parameters updated (example - layer1 weight bias):")
# print(model.layer1.bias) # å¯ä»¥çœ‹åˆ° bias å€¼å‘ç”Ÿäº†å˜åŒ–
```

**B.2.5 æ•°æ®åŠ è½½ (`Dataset` & `DataLoader`)**

* ä¸ºäº†é«˜æ•ˆåœ°åŠ è½½å’Œå¤„ç†æ•°æ®ï¼ŒPyTorch æä¾›äº† `Dataset` å’Œ `DataLoader` ç±»ã€‚
* **`torch.utils.data.Dataset`:** æŠ½è±¡ç±»ï¼Œéœ€è¦ç»§æ‰¿å¹¶å®ç°ä¸¤ä¸ªæ–¹æ³•ï¼š
  * `__len__()`: è¿”å›æ•°æ®é›†çš„å¤§å°ã€‚
  * `__getitem__(idx)`: æ ¹æ®ç´¢å¼• `idx` è¿”å›ä¸€æ¡æ•°æ®æ ·æœ¬ï¼ˆé€šå¸¸æ˜¯åŒ…å«è¾“å…¥å’Œæ ‡ç­¾çš„å…ƒç»„æˆ–å­—å…¸ï¼‰ã€‚
* **`torch.utils.data.DataLoader`:** åŒ…è£… `Dataset`ï¼Œæä¾›æ–¹ä¾¿çš„æ•°æ®æ‰¹å¤„ç†ã€æ‰“ä¹± (shuffle) å’Œå¹¶è¡ŒåŠ è½½ (multiprocessing) åŠŸèƒ½ã€‚

```python
from torch.utils.data import Dataset, DataLoader

# è‡ªå®šä¹‰æ•°æ®é›†ç¤ºä¾‹
class MyDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features # å‡è®¾æ˜¯ Tensor æˆ– NumPy æ•°ç»„
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        sample = {"feature": self.features[idx], "label": self.labels[idx]}
        return sample

# åˆ›å»ºå‡æ•°æ®
dummy_features = torch.randn(100, input_dim)
dummy_labels = torch.randint(0, output_dim, (100,))

# å®ä¾‹åŒ– Dataset
my_dataset = MyDataset(dummy_features, dummy_labels)
print("Dataset size:", len(my_dataset))
print("First sample:", my_dataset[0])

# å®ä¾‹åŒ– DataLoader
batch_sz = 16
my_dataloader = DataLoader(my_dataset, batch_size=batch_sz, shuffle=True, num_workers=0)
# num_workers > 0 å¯ä»¥ä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®ï¼ŒåŠ é€Ÿ IO (Windows ä¸‹å¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†)

# éå† DataLoader
print("\nIterating through DataLoader:")
for i, batch in enumerate(my_dataloader):
    print(f"Batch {i+1}:")
    print("Feature shape:", batch["feature"].shape) # torch.Size([16, 10])
    print("Label shape:", batch["label"].shape)   # torch.Size([16])
    if i >= 1: # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªæ‰¹æ¬¡
        break
```

**B.3 å°ç»“**

æœ¬é™„å½•å¿«é€Ÿå›é¡¾äº† Python çš„åŸºç¡€è¯­æ³•å’Œæ•°æ®ç»“æ„ï¼Œå¹¶é‡ç‚¹ä»‹ç»äº† PyTorch çš„æ ¸å¿ƒç»„ä»¶ï¼š

* **Tensor:** åŸºæœ¬æ•°æ®ç»“æ„ï¼Œæ”¯æŒ GPU åŠ é€Ÿã€‚
* **Autograd:** è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼Œå®ç°åå‘ä¼ æ’­ã€‚
* **`nn.Module`:** æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹çš„åŸºç¡€ã€‚
* **`torch.optim`:** æä¾›ä¼˜åŒ–ç®—æ³•ç”¨äºæ›´æ–°å‚æ•°ã€‚
* **`Dataset` & `DataLoader`:** é«˜æ•ˆåŠ è½½å’Œæ‰¹å¤„ç†æ•°æ®ã€‚

æŒæ¡è¿™äº›åŸºç¡€çŸ¥è¯†ï¼Œå°†æœ‰åŠ©äºä½ ç†è§£æœ¬ä¹¦ä¸­çš„ PyTorch ä»£ç ç¤ºä¾‹ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥æ·±å…¥å­¦ä¹ å’Œå®è·µæ‰“ä¸‹åŸºç¡€ã€‚å¯¹äºæ›´è¯¦ç»†çš„ PyTorch ç”¨æ³•ï¼Œå»ºè®®æŸ¥é˜… PyTorch å®˜æ–¹æ–‡æ¡£å’Œæ•™ç¨‹ã€‚

---

## é™„å½•Cï¼šHugging Face ç”Ÿæ€ç³»ç»Ÿç®€ä»‹

åœ¨æœ¬ä¹¦çš„è®¸å¤šä»£ç ç¤ºä¾‹å’Œå®æˆ˜é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å¤§é‡ä½¿ç”¨äº† Hugging Face å…¬å¸å¼€å‘çš„ä¸€ç³»åˆ—å¼€æºåº“ã€‚Hugging Face å·²ç»æˆä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¹ƒè‡³æ›´å¹¿æ³› AI é¢†åŸŸäº‹å®ä¸Šçš„æ ‡å‡†å¹³å°å’Œå·¥å…·é›†ä¹‹ä¸€ã€‚å…¶å›´ç»• Transformer æ¨¡å‹æ„å»ºçš„ç”Ÿæ€ç³»ç»Ÿæå¤§åœ°é™ä½äº†ç ”ç©¶äººå‘˜å’Œå¼€å‘è€…ä½¿ç”¨ã€è®­ç»ƒã€åˆ†äº«å’Œéƒ¨ç½² SOTA æ¨¡å‹çš„é—¨æ§›ã€‚

æœ¬é™„å½•æ—¨åœ¨ç®€è¦ä»‹ç» Hugging Face ç”Ÿæ€ç³»ç»Ÿä¸­çš„å‡ ä¸ªæ ¸å¿ƒåº“ï¼Œç‰¹åˆ«æ˜¯æˆ‘ä»¬åœ¨æœ¬ä¹¦ä¸­ç»å¸¸é‡åˆ°çš„ `transformers`, `datasets`, `tokenizers`, `evaluate`, `peft`, å’Œ `trl`ã€‚äº†è§£è¿™äº›åº“çš„åŸºæœ¬åŠŸèƒ½å’Œç›¸äº’å…³ç³»ï¼Œå°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æœ¬ä¹¦ä»£ç ï¼Œå¹¶èƒ½ç‹¬ç«‹åœ°åˆ©ç”¨è¿™ä¸ªå¼ºå¤§çš„ç”Ÿæ€ç³»ç»Ÿè¿›è¡Œä½ è‡ªå·±çš„å¤§æ¨¡å‹æ¢ç´¢å’Œå¼€å‘ã€‚

**C.1 Hugging Face Hub: æ¨¡å‹ã€æ•°æ®é›†ä¸ Demo çš„ä¸­å¿ƒ**

Hugging Face Hub (huggingface.co) æ˜¯æ•´ä¸ªç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒã€‚å®ƒæ˜¯ä¸€ä¸ª**åœ¨çº¿å¹³å°**ï¼Œæ±‡é›†äº†ï¼š

* **æ•°ä»¥ä¸‡è®¡çš„é¢„è®­ç»ƒæ¨¡å‹ (Models):** æ¶µç›– NLPã€è®¡ç®—æœºè§†è§‰ã€éŸ³é¢‘ã€å¤šæ¨¡æ€ç­‰å¤šä¸ªé¢†åŸŸã€‚ä½ å¯ä»¥è½»æ¾æœç´¢ã€æµè§ˆå’Œä¸‹è½½è¿™äº›æ¨¡å‹ï¼ˆåŒ…æ‹¬å®ƒä»¬çš„æƒé‡ã€é…ç½®æ–‡ä»¶å’Œåˆ†è¯å™¨ï¼‰ã€‚ä¸»æµçš„å¤§æ¨¡å‹ï¼ˆå¦‚ BERT, GPT-2, T5, Llama, Mistral, Stable Diffusion, CLIP ç­‰ï¼‰åŠå…¶å„ç§å¾®è°ƒç‰ˆæœ¬éƒ½å¯ä»¥åœ¨ Hub ä¸Šæ‰¾åˆ°ã€‚æ¨¡å‹ç”± Hugging Face å®˜æ–¹ã€ç ”ç©¶æœºæ„ã€å…¬å¸å’Œç¤¾åŒºç”¨æˆ·ä¸Šä¼ å’Œç»´æŠ¤ã€‚
* **æµ·é‡çš„å…¬å…±æ•°æ®é›† (Datasets):** æä¾›äº†æ•°åƒä¸ªç”¨äºè®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹çš„æ•°æ®é›†ï¼Œæ¶µç›–å„ç§ä»»åŠ¡å’Œè¯­è¨€ã€‚`datasets` åº“å¯ä»¥æ–¹ä¾¿åœ°åŠ è½½å’Œå¤„ç†è¿™äº›æ•°æ®é›†ã€‚
* **äº¤äº’å¼æ¼”ç¤º (Spaces):** ç”¨æˆ·å¯ä»¥ä½¿ç”¨ Gradio æˆ– Streamlit å¿«é€Ÿæ„å»ºå’Œéƒ¨ç½²æ¨¡å‹çš„åœ¨çº¿æ¼”ç¤º (Demo)ï¼Œæ–¹ä¾¿åˆ†äº«å’Œå±•ç¤ºæ¨¡å‹æ•ˆæœã€‚
* **æ–‡æ¡£ä¸æ•™ç¨‹:** æä¾›è¯¦ç»†çš„åº“æ–‡æ¡£ã€æ•™ç¨‹å’Œæ¦‚å¿µè§£é‡Šã€‚
* **ç¤¾åŒºåŠŸèƒ½:** ç”¨æˆ·å¯ä»¥åˆ›å»ºç»„ç»‡ã€è®¨è®ºæ¨¡å‹ã€åˆ†äº«ä»£ç ã€‚

Hub é€šè¿‡æä¾›ç»Ÿä¸€çš„æ¥å£å’Œä¾¿æ·çš„å·¥å…·ï¼Œæå¤§åœ°ä¿ƒè¿›äº† AI æ¨¡å‹å’Œèµ„æºçš„å…±äº«ä¸åä½œã€‚

**C.2 `transformers` åº“: SOTA æ¨¡å‹çš„æ ¸å¿ƒ**

`transformers` åº“æ˜¯ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„åŸºçŸ³ï¼Œæä¾›äº†å¯¹å¤§é‡åŸºäº Transformer çš„é¢„è®­ç»ƒæ¨¡å‹çš„æ ‡å‡†åŒ–è®¿é—®æ¥å£ã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **æ¨¡å‹åŠ è½½:** é€šè¿‡ç®€å•çš„ `AutoModel.from_pretrained("model_name")` æˆ–ç‰¹å®šæ¨¡å‹ç±»ï¼ˆå¦‚ `BertModel.from_pretrained()`, `GPT2LMHeadModel.from_pretrained()`ï¼‰å³å¯åŠ è½½ Hub ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡å’Œé…ç½®ã€‚
  * **åˆ†è¯å™¨åŠ è½½:** ç±»ä¼¼åœ°ï¼Œ`AutoTokenizer.from_pretrained("model_name")` å¯ä»¥åŠ è½½ä¸æ¨¡å‹åŒ¹é…çš„åˆ†è¯å™¨ã€‚
  * **æ¨¡å‹é…ç½®:** `AutoConfig.from_pretrained("model_name")` åŠ è½½æ¨¡å‹çš„é…ç½®ä¿¡æ¯ï¼ˆå¦‚å±‚æ•°ã€éšè—ç»´åº¦ã€å¤´æ•°ç­‰ï¼‰ã€‚
  * **æ ‡å‡†åŒ–æ¨¡å‹æ¥å£:** æä¾›äº†ç»Ÿä¸€çš„ `forward` æ–¹æ³•æ¥å£å’Œæ ‡å‡†çš„è¾“å…¥/è¾“å‡ºæ ¼å¼ï¼ˆé€šå¸¸æ˜¯ç‰¹å®šçš„ `Output` å¯¹è±¡ï¼ŒåŒ…å« `loss`, `logits`, `hidden_states`, `attentions` ç­‰ï¼‰ã€‚
  * **Pipeline API:** æä¾›äº†ä¸€ä¸ªéå¸¸æ˜“ç”¨çš„é«˜çº§æ¥å£ (`pipeline()`)ï¼Œç”¨äºå¿«é€Ÿæ‰§è¡Œå¸¸è§çš„ NLP ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€ç¿»è¯‘ã€æ‘˜è¦ã€æ–‡æœ¬ç”Ÿæˆã€ç‰¹å¾æå–ç­‰ï¼‰ï¼Œå°è£…äº†ä»æ•°æ®é¢„å¤„ç†åˆ°æ¨¡å‹æ¨ç†å†åˆ°åå¤„ç†çš„æ•´ä¸ªæµç¨‹ï¼ˆæˆ‘ä»¬åœ¨ç¬¬10ç« çš„æ¡ˆä¾‹ä¸­å¤šæ¬¡ä½¿ç”¨ï¼‰ã€‚
  * **Trainer API:** æä¾›äº†ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„è®­ç»ƒå™¨ç±» (`Trainer`)ï¼Œç”¨äºç®€åŒ– PyTorch æ¨¡å‹çš„è®­ç»ƒå’Œå¾®è°ƒè¿‡ç¨‹ï¼ˆæˆ‘ä»¬åœ¨ç¬¬6ç« å’Œç¬¬11ç« çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨ï¼‰ã€‚å®ƒé›†æˆäº†è®­ç»ƒå¾ªç¯ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•ã€æ£€æŸ¥ç‚¹ä¿å­˜ã€æ··åˆç²¾åº¦è®­ç»ƒã€åˆ†å¸ƒå¼è®­ç»ƒï¼ˆä¸ `accelerate` åº“é…åˆï¼‰ã€è¶…å‚æ•°æœç´¢ç­‰åŠŸèƒ½ã€‚
  * **æ¨¡å‹ä¿å­˜:** `model.save_pretrained("save_directory")` å’Œ `tokenizer.save_pretrained("save_directory")` å¯ä»¥å°†æ¨¡å‹å’Œåˆ†è¯å™¨ä¿å­˜åˆ°æœ¬åœ°ï¼Œæ–¹ä¾¿åç»­åŠ è½½æˆ–åˆ†äº«ã€‚
* **æ”¯æŒçš„æ¶æ„:** æ”¯æŒå‡ ä¹æ‰€æœ‰ä¸»æµçš„ Transformer æ¶æ„å˜ç§ï¼ˆEncoder-Only, Decoder-Only, Encoder-Decoderï¼‰ä»¥åŠä¸€äº›é Transformer æ¨¡å‹ã€‚
* **æ¡†æ¶å…¼å®¹æ€§:** æ”¯æŒ PyTorch, TensorFlow, å’Œ JAX ä¸‰å¤§ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

`transformers` åº“æå¤§åœ°ç®€åŒ–äº†ä½¿ç”¨å’Œå¾®è°ƒå¤æ‚ Transformer æ¨¡å‹çš„è¿‡ç¨‹ï¼Œè®©å¼€å‘è€…å¯ä»¥ä¸“æ³¨äºä»»åŠ¡æœ¬èº«ï¼Œè€Œä¸æ˜¯åº•å±‚çš„æ¨¡å‹å®ç°ç»†èŠ‚ã€‚

**C.3 `datasets` åº“: æ•°æ®é›†è®¿é—®ä¸å¤„ç†**

å¤„ç†å’Œå‡†å¤‡æ•°æ®æ˜¯æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„å…³é”®ç¯èŠ‚ã€‚`datasets` åº“æ—¨åœ¨ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **ä¸€é”®åŠ è½½æ•°æ®é›†:** é€šè¿‡ `load_dataset("dataset_name", "subset_name")` å¯ä»¥è½»æ¾ä¸‹è½½å’ŒåŠ è½½ Hugging Face Hub ä¸Šçš„æ•°åƒä¸ªæ•°æ®é›†ã€‚æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼ˆCSV, JSON, Parquet, æ–‡æœ¬ç­‰ï¼‰ã€‚
  * **é«˜æ•ˆçš„æ•°æ®å¤„ç†:** ä½¿ç”¨ Apache Arrow ä½œä¸ºåç«¯ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜æ˜ å°„ (Memory Mapping) å’Œé›¶æ‹·è´ (Zero-copy) æ•°æ®è¯»å–ï¼Œå³ä½¿å¯¹äºéå¸¸å¤§çš„æ•°æ®é›†ä¹Ÿèƒ½å¿«é€Ÿè®¿é—®å’Œå¤„ç†ã€‚
  * **æ•°æ®è½¬æ¢ API:** æä¾›äº†ç±»ä¼¼ Pandas DataFrame çš„ APIï¼ˆå¦‚ `.map()`, `.filter()`, `.shuffle()`, `.select()`, `.train_test_split()`ï¼‰ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€æ¸…æ´—å’Œè½¬æ¢ï¼Œå¹¶ä¸”æ”¯æŒå¤šè¿›ç¨‹åŠ é€Ÿã€‚æˆ‘ä»¬åœ¨ç¬¬6ç« å’Œç¬¬11ç« çš„æ•°æ®é¢„å¤„ç†ä¸­å¹¿æ³›ä½¿ç”¨äº† `.map()` æ–¹æ³•ã€‚
  * **æµå¼å¤„ç† (Streaming):** å¯¹äºæ— æ³•å®Œå…¨åŠ è½½åˆ°å†…å­˜çš„è¶…å¤§æ•°æ®é›†ï¼Œæ”¯æŒæµå¼å¤„ç†æ¨¡å¼ï¼Œå¯ä»¥é€æ‰¹åŠ è½½å’Œå¤„ç†æ•°æ®ï¼Œæ— éœ€ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ã€‚
  * **ä¸å…¶ä»–åº“é›†æˆ:** å¯ä»¥æ–¹ä¾¿åœ°å°† `Dataset` å¯¹è±¡è½¬æ¢ä¸º PyTorch `DataLoader`, TensorFlow `tf.data.Dataset` æˆ– Pandas DataFrameã€‚
  * **æ•°æ®é›†æŒ‡æ ‡:** ä¸ `evaluate` åº“é›†æˆï¼Œå¯ä»¥æ–¹ä¾¿åœ°åŠ è½½å’Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚

`datasets` åº“ä½¿å¾—è®¿é—®å’Œå¤„ç†å„ç§è§„æ¨¡çš„æ•°æ®é›†å˜å¾—æ ‡å‡†åŒ–å’Œé«˜æ•ˆã€‚

**C.4 `tokenizers` åº“: é«˜æ•ˆåˆ†è¯**

åˆ†è¯æ˜¯å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„ Token ID åºåˆ—çš„å…³é”®æ­¥éª¤ã€‚`tokenizers` åº“æä¾›äº†ä¸»æµå­è¯åˆ†è¯ç®—æ³•ï¼ˆBPE, WordPiece, Unigram/SentencePieceï¼‰çš„é«˜æ•ˆ Rust å®ç°ï¼Œå¹¶æä¾›äº† Python ç»‘å®šã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **è®­ç»ƒåˆ†è¯å™¨:** å¯ä»¥ä»å¤´å¼€å§‹åœ¨ä½ çš„è¯­æ–™åº“ä¸Šè®­ç»ƒè‡ªå®šä¹‰çš„åˆ†è¯å™¨ï¼ˆè§ 2.2.2 æ¡ˆä¾‹ï¼‰ã€‚
  * **åŠ è½½/ä¿å­˜åˆ†è¯å™¨:** å¯ä»¥åŠ è½½ `transformers` åº“ä½¿ç”¨çš„é¢„è®­ç»ƒåˆ†è¯å™¨ï¼ˆé€šå¸¸æ˜¯ä»¥ `tokenizer.json` æ–‡ä»¶çš„å½¢å¼ï¼‰ï¼Œæˆ–ä¿å­˜è‡ªå·±è®­ç»ƒçš„åˆ†è¯å™¨ã€‚
  * **å¿«é€Ÿç¼–ç /è§£ç :** æä¾›éå¸¸å¿«é€Ÿçš„æ–‡æœ¬ç¼–ç ï¼ˆæ–‡æœ¬ -> Token IDï¼‰å’Œè§£ç ï¼ˆToken ID -> æ–‡æœ¬ï¼‰åŠŸèƒ½ã€‚
  * **ä¸°å¯Œçš„é¢„å¤„ç†/åå¤„ç†:** æ”¯æŒé…ç½®é¢„åˆ†è¯å™¨ï¼ˆå¦‚ä½•åˆæ­¥åˆ‡åˆ†æ–‡æœ¬ï¼‰ã€æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚å°å†™è½¬æ¢ã€NFC è§„èŒƒåŒ–ï¼‰ã€åå¤„ç†å™¨ï¼ˆå¦‚è‡ªåŠ¨æ·»åŠ  `[CLS]`, `[SEP]` ç­‰ç‰¹æ®Š Tokenï¼‰ã€‚
* **ä¸ `transformers` é›†æˆ:** `transformers` åº“ä¸­çš„ `AutoTokenizer` å®é™…ä¸Šåœ¨åº•å±‚å¤§é‡ä½¿ç”¨äº† `tokenizers` åº“ï¼ˆå¯¹äºæ‰€è°“çš„ "Fast Tokenizer" å®ç°ï¼‰ã€‚

è™½ç„¶æˆ‘ä»¬é€šå¸¸é€šè¿‡ `transformers` åº“é—´æ¥ä½¿ç”¨åˆ†è¯å™¨ï¼Œä½†äº†è§£ `tokenizers` åº“æœ¬èº«æœ‰åŠ©äºæˆ‘ä»¬ç†è§£åˆ†è¯è¿‡ç¨‹çš„ç»†èŠ‚ï¼Œå¹¶åœ¨éœ€è¦æ—¶è¿›è¡Œæ›´åº•å±‚çš„å®šåˆ¶ã€‚

**C.5 `evaluate` åº“: è¯„ä¼°æŒ‡æ ‡è®¡ç®—**

è¯„ä¼°æ¨¡å‹æ€§èƒ½éœ€è¦å¯é çš„æŒ‡æ ‡è®¡ç®—ã€‚`evaluate` åº“æ—¨åœ¨æä¾›ä¸€ä¸ªç»Ÿä¸€ã€ç®€æ´çš„æ–¹å¼æ¥åŠ è½½ã€è®¡ç®—å’Œæ¯”è¾ƒå„ç§æœºå™¨å­¦ä¹ è¯„ä¼°æŒ‡æ ‡ã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **åŠ è½½æŒ‡æ ‡:** é€šè¿‡ `evaluate.load("metric_name", "subset_name")` å¯ä»¥åŠ è½½ Hub ä¸Šçš„å„ç§è¯„ä¼°æŒ‡æ ‡å®ç°ï¼ˆå¦‚ `accuracy`, `f1`, `precision`, `recall`, `bleu`, `rouge`, `perplexity`, ä»¥åŠ GLUE, SQuAD ç­‰åŸºå‡†çš„ç»„åˆæŒ‡æ ‡ï¼‰ã€‚
  * **è®¡ç®—æŒ‡æ ‡:** åŠ è½½åçš„ `metric` å¯¹è±¡æä¾› `.compute(predictions=..., references=...)` æ–¹æ³•æ¥è®¡ç®—æŒ‡æ ‡å€¼ã€‚
  * **èšåˆç»“æœ:** å¯¹äºåˆ†å¸ƒå¼è¯„ä¼°ï¼Œæä¾›äº† `.add_batch()` å’Œ `.compute()` çš„æ¥å£æ¥ç´¯ç§¯è®¡ç®—ã€‚
  * **æ¯”è¾ƒæ¨¡å‹:** æä¾›å·¥å…·æ¥æ¯”è¾ƒä¸åŒæ¨¡å‹åœ¨å¤šä¸ªæŒ‡æ ‡ä¸Šçš„è¡¨ç°ã€‚
* **ä¸ `Trainer` é›†æˆ:** `transformers` çš„ `Trainer` API å¯ä»¥ç›´æ¥æ¥æ”¶ä¸€ä¸ª `compute_metrics` å‡½æ•°ï¼Œè¯¥å‡½æ•°å†…éƒ¨é€šå¸¸ä½¿ç”¨ `evaluate` åº“æ¥è®¡ç®—éªŒè¯é›†ä¸Šçš„æŒ‡æ ‡ï¼ˆè§ 6.2.3 å’Œ 9.3.4 æ¡ˆä¾‹ï¼‰ã€‚

`evaluate` åº“ç®€åŒ–äº†è¯„ä¼°æµç¨‹ï¼Œä½¿å¾—è·å–æ ‡å‡†ã€å¯é çš„è¯„ä¼°ç»“æœæ›´åŠ å®¹æ˜“ã€‚

**C.6 `peft` åº“: å‚æ•°é«˜æ•ˆå¾®è°ƒ**

å¦‚ç¬¬ 6.3 èŠ‚æ‰€è¿°ï¼Œå‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) å¯¹äºé™ä½å¤§æ¨¡å‹å¾®è°ƒæˆæœ¬è‡³å…³é‡è¦ã€‚`peft` åº“æä¾›äº†ä¸€ç³»åˆ—ä¸»æµ PEFT æ–¹æ³•çš„æ˜“ç”¨å®ç°ã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **æ”¯æŒå¤šç§ PEFT æ–¹æ³•:** ç›®å‰ä¸»è¦æ”¯æŒ LoRA (åŠå…¶å˜ç§å¦‚ QLoRA - é‡åŒ– LoRA), Prefix Tuning, P-Tuning, Prompt Tuning, AdaLoRA ç­‰ã€‚
  * **æ˜“äºé›†æˆ:** é€šè¿‡ç®€å•çš„é…ç½®å¯¹è±¡ (`LoraConfig`, `PromptTuningConfig` ç­‰) å’Œ `get_peft_model()` å‡½æ•°ï¼Œå¯ä»¥è½»æ¾åœ°å°† PEFT æ–¹æ³•åº”ç”¨äºä»»ä½• `transformers` åº“ä¸­çš„æ¨¡å‹ã€‚
  * **ä¸ `transformers` æ— ç¼é›†æˆ:** åŒ…è£…åçš„ PEFT æ¨¡å‹ä»ç„¶å¯ä»¥ä½¿ç”¨ `Trainer` API è¿›è¡Œè®­ç»ƒï¼Œæˆ–è€…ä½¿ç”¨æ ‡å‡†çš„ PyTorch è®­ç»ƒå¾ªç¯ã€‚
  * **æ¨¡å‹ä¿å­˜/åŠ è½½:** åªéœ€ä¿å­˜/åŠ è½½è½»é‡çº§çš„é€‚é…å™¨ (Adapter) æƒé‡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹ã€‚
  * **æƒé‡åˆå¹¶:** å¯¹äº LoRAï¼Œæ”¯æŒå°†é€‚é…å™¨æƒé‡åˆå¹¶å›åŸºç¡€æ¨¡å‹ï¼Œå®ç°é›¶æ¨ç†å¼€é”€ã€‚
* **æˆ‘ä»¬åœ¨ 6.3.3 èŠ‚çš„ LoRA æ¡ˆä¾‹ä¸­æ¼”ç¤ºäº† `peft` åº“çš„åŸºæœ¬ç”¨æ³•ã€‚**

`peft` åº“æå¤§åœ°é™ä½äº†ä½¿ç”¨å…ˆè¿› PEFT æŠ€æœ¯çš„é—¨æ§›ï¼Œä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿæ›´ç»æµã€æ›´çµæ´»åœ°å¾®è°ƒå¤§æ¨¡å‹ã€‚

**C.7 `trl` åº“: Transformer å¼ºåŒ–å­¦ä¹ **

å¦‚ç¬¬ 7.2 èŠ‚æ‰€è¿°ï¼ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹  (RLHF) æ˜¯å¯¹é½å¤§æ¨¡å‹çš„å…³é”®æŠ€æœ¯ã€‚`trl` (Transformer Reinforcement Learning) åº“æ—¨åœ¨ç®€åŒ–ä½¿ç”¨ RLï¼ˆç‰¹åˆ«æ˜¯ PPOï¼‰æ¥å¾®è°ƒ `transformers` æ¨¡å‹çš„è¿‡ç¨‹ã€‚

* **æ ¸å¿ƒåŠŸèƒ½:**
  * **PPOTrainer:** æä¾›äº† PPO ç®—æ³•çš„æ ¸å¿ƒå®ç°ï¼Œå°è£…äº† Rolloutï¼ˆä»æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ï¼‰ã€å¥–åŠ±è®¡ç®—ã€ä¼˜åŠ¿ä¼°è®¡ã€ç­–ç•¥å’Œä»·å€¼å‡½æ•°æ›´æ–°ç­‰å¤æ‚æ­¥éª¤ã€‚
  * **ç‰¹æ®Šæ¨¡å‹å¤´:** æä¾›äº† `AutoModelForCausalLMWithValueHead` ç­‰æ¨¡å‹ç±»ï¼Œæ–¹ä¾¿åœ°åœ¨è¯­è¨€æ¨¡å‹åŸºç¡€ä¸Šæ·»åŠ ä¸€ä¸ªä»·å€¼å¤´ (Value Head) ç”¨äº PPO è®­ç»ƒä¸­çš„ä»·å€¼ä¼°è®¡ã€‚
  * **å¥–åŠ±æ¨¡å‹é›†æˆ:** å¯ä»¥æ–¹ä¾¿åœ°é›†æˆå¤–éƒ¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹ã€‚
  * **æ˜“äºä½¿ç”¨çš„ API:** æä¾›äº†ç›¸å¯¹ç®€æ´çš„æ¥å£æ¥é…ç½®å’Œè¿è¡Œ PPO è®­ç»ƒå¾ªç¯ã€‚
* **æˆ‘ä»¬åœ¨ 7.2.2 èŠ‚çš„æ¦‚å¿µä»£ç ä¸­å±•ç¤ºäº†ä½¿ç”¨ `trl` è¿›è¡Œ RLHF çš„åŸºæœ¬æ¡†æ¶ã€‚**

`trl` åº“ä½¿å¾—åŸæœ¬éå¸¸å¤æ‚çš„ RLHF æµç¨‹å˜å¾—ç›¸å¯¹å®¹æ˜“ä¸Šæ‰‹ï¼Œä¿ƒè¿›äº†å¯¹é½æŠ€æœ¯çš„ç ”ç©¶å’Œåº”ç”¨ã€‚

**C.8 ç”Ÿæ€ç³»ç»Ÿçš„ååŒä½œç”¨**

Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„å¼ºå¤§ä¹‹å¤„åœ¨äºè¿™äº›åº“ä¹‹é—´çš„**ååŒä½œç”¨**:

* ä½ å¯ä»¥ä½¿ç”¨ `datasets` åŠ è½½æ•°æ®ã€‚
* ä½¿ç”¨ `tokenizers` (é€šè¿‡ `transformers`) è¿›è¡Œåˆ†è¯ã€‚
* ä½¿ç”¨ `transformers` åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚
* ä½¿ç”¨ `peft` åº”ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒã€‚
* ä½¿ç”¨ `Trainer` (æˆ– `trl` ä¸­çš„ `PPOTrainer`) è¿›è¡Œè®­ç»ƒ/å¾®è°ƒã€‚
* ä½¿ç”¨ `evaluate` è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚
* æœ€ç»ˆå°†ä½ çš„æ¨¡å‹ã€æ•°æ®é›†ã€æ¼”ç¤ºç”šè‡³è¯„ä¼°ç»“æœåˆ†äº«åˆ° `Hub` ä¸Šã€‚

è¿™ç§ç«¯åˆ°ç«¯çš„ã€é«˜åº¦é›†æˆåŒ–çš„å·¥å…·é“¾æå¤§åœ°æé«˜äº†å¼€å‘æ•ˆç‡ï¼Œä¿ƒè¿›äº†ç¤¾åŒºçš„ç¹è£å’ŒæŠ€æœ¯çš„å¿«é€Ÿè¿­ä»£ã€‚ç†Ÿç»ƒæŒæ¡ Hugging Face ç”Ÿæ€ç³»ç»Ÿçš„ä½¿ç”¨ï¼Œæ˜¯è¿›è¡Œç°ä»£å¤§æ¨¡å‹å¼€å‘ä¸ç ”ç©¶çš„å…³é”®æŠ€èƒ½ä¹‹ä¸€ã€‚

---

## **é™„å½•Dï¼šå¸¸ç”¨å¤§æ¨¡å‹èµ„æºåˆ—è¡¨**

å¤§æ¨¡å‹é¢†åŸŸå‘å±•è¿…é€Ÿï¼Œä¿¡æ¯æµ©å¦‚çƒŸæµ·ã€‚ä¸ºäº†å¸®åŠ©è¯»è€…åœ¨æœ¬ä¹¦è®°ç¡€ä¸Šè¿›ä¸€æ­¥æ·±å…¥å­¦ä¹ ã€è¿½è¸ªå‰æ²¿åŠ¨æ€ä»¥åŠæŸ¥æ‰¾å®ç”¨èµ„æºï¼Œæœ¬é™„å½•æ•´ç†äº†ä¸€äº›å¸¸ç”¨å’Œæœ‰ä»·å€¼çš„å¤§æ¨¡å‹ç›¸å…³èµ„æºï¼Œæ¶µç›–æ¨¡å‹ã€æ•°æ®é›†ã€è®ºæ–‡ã€æ¡†æ¶ã€ç¤¾åŒºå’Œèµ„è®¯ç­‰æ–¹é¢ã€‚è¯·æ³¨æ„ï¼Œç”±äºé¢†åŸŸå‘å±•æå¿«ï¼Œæ­¤åˆ—è¡¨å¯èƒ½æ— æ³•åšåˆ°å®Œå…¨å®æ—¶æ›´æ–°ï¼Œä½†å¯ä»¥ä½œä¸ºä¸€ä¸ªè‰¯å¥½çš„èµ·ç‚¹ã€‚

**D.1 æ¨¡å‹ä¸æ¨¡å‹ä¸­å¿ƒ (Models & Model Hubs)**

* **Hugging Face Hub (Models):** (https://huggingface.co/models)
  * ç›®å‰æœ€å¤§ã€æœ€æ´»è·ƒçš„æ¨¡å‹ä¸­å¿ƒï¼ŒåŒ…å«æ•°ä»¥ä¸‡è®¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆNLP, CV, Audio, Multimodalï¼‰ï¼Œæ”¯æŒæŒ‰ä»»åŠ¡ã€æ¡†æ¶ã€è¯­è¨€ã€åº“ç­‰ç­›é€‰ã€‚æ˜¯æŸ¥æ‰¾å’Œä¸‹è½½å¼€æºæ¨¡å‹çš„é¦–é€‰ä¹‹åœ°ã€‚
  * è‘—åæ¨¡å‹ç³»åˆ—ç¤ºä¾‹ï¼šBERT, GPT-2, T5, BART, RoBERTa, ALBERT, DistilBERT, ELECTRA, XLNet, Llama ç³»åˆ— (Meta), Mistral/Mixtral (Mistral AI), Gemma (Google), Qwen (Alibaba), Phi (Microsoft), Falcon, MPT, Stable Diffusion, CLIP, ViT ç­‰ã€‚
* **OpenAI Models:** (https://platform.openai.com/docs/models)
  * æä¾›å¼ºå¤§çš„é—­æºå•†ç”¨æ¨¡å‹ APIï¼Œå¦‚ GPT-4, GPT-3.5-Turbo, DALL-E 3, Embedding models (Ada), Whisper (ASR)ã€‚éœ€è¦ API Key å’Œä»˜è´¹ä½¿ç”¨ã€‚
* **Anthropic Models:** (https://www.anthropic.com/product)
  * æä¾› Claude ç³»åˆ—æ¨¡å‹ APIï¼ˆClaude 3 Opus/Sonnet/Haikuï¼‰ï¼Œä»¥å¼ºå¤§çš„å¯¹è¯èƒ½åŠ›ã€é•¿ä¸Šä¸‹æ–‡å’Œå¯¹é½ï¼ˆConstitutional AIï¼‰è‘—ç§°ã€‚
* **Google AI / Vertex AI Models:** (https://ai.google/discover/generativeai/, https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models)
  * æä¾› Gemini ç³»åˆ—æ¨¡å‹ API (Gemini Pro/Ultra)ï¼Œä»¥åŠ PaLM, Imagen (æ–‡ç”Ÿå›¾), Codey (ä»£ç ) ç­‰æ¨¡å‹ã€‚éƒ¨åˆ†æ¨¡å‹é€šè¿‡ Google Cloud Vertex AI æä¾›ã€‚
* **Meta AI Models:** (https://ai.meta.com/models/)
  * å‘å¸ƒäº†é‡è¦çš„å¼€æºæ¨¡å‹ç³»åˆ—ï¼Œå¦‚ Llama (Llama 2, Llama 3), Code Llama, SeamlessM4T (å¤šæ¨¡æ€ç¿»è¯‘/è¯­éŸ³) ç­‰ï¼Œé€šå¸¸åœ¨ Hugging Face Hub ä¸Šå¯ä»¥æ‰¾åˆ°ã€‚
* **Mistral AI:** (https://mistral.ai/technology/)
  * å‘å¸ƒäº†é«˜æ€§èƒ½çš„å¼€æºæ¨¡å‹ Mistral 7B å’Œ Mixtral 8x7B (MoE)ï¼Œä»¥åŠé—­æºçš„ Mistral Large/Small æ¨¡å‹ APIã€‚
* **Cohere Models:** (https://cohere.com/models)
  * æä¾› Command, Embed, Rerank ç­‰é¢å‘ä¼ä¸šåº”ç”¨çš„ APIã€‚

**D.2 æ•°æ®é›†ä¸æ•°æ®é›†ä¸­å¿ƒ (Datasets & Dataset Hubs)**

* **Hugging Face Hub (Datasets):** (https://huggingface.co/datasets)
  * æœ€å¤§çš„å…¬å…±æ•°æ®é›†ä¸­å¿ƒï¼Œå¯é€šè¿‡ `datasets` åº“è½»æ¾è®¿é—®ã€‚
  * è‘—åæ•°æ®é›†ç¤ºä¾‹ï¼šGLUE, SuperGLUE, SQuAD, CNN/DailyMail, IMDB, SST-2, WMT (ç¿»è¯‘), Common Crawl (éœ€è¦è‡ªè¡Œå¤„ç†), Wikipedia, BooksCorpus, C4 (Colossal Clean Crawled Corpus), The Pile, RedPajama, SlimPajama, OpenWebText, COCO (å›¾åƒæè¿°), LAION (å›¾æ–‡å¯¹), LibriSpeech (è¯­éŸ³) ç­‰ã€‚
* **Papers with Code Datasets:** (https://paperswithcode.com/datasets)
  * ä¸€ä¸ªå¹¿æ³›çš„æœºå™¨å­¦ä¹ æ•°æ®é›†ç›®å½•ï¼ŒåŒ…å«ä»»åŠ¡ã€æŒ‡æ ‡å’Œç›¸å…³è®ºæ–‡ã€‚
* **Kaggle Datasets:** (https://www.kaggle.com/datasets)
  * åŒ…å«å¤§é‡ç”¨äºæ•°æ®ç§‘å­¦ç«èµ›å’Œç»ƒä¹ çš„æ•°æ®é›†ï¼Œç§ç±»ç¹å¤šã€‚
* **Google Dataset Search:** (https://datasetsearch.research.google.com/)
  * ä¸€ä¸ªæœç´¢å¼•æ“ï¼Œå¯ä»¥æŸ¥æ‰¾äº’è”ç½‘ä¸Šå…¬å¼€å¯ç”¨çš„æ•°æ®é›†ã€‚
* **Awesome Public Datasets:** (https://github.com/awesomedata/awesome-public-datasets)
  * GitHub ä¸Šçš„ä¸€ä¸ªç²¾é€‰åˆ—è¡¨ï¼Œæ¶µç›–å„ç§é¢†åŸŸçš„å…¬å…±æ•°æ®é›†ã€‚

**D.3 é‡è¦è®ºæ–‡ä¸æ–‡çŒ®åº“ (Key Papers & Literature Databases)**

è¿½è¸ªæœ€æ–°çš„ç ”ç©¶è¿›å±•éœ€è¦é˜…è¯»ç›¸å…³è®ºæ–‡ã€‚

* **å¿…è¯»ç»å…¸è®ºæ–‡ (ç¤ºä¾‹ï¼Œé exhaustive):**
  * Attention Is All You Need (Transformer): Vaswani et al., 2017
  * BERT: Pre-training of Deep Bidirectional Transformers...: Devlin et al., 2018
  * Language Models are Unsupervised Multitask Learners (GPT-2): Radford et al., 2019
  * Language Models are Few-Shot Learners (GPT-3): Brown et al., 2020
  * Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5): Raffel et al., 2019
  * BART: Denoising Sequence-to-Sequence Pre-training...: Lewis et al., 2019
  * RoBERTa: A Robustly Optimized BERT Pretraining Approach: Liu et al., 2019
  * Learning Transferable Visual Models From Natural Language Supervision (CLIP): Radford et al., 2021
  * An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT): Dosovitskiy et al., 2020
  * Scaling Language Models: Methods, Analysis & Insights... (Scaling Laws): Kaplan et al., 2020
  * Training Compute-Optimal Large Language Models (Chinchilla Scaling Laws): Hoffmann et al., 2022
  * LoRA: Low-Rank Adaptation of Large Language Models: Hu et al., 2021
  * Training Language Models to Follow Instructions... (InstructGPT/RLHF): Ouyang et al., 2022
  * Constitutional AI: Harmlessness from AI Feedback: Bai et al., 2022
  * Direct Preference Optimization: Your Language Model is Secretly a Reward Model: Rafailov et al., 2023
  * LLaMA: Open and Efficient Foundation Language Models: Touvron et al., 2023
  * Chain-of-Thought Prompting Elicits Reasoning...: Wei et al., 2022
  * Large Language Models are Zero-Shot Reasoners (Zero-shot CoT): Kojima et al., 2022
  * Self-Consistency Improves Chain of Thought Reasoning...: Wang et al., 2022
* **æ–‡çŒ®æ•°æ®åº“ä¸å·¥å…·:**
  * **arXiv (Computer Science - Computation and Language, Machine Learning):** (https://arxiv.org/list/cs.CL/recent, https://arxiv.org/list/cs.LG/recent) è·å–æœ€æ–°é¢„å°æœ¬è®ºæ–‡çš„ä¸»è¦æ¥æºã€‚
  * **Google Scholar:** (https://scholar.google.com/) å¼ºå¤§çš„å­¦æœ¯æœç´¢å¼•æ“ã€‚
  * **Semantic Scholar:** (https://www.semanticscholar.org/) æä¾› AI é©±åŠ¨çš„è®ºæ–‡æœç´¢ã€å¼•ç”¨å›¾è°±å’Œæ‘˜è¦ã€‚
  * **Papers with Code:** (https://paperswithcode.com/) å°†è®ºæ–‡ã€ä»£ç å®ç°å’Œè¯„ä¼°ç»“æœè”ç³»èµ·æ¥çš„å¹³å°ï¼Œè¿½è¸ª SOTA è¿›å±•çš„å¥½åœ°æ–¹ã€‚
  * **Connected Papers:** (https://www.connectedpapers.com/) å¯è§†åŒ–è®ºæ–‡å¼•ç”¨ç½‘ç»œï¼Œå¸®åŠ©å‘ç°ç›¸å…³æ–‡çŒ®ã€‚
  * **ä¸»è¦ AI/ML/NLP ä¼šè®®:** NeurIPS, ICML, ICLR, ACL, EMNLP, NAACL, CVPR, ICCV, ECCV (å…³æ³¨è¿™äº›ä¼šè®®çš„è®ºæ–‡é›†)ã€‚

**D.4 æ¡†æ¶ä¸å·¥å…· (Frameworks & Tools)**

* **æ·±åº¦å­¦ä¹ æ¡†æ¶:**
  * **PyTorch:** (https://pytorch.org/) æœ¬ä¹¦ä¸»è¦ä½¿ç”¨çš„æ¡†æ¶ï¼Œçµæ´»æ˜“ç”¨ã€‚
  * **TensorFlow:** (https://www.tensorflow.org/) Google å¼€å‘çš„æ¡†æ¶ï¼Œç”Ÿæ€æˆç†Ÿï¼Œéƒ¨ç½²å·¥å…·å®Œå–„ã€‚
  * **JAX:** (https://github.com/google/jax) Google å¼€å‘çš„ç”¨äºé«˜æ€§èƒ½æ•°å€¼è®¡ç®—å’Œæœºå™¨å­¦ä¹ ç ”ç©¶çš„åº“ï¼Œä»¥å‡½æ•°å˜æ¢ï¼ˆå¦‚è‡ªåŠ¨å¾®åˆ†ã€å‘é‡åŒ–ã€å¹¶è¡ŒåŒ–ï¼‰ä¸ºç‰¹è‰²ã€‚
* **Hugging Face ç”Ÿæ€:** (å¦‚é™„å½• C æ‰€è¿°)
  * `transformers`, `datasets`, `tokenizers`, `evaluate`, `peft`, `trl`, `accelerate`
* **åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶:**
  * **DeepSpeed:** (https://github.com/microsoft/DeepSpeed) ç”¨äºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–åº“ï¼ˆZeRO, MoE ç­‰ï¼‰ã€‚
  * **Megatron-LM:** (https://github.com/NVIDIA/Megatron-LM) NVIDIA å¼€å‘çš„é«˜æ•ˆå¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œå®ç°ã€‚
  * **PyTorch FSDP:** (https://pytorch.org/docs/stable/fsdp.html) PyTorch å†…ç½®çš„å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œã€‚
  * **Colossal-AI:** (https://github.com/hpcaitech/ColossalAI) æä¾›ç»Ÿä¸€æ¥å£çš„åˆ†å¸ƒå¼è®­ç»ƒç³»ç»Ÿã€‚
* **LLM åº”ç”¨å¼€å‘æ¡†æ¶:**
  * **LangChain:** (https://github.com/langchain-ai/langchain) ç”¨äºæ„å»º LLM åº”ç”¨çš„æµè¡Œæ¡†æ¶ï¼Œæä¾›äº† Agentã€Chainã€Memoryã€Document Loaders/Splittersã€Vector Stores é›†æˆç­‰æ¨¡å—ã€‚
  * **LlamaIndex:** (https://github.com/run-llama/llama_index) ä¸“æ³¨äºå°† LLM ä¸å¤–éƒ¨æ•°æ®è¿æ¥ï¼ˆç‰¹åˆ«æ˜¯ RAGï¼‰ï¼Œæä¾›äº†å¼ºå¤§çš„æ•°æ®ç´¢å¼•å’Œæ£€ç´¢åŠŸèƒ½ã€‚
* **å‘é‡æ•°æ®åº“:**
  * **FAISS:** (https://github.com/facebookresearch/faiss) é«˜æ•ˆå‘é‡æœç´¢åº“ã€‚
  * **ChromaDB:** (https://github.com/chroma-core/chroma) å¼€æºåµŒå…¥æ•°æ®åº“ã€‚
  * **Milvus:** (https://milvus.io/) å¼€æºå‘é‡æ•°æ®åº“ã€‚
  * **Weaviate:** (https://github.com/weaviate/weaviate) å¼€æºå‘é‡æœç´¢å¼•æ“ã€‚
  * **Pinecone / Google Vertex AI Matching Engine / AWS OpenSearch (k-NN):** äº‘æœåŠ¡ã€‚
* **å®éªŒè·Ÿè¸ªä¸ç›‘æ§:**
  * **TensorBoard:** (https://www.tensorflow.org/tensorboard) å¼€æºçš„å¯è§†åŒ–å·¥å…·åŒ…ã€‚
  * **Weights & Biases (WandB):** (https://wandb.ai/) æµè¡Œçš„å•†ä¸šå®éªŒè·Ÿè¸ªå¹³å°ï¼ˆæä¾›å…è´¹ä¸ªäººç‰ˆï¼‰ã€‚
  * **MLflow:** (https://mlflow.org/) å¼€æºçš„æœºå™¨å­¦ä¹ ç”Ÿå‘½å‘¨æœŸç®¡ç†å¹³å°ã€‚
* **ç«¯ä¾§æ¨ç†æ¡†æ¶:**
  * **llama.cpp:** (https://github.com/ggerganov/llama.cpp) åœ¨ CPU (å’Œ Metal/OpenCL) ä¸Šé«˜æ•ˆè¿è¡Œ Llama ç±»æ¨¡å‹çš„ C++ å®ç°ã€‚
  * **MLC LLM:** (https://github.com/mlc-ai/mlc-llm) å°† LLM ç¼–è¯‘åˆ°å„ç§æœ¬åœ°å’Œ Web åç«¯çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚
  * **MediaPipe LLM Inference API:** (https://developers.google.com/mediapipe/solutions/llm_inference) Google æä¾›çš„ç«¯ä¾§ LLM æ¨ç† APIã€‚

**D.5 ç¤¾åŒºã€åšå®¢ä¸èµ„è®¯ (Communities, Blogs & News)**

ä¿æŒä¿¡æ¯æ›´æ–°éœ€è¦å…³æ³¨æ´»è·ƒçš„ç¤¾åŒºå’Œä¿¡æ¯æºã€‚

* **åœ¨çº¿ç¤¾åŒº:**
  * **Hugging Face Forums:** (https://discuss.huggingface.co/) å®˜æ–¹è®ºå›ï¼Œè®¨è®ºåº“ä½¿ç”¨ã€æ¨¡å‹é—®é¢˜ç­‰ã€‚
  * **Reddit:**
    * r/MachineLearning: ç»¼åˆæ€§æœºå™¨å­¦ä¹ è®¨è®ºã€‚
    * r/LocalLLaMA: ä¸“æ³¨äºåœ¨æœ¬åœ°è¿è¡Œå¼€æº LLM çš„è®¨è®ºã€‚
    * r/LanguageTechnology: NLP ç›¸å…³è®¨è®ºã€‚
  * **Discord æœåŠ¡å™¨:** è®¸å¤šå¼€æºé¡¹ç›®ï¼ˆå¦‚ EleutherAI, LAION, LlamaIndex, LangChainï¼‰éƒ½æœ‰æ´»è·ƒçš„ Discord æœåŠ¡å™¨ã€‚
  * **Stack Overflow / AI Stack Exchange:** é—®ç­”ç¤¾åŒºã€‚
* **çŸ¥åç ”ç©¶æœºæ„/å…¬å¸åšå®¢:**
  * OpenAI Blog: (https://openai.com/blog/)
  * Google AI Blog: (https://ai.googleblog.com/)
  * Meta AI Blog: (https://ai.meta.com/blog/)
  * DeepMind Blog: (https://deepmind.google/blog/) (ç°åœ¨å¯èƒ½æ•´åˆåˆ° Google AI Blog)
  * Anthropic News & Blog: (https://www.anthropic.com/news)
  * Microsoft Research Blog (AI Section): (https://www.microsoft.com/en-us/research/blog/?facet_filter_entity=ai)
  * Stanford AI Lab (SAIL) Blog: (https://ai.stanford.edu/blog/)
  * Berkeley Artificial Intelligence Research (BAIR) Blog: (https://bair.berkeley.edu/blog/)
  * Hugging Face Blog: (https://huggingface.co/blog)
* **AI æ–°é—»ä¸èµ„è®¯:**
  * **Import AI Newsletter (by Jack Clark):** è´¨é‡å¾ˆé«˜çš„ AI å‘¨æŠ¥ï¼Œæ€»ç»“é‡è¦è¿›å±•å’Œæ€è€ƒã€‚
  * **The Batch (DeepLearning.AI):** å´æ©è¾¾å›¢é˜Ÿå‡ºå“çš„ AI å‘¨æŠ¥ã€‚
  * **TechCrunch (AI Section), VentureBeat (AI Section):** ç§‘æŠ€åª’ä½“çš„ AI æŠ¥é“ã€‚
  * **Twitter / X:** å…³æ³¨é¢†åŸŸå†…çš„ç ”ç©¶è€…ã€å·¥ç¨‹å¸ˆå’Œæ„è§é¢†è¢–æ˜¯è·å–å¿«é€Ÿä¿¡æ¯çš„é‡è¦é€”å¾„ã€‚
* **æ•™ç¨‹ä¸è¯¾ç¨‹:**
  * Hugging Face Course: (https://huggingface.co/learn/nlp-course) å…è´¹çš„ NLP ä¸ Transformers æ•™ç¨‹ã€‚
  * DeepLearning.AI Courses (Coursera): æä¾› NLPã€ç”Ÿæˆå¼ AI ç­‰ä¸“é¡¹è¯¾ç¨‹ã€‚
  * Stanford CS224n (NLP with Deep Learning): ç»å…¸çš„ NLP è¯¾ç¨‹ï¼Œè®²ä¹‰å’Œè§†é¢‘å…¬å¼€ã€‚
  * Full Stack Deep Learning: (https://fullstackdeeplearning.com/) å…³æ³¨å°†æ·±åº¦å­¦ä¹ æ¨¡å‹æŠ•å…¥ç”Ÿäº§çš„å®è·µè¯¾ç¨‹ã€‚

è¿™ä¸ªåˆ—è¡¨æä¾›äº†ä¸€ä¸ªèµ·ç‚¹ï¼Œé¼“åŠ±è¯»è€…æ ¹æ®è‡ªå·±çš„å…´è¶£å’Œéœ€æ±‚è¿›ä¸€æ­¥æ¢ç´¢å’Œå‘ç°æ›´å¤šæœ‰ä»·å€¼çš„èµ„æºã€‚ç§¯æå‚ä¸ç¤¾åŒºã€é˜…è¯»æœ€æ–°æ–‡çŒ®ã€åŠ¨æ‰‹å®è·µæ˜¯è·Ÿä¸Šå¤§æ¨¡å‹é¢†åŸŸå¿«é€Ÿå‘å±•çš„å…³é”®ã€‚

---

## é™„å½•Eï¼šæœ¯è¯­è¡¨ (Glossary)

æœ¬æœ¯è¯­è¡¨æ—¨åœ¨è§£é‡Šæœ¬ä¹¦ä¸­å‡ºç°çš„ä»¥åŠå¤§æ¨¡å‹é¢†åŸŸå¸¸ç”¨çš„ä¸€äº›å…³é”®æœ¯è¯­å’Œç¼©å†™ï¼Œæ–¹ä¾¿è¯»è€…æŸ¥é˜…å’Œç†è§£ã€‚æœ¯è¯­æŒ‰å­—æ¯é¡ºåºæ’åˆ—ã€‚

* **Activation Function (æ¿€æ´»å‡½æ•°):**
  ç¥ç»ç½‘ç»œä¸­å¼•å…¥éçº¿æ€§çš„å‡½æ•°ï¼Œä½œç”¨äºç¥ç»å…ƒçš„åŠ æƒè¾“å…¥ã€‚å¸¸è§çš„æœ‰ ReLU, GeLU, Swish/SiLUã€‚ (è§ 2.1.3)
* **Adapter Tuning (é€‚é…å™¨å¾®è°ƒ):**
  ä¸€ç§å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•ï¼Œé€šè¿‡åœ¨ Transformer å±‚ä¸­æ’å…¥å°å‹å¯è®­ç»ƒçš„â€œé€‚é…å™¨â€æ¨¡å—æ¥è¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒå‚æ•°ã€‚ (è§ 6.3.2)
* **Agent (æ™ºèƒ½ä½“):**
  èƒ½å¤Ÿæ„ŸçŸ¥ç¯å¢ƒã€è¿›è¡Œè§„åˆ’ã€åšå‡ºå†³ç­–å¹¶æ‰§è¡ŒåŠ¨ä½œä»¥è¾¾æˆç›®æ ‡çš„ç³»ç»Ÿã€‚åœ¨ LLM é¢†åŸŸï¼ŒæŒ‡èƒ½å¤Ÿä½¿ç”¨å·¥å…·ã€ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„ LLM ç³»ç»Ÿã€‚ (è§ 12.3)
* **AGI (Artificial General Intelligence, é€šç”¨äººå·¥æ™ºèƒ½):**
  å…·å¤‡ä¸äººç±»ç›¸å½“æˆ–è¶…è¶Šäººç±»çš„é€šç”¨æ™ºèƒ½æ°´å¹³ï¼Œèƒ½å¤Ÿå­¦ä¹ å’Œæ‰§è¡Œä»»ä½•æ™ºåŠ›ä»»åŠ¡çš„å‡æƒ³äººå·¥æ™ºèƒ½ã€‚ (è§ 13.4.5)
* **Alignment (å¯¹é½):**
  ç¡®ä¿ AI ç³»ç»Ÿçš„è¡Œä¸ºç¬¦åˆäººç±»æ„å›¾ã€ä»·å€¼è§‚å’Œåå¥½çš„è¿‡ç¨‹ã€‚ç›®æ ‡é€šå¸¸æ¦‚æ‹¬ä¸º HHH åŸåˆ™ï¼ˆæœ‰ç”¨ã€è¯šå®ã€æ— å®³ï¼‰ã€‚ (è§ ç¬¬ 7 ç« )
* **All-Reduce:**
  ä¸€ç§åˆ†å¸ƒå¼è®¡ç®—ä¸­çš„é›†ä½“é€šä¿¡æ“ä½œï¼Œç”¨äºå°†æ‰€æœ‰è¿›ç¨‹ï¼ˆèŠ‚ç‚¹/è®¾å¤‡ï¼‰ä¸Šçš„æ•°æ®ï¼ˆå¦‚æ¢¯åº¦ï¼‰è¿›è¡Œèšåˆï¼ˆå¦‚æ±‚å’Œæˆ–å¹³å‡ï¼‰ï¼Œå¹¶å°†æœ€ç»ˆç»“æœå¹¿æ’­å›æ‰€æœ‰è¿›ç¨‹ã€‚å¸¸ç”¨äºæ•°æ®å¹¶è¡Œè®­ç»ƒã€‚ (è§ 4.4.1)
* **Alpaca:**
  ä¸€ä¸ªåŸºäº Llama æ¨¡å‹ï¼Œé€šè¿‡ Self-Instruct æ–¹æ³•ï¼ˆä½¿ç”¨ `text-davinci-003` ç”ŸæˆæŒ‡ä»¤æ•°æ®ï¼‰è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒå¾—åˆ°çš„æ¨¡å‹ï¼Œå±•ç¤ºäº†å¼ºå¤§çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ (è§ 6.4.2)
* **Attention Mechanism (æ³¨æ„åŠ›æœºåˆ¶):**
  å…è®¸æ¨¡å‹åœ¨å¤„ç†åºåˆ—æ—¶åŠ¨æ€åœ°å…³æ³¨è¾“å…¥ï¼ˆæˆ–è‡ªèº«ï¼‰ä¸åŒéƒ¨åˆ†é‡è¦æ€§çš„æœºåˆ¶ã€‚æ˜¯ Transformer çš„æ ¸å¿ƒã€‚ (è§ 1.2.4, 3.2)
* **Autoregressive (è‡ªå›å½’):**
  ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œå…¶é¢„æµ‹å½“å‰å…ƒç´ ï¼ˆå¦‚ Tokenï¼‰çš„æ¦‚ç‡ä»…ä¾èµ–äºå…ˆå‰ç”Ÿæˆçš„å…ƒç´ ã€‚CLMï¼ˆå› æœè¯­è¨€æ¨¡å‹ï¼‰æ˜¯è‡ªå›å½’çš„ã€‚ (è§ 5.2.2, 3.6.1)
* **Backpropagation (åå‘ä¼ æ’­):**
  è®­ç»ƒç¥ç»ç½‘ç»œçš„æ ¸å¿ƒç®—æ³•ï¼Œé€šè¿‡é“¾å¼æ³•åˆ™è®¡ç®—æŸå¤±å‡½æ•°ç›¸å¯¹äºç½‘ç»œå‚æ•°çš„æ¢¯åº¦ã€‚ (è§ 2.1.1)
* **BART (Bidirectional and Auto-Regressive Transformer):**
  ä¸€ç§åŸºäº Transformer çš„ Encoder-Decoder æ¶æ„æ¨¡å‹ï¼Œé€šè¿‡å»å™ªè‡ªç¼–ç ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬å¡«å……ï¼‰è¿›è¡Œé¢„è®­ç»ƒã€‚ (è§ 3.8.3, 5.2.3)
* **Batch Normalization (BatchNorm, BN, æ‰¹å½’ä¸€åŒ–):**
  ä¸€ç§å½’ä¸€åŒ–æŠ€æœ¯ï¼Œä¸»è¦ç”¨äº CNNï¼Œåœ¨æ‰¹æ¬¡ç»´åº¦ä¸Šå¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ã€‚åœ¨å¤§æ¨¡å‹ï¼ˆå°¤å…¶æ˜¯ Transformerï¼‰ä¸­è¾ƒå°‘ä½¿ç”¨ï¼Œé€šå¸¸è¢« Layer Normalization æ›¿ä»£ã€‚ (è§ 2.1.4)
* **Batch Size (æ‰¹æ¬¡å¤§å°):**
  åœ¨ä¸€æ¬¡è®­ç»ƒè¿­ä»£ä¸­ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚å¯¹äºåˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒºåˆ†å…¨å±€æ‰¹æ¬¡å¤§å° (Global Batch Size) å’Œæ¯ä¸ªè®¾å¤‡çš„å¾®æ‰¹æ¬¡å¤§å° (Micro-batch Size)ã€‚
* **Benchmark (åŸºå‡†æµ‹è¯•):**
  ä¸€ç»„æ ‡å‡†åŒ–çš„ä»»åŠ¡ã€æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å’Œæ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚ä¾‹å¦‚ GLUE, SuperGLUE, MMLU, BIG-bench, HELMã€‚ (è§ 9.3)
* **BERT (Bidirectional Encoder Representations from Transformers):**
  åŸºäº Transformer Encoder çš„é‡Œç¨‹ç¢‘å¼é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨ MLM ä»»åŠ¡å­¦ä¹ æ·±åº¦åŒå‘ä¸Šä¸‹æ–‡è¡¨ç¤ºï¼Œæ“…é•¿ NLU ä»»åŠ¡ã€‚ (è§ 1.2.6, 3.8.1, 5.2.1)
* **Bias (åè§):**
  æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ åˆ°çš„ã€åæ˜ ç¤¾ä¼šåˆ»æ¿å°è±¡æˆ–å¯¹ç‰¹å®šç¾¤ä½“ä¸å…¬å¹³çš„ç³»ç»Ÿæ€§å€¾å‘ã€‚ (è§ 13.2.1)
* **Bias (åç½®):**
  ç¥ç»ç½‘ç»œä¸­åŠ åˆ°åŠ æƒè¾“å…¥ä¸Šçš„å¯å­¦ä¹ å‚æ•° `b`ã€‚
* **BLEU (Bilingual Evaluation Understudy):**
  ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘è´¨é‡çš„æŒ‡æ ‡ï¼ŒåŸºäº n-gram é‡å åº¦ã€‚ (è§ 9.2.2)
* **BPE (Byte-Pair Encoding, å­—èŠ‚å¯¹ç¼–ç ):**
  ä¸€ç§å¸¸ç”¨çš„å­è¯åˆ†è¯ç®—æ³•ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é¢‘ç‡æœ€é«˜çš„ç›¸é‚»å­—èŠ‚å¯¹æ¥æ„å»ºè¯æ±‡è¡¨ã€‚ (è§ 2.2.2)
* **CAI (Constitutional AI, å®ªæ³• AI):**
  ä¸€ç§å¯¹é½æ–¹æ³•ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„åŸåˆ™ï¼ˆå®ªæ³•ï¼‰æ¥æŒ‡å¯¼æ¨¡å‹è‡ªæˆ‘ä¿®æ­£æˆ–æä¾›åé¦ˆï¼Œä»¥å‡å°‘å¯¹äººç±»åé¦ˆçš„ä¾èµ–ã€‚ (è§ 7.3.1)
* **Catastrophic Forgetting (ç¾éš¾æ€§é—å¿˜):**
  ç¥ç»ç½‘ç»œï¼ˆå°¤å…¶æ˜¯åœ¨æŒç»­å­¦ä¹ åœºæ™¯ä¸‹ï¼‰åœ¨å­¦ä¹ æ–°ä»»åŠ¡æ—¶ï¼Œé—å¿˜æ‰ä¹‹å‰ä»»åŠ¡å­¦åˆ°çŸ¥è¯†çš„ç°è±¡ã€‚ (è§ 6.2.1, 13.3)
* **Causal Language Modeling (CLM, å› æœè¯­è¨€æ¨¡å‹):**
  ä¸€ç§é¢„è®­ç»ƒä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ª Tokenï¼Œç»™å®šå‰é¢çš„æ‰€æœ‰ Tokenã€‚æ˜¯ GPT ç­‰ Decoder-Only æ¨¡å‹çš„ä¸»è¦é¢„è®­ç»ƒæ–¹å¼ã€‚ (è§ 5.2.2)
* **Chain-of-Thought (CoT, æ€ç»´é“¾):**
  ä¸€ç§æç¤ºæŠ€å·§ï¼Œé€šè¿‡å¼•å¯¼æ¨¡å‹ç”Ÿæˆä¸­é—´æ¨ç†æ­¥éª¤æ¥æé«˜å…¶åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚ (è§ 8.3.1)
* **Checkpointing (æ£€æŸ¥ç‚¹):**
  åœ¨é•¿æ—¶é—´è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆå‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€ã€æ­¥æ•°ç­‰ï¼‰ï¼Œä»¥ä¾¿åœ¨ä¸­æ–­åæ¢å¤è®­ç»ƒã€‚ (è§ 5.3.4)
* **Chunking (åˆ†å—):**
  åœ¨ RAG ç­‰åº”ç”¨ä¸­ï¼Œå°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆè¾ƒå°çš„æ–‡æœ¬å—ä»¥ä¾¿è¿›è¡ŒåµŒå…¥å’Œæ£€ç´¢çš„è¿‡ç¨‹ã€‚ (è§ 11.2.2)
* **Classifier Head (åˆ†ç±»å¤´):**
  æ·»åŠ åˆ°é¢„è®­ç»ƒæ¨¡å‹é¡¶éƒ¨ï¼Œç”¨äºç‰¹å®šåˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºå±‚ï¼ˆé€šå¸¸æ˜¯çº¿æ€§å±‚ + Softmaxï¼‰ã€‚ (è§ 6.2.2)
* **CLIP (Contrastive Language-Image Pre-training):**
  ä¸€ç§é€šè¿‡å¯¹æ¯”å­¦ä¹ åœ¨å¤§è§„æ¨¡å›¾æ–‡å¯¹ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå­¦ä¹ å›¾æ–‡å…±äº«è¡¨ç¤ºç©ºé—´ï¼Œå¸¸ç”¨äºå¤šæ¨¡æ€ä»»åŠ¡çš„åŸºç¡€ã€‚ (è§ 12.1.2)
* **CLM (Causal Language Model):**
  è§ Causal Language Modelingã€‚
* **[CLS] Token:**
  BERT ç­‰æ¨¡å‹ä¸­ç”¨äºåºåˆ—å¼€å¤´çš„ç‰¹æ®Š Tokenï¼Œå…¶å¯¹åº”çš„æœ€ç»ˆéšè—çŠ¶æ€é€šå¸¸è¢«ç”¨ä½œæ•´ä¸ªåºåˆ—çš„èšåˆè¡¨ç¤ºï¼Œè¾“å…¥åˆ°åˆ†ç±»å¤´ä¸­ã€‚ (è§ 2.2.2)
* **Common Crawl:**
  ä¸€ä¸ªåŒ…å«æµ·é‡ç½‘é¡µå­˜æ¡£æ•°æ®çš„å…¬å¼€é¡¹ç›®ï¼Œæ˜¯è®¸å¤šå¤§æ¨¡å‹é¢„è®­ç»ƒæ•°æ®çš„ä¸»è¦æ¥æºã€‚ (è§ 4.1.1)
* **Compute (ç®—åŠ›):**
  æ‰§è¡Œè®¡ç®—çš„èƒ½åŠ›ï¼Œé€šå¸¸æŒ‡ç”¨äºè®­ç»ƒæˆ–æ¨ç† AI æ¨¡å‹çš„è®¡ç®—èµ„æºï¼ˆå¦‚ GPU/TPU å°æ—¶æ•°ï¼ŒFLOPSï¼‰ã€‚ (è§ 4.3)
* **Compute-Optimal (è®¡ç®—æœ€ä¼˜):**
  æŒ‡åœ¨ç»™å®šçš„æ€»è®¡ç®—é¢„ç®—ä¸‹ï¼Œé€šè¿‡åˆç†åˆ†é…æ¨¡å‹å‚æ•°é‡ (N) å’Œè®­ç»ƒæ•°æ®é‡ (D) ä»¥è¾¾åˆ°æœ€ä½³æ€§èƒ½çš„çŠ¶æ€ã€‚ (è§ 4.2.2, Chinchilla Scaling Laws)
* **Context Window (ä¸Šä¸‹æ–‡çª—å£):**
  æ¨¡å‹åœ¨è¿›è¡Œé¢„æµ‹æ—¶èƒ½å¤Ÿè€ƒè™‘çš„è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼ˆä»¥ Token æ•°é‡è¡¡é‡ï¼‰ã€‚ (è§ 8.2.2, 11.2.2, 12.5)
* **Contextual Embedding (ä¸Šä¸‹æ–‡åµŒå…¥):**
  ä¸ Word2Vec ç­‰é™æ€è¯åµŒå…¥ä¸åŒï¼Œä¸Šä¸‹æ–‡åµŒå…¥ï¼ˆå¦‚ BERT æˆ– GPT çš„è¾“å‡ºï¼‰ä¼šæ ¹æ®è¯è¯­æ‰€å¤„çš„å…·ä½“è¯­å¢ƒè€Œå˜åŒ–ï¼Œèƒ½æ›´å¥½åœ°åŒºåˆ†è¯ä¹‰ã€‚
* **Contrastive Learning (å¯¹æ¯”å­¦ä¹ ):**
  ä¸€ç§è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œé€šè¿‡æ‹‰è¿‘ç›¸ä¼¼æ ·æœ¬ï¼ˆæ­£æ ·æœ¬å¯¹ï¼‰çš„è¡¨ç¤ºã€æ¨è¿œä¸ç›¸ä¼¼æ ·æœ¬ï¼ˆè´Ÿæ ·æœ¬å¯¹ï¼‰çš„è¡¨ç¤ºæ¥å­¦ä¹ ç‰¹å¾ã€‚CLIP ä½¿ç”¨äº†å¯¹æ¯”å­¦ä¹ ã€‚ (è§ 12.1.2)
* **Cosine Similarity (ä½™å¼¦ç›¸ä¼¼åº¦):**
  è¡¡é‡ä¸¤ä¸ªå‘é‡åœ¨æ–¹å‘ä¸Šç›¸ä¼¼ç¨‹åº¦çš„æŒ‡æ ‡ï¼Œè®¡ç®—å®ƒä»¬å¤¹è§’çš„ä½™å¼¦å€¼ã€‚å¸¸ç”¨äºè¡¡é‡åµŒå…¥å‘é‡çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ (è§ 10.8)
* **CoT (Chain-of-Thought):**
  è§ Chain-of-Thoughtã€‚
* **Cross-Entropy Loss (äº¤å‰ç†µæŸå¤±):**
  å¸¸ç”¨äºåˆ†ç±»ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒä¸çœŸå®æ ‡ç­¾åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚æ˜¯ MLM å’Œ CLM é¢„è®­ç»ƒçš„ä¸»è¦æŸå¤±å‡½æ•°ã€‚ (è§ 2.1.2, A.4)
* **CUDA (Compute Unified Device Architecture):**
  NVIDIA å¼€å‘çš„å¹¶è¡Œè®¡ç®—å¹³å°å’Œç¼–ç¨‹æ¨¡å‹ï¼Œå…è®¸å¼€å‘è€…ä½¿ç”¨ C++/Python ç­‰è¯­è¨€åˆ©ç”¨ NVIDIA GPU è¿›è¡Œé€šç”¨è®¡ç®—ã€‚æ˜¯ PyTorch ç­‰æ¡†æ¶åœ¨ NVIDIA GPU ä¸Šè¿è¡Œçš„åŸºç¡€ã€‚ (è§ 2.3.3)
* **Data Augmentation (æ•°æ®å¢å¼º):**
  é€šè¿‡å¯¹ç°æœ‰æ•°æ®è¿›è¡Œå˜æ¢ï¼ˆå¦‚æ–‡æœ¬æ›¿æ¢ã€å›è¯‘ï¼‰æ¥æ‰©å……è®­ç»ƒæ•°æ®é›†çš„æŠ€æœ¯ï¼Œç”¨äºæé«˜æ¨¡å‹é²æ£’æ€§æˆ–å¤„ç†æ•°æ®ä¸å¹³è¡¡ã€‚
* **Data Parallelism (DP, æ•°æ®å¹¶è¡Œ):**
  ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå°†æ¨¡å‹å‰¯æœ¬å¤åˆ¶åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œæ¯ä¸ªè®¾å¤‡å¤„ç†ä¸åŒæ‰¹æ¬¡çš„æ•°æ®ï¼Œç„¶åèšåˆæ¢¯åº¦è¿›è¡Œæ›´æ–°ã€‚ (è§ 4.4.1)
* **Dataset (æ•°æ®é›†):**
  ç”¨äºè®­ç»ƒã€éªŒè¯æˆ–æµ‹è¯•æ¨¡å‹çš„ç»“æ„åŒ–æ•°æ®é›†åˆã€‚
* **DDP (Distributed Data Parallel):**
  PyTorch ä¸­æ¨èçš„ã€é«˜æ•ˆçš„æ•°æ®å¹¶è¡Œå®ç°ï¼ŒåŸºäºå¤šè¿›ç¨‹å’Œ NCCL ç­‰åç«¯ã€‚ (è§ 4.4.1)
* **Decoder (è§£ç å™¨):**
  Transformer æ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ç”¨äºç”Ÿæˆç›®æ ‡åºåˆ—ã€‚åŒ…å«å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›ã€ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚ (è§ 3.6)
* **Decoder-Only Architecture (ä»…è§£ç å™¨æ¶æ„):**
  åªä½¿ç”¨ Transformer è§£ç å™¨éƒ¨åˆ†çš„æ¨¡å‹æ¶æ„ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼Œæ“…é•¿æ–‡æœ¬ç”Ÿæˆã€‚ (è§ 3.8.3)
* **Deep Learning (æ·±åº¦å­¦ä¹ ):**
  æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨åŒ…å«å¤šä¸ªå¤„ç†å±‚ï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰çš„æ¨¡å‹æ¥å­¦ä¹ æ•°æ®çš„åˆ†å±‚è¡¨ç¤ºã€‚ (è§ 2.1)
* **DeepSpeed:**
  å¾®è½¯å¼€å‘çš„ç”¨äºå¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒçš„ä¼˜åŒ–åº“ï¼ŒåŒ…å« ZeRO, MoE ç­‰æŠ€æœ¯ã€‚ (è§ 4.4.5, 4.5.1)
* **Diffusion Model (æ‰©æ•£æ¨¡å‹):**
  ä¸€ç§ç”Ÿæˆæ¨¡å‹ï¼Œé€šè¿‡æ¨¡æ‹Ÿä»å™ªå£°æ•°æ®é€æ­¥å»å™ªçš„è¿‡ç¨‹æ¥ç”Ÿæˆæ•°æ®ã€‚åœ¨å›¾åƒç”Ÿæˆï¼ˆå¦‚ Stable Diffusion, DALL-E 2ï¼‰ç­‰é¢†åŸŸéå¸¸æˆåŠŸã€‚ (è§ 12.1.3)
* **Distributed Training (åˆ†å¸ƒå¼è®­ç»ƒ):**
  ä½¿ç”¨å¤šä¸ªè®¡ç®—è®¾å¤‡ï¼ˆGPU/TPUï¼‰æˆ–å¤šå°æœºå™¨ï¼ˆèŠ‚ç‚¹ï¼‰ååŒè®­ç»ƒä¸€ä¸ªæ¨¡å‹çš„æŠ€æœ¯ï¼Œç”¨äºå¤„ç†å¤§è§„æ¨¡æ¨¡å‹å’Œæ•°æ®ã€‚ (è§ 4.3.2, 4.4)
* **DPO (Direct Preference Optimization):**
  ä¸€ç§å¯¹é½æ–¹æ³•ï¼Œç›´æ¥ä½¿ç”¨äººç±»åå¥½æ•°æ®ä¼˜åŒ–è¯­è¨€æ¨¡å‹ç­–ç•¥ï¼Œæ— éœ€æ˜¾å¼è®­ç»ƒå¥–åŠ±æ¨¡å‹æˆ–ä½¿ç”¨ RLã€‚ (è§ 7.3.2)
* **Dropout:**
  ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºå°†ä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºç½®é›¶ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ (è§ 2.1.4)
* **Embedding (åµŒå…¥):**
  å°†ç¦»æ•£çš„è¾“å…¥ï¼ˆå¦‚å•è¯ã€Tokenï¼‰æˆ–æ•´ä¸ªæ•°æ®ç‚¹ï¼ˆå¦‚å¥å­ã€å›¾åƒï¼‰æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´ã€ç¨ å¯†çš„è¿ç»­å‘é‡ç©ºé—´ä¸­çš„è¡¨ç¤ºã€‚ (è§ 1.2.2, 2.2.1, 10.8)
* **Embodied AI (å…·èº«æ™ºèƒ½):**
  èƒ½å¤Ÿé€šè¿‡ç‰©ç†å®ä½“ï¼ˆå¦‚æœºå™¨äººï¼‰ä¸ç‰©ç†ä¸–ç•Œè¿›è¡Œäº¤äº’ã€æ„ŸçŸ¥å’Œè¡ŒåŠ¨çš„ AI ç³»ç»Ÿã€‚ (è§ 13.4.2)
* **Emergent Abilities (æ¶Œç°èƒ½åŠ›):**
  é‚£äº›åœ¨å°å‹æ¨¡å‹ä¸Šä¸æ˜æ˜¾æˆ–ä¸å­˜åœ¨ï¼Œä½†å½“æ¨¡å‹è§„æ¨¡å¢å¤§åˆ°ä¸€å®šç¨‹åº¦åçªç„¶å‡ºç°å¹¶æ˜¾è‘—æå‡çš„èƒ½åŠ›ï¼ˆå¦‚ä¸Šä¸‹æ–‡å­¦ä¹ ã€æ€ç»´é“¾æ¨ç†ï¼‰ã€‚ (è§ 1.1)
* **Encoder (ç¼–ç å™¨):**
  Transformer æ¶æ„çš„ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸ç”¨äºå¤„ç†è¾“å…¥åºåˆ—å¹¶ç”Ÿæˆä¸Šä¸‹æ–‡è¡¨ç¤ºã€‚åŒ…å«è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œã€‚ (è§ 3.5)
* **Encoder-Decoder Architecture (ç¼–ç å™¨-è§£ç å™¨æ¶æ„):**
  åŒ…å«å®Œæ•´ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆçš„æ¨¡å‹æ¶æ„ï¼ˆå¦‚ T5, BARTï¼‰ï¼Œé€‚ç”¨äºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ã€‚ (è§ 3.7, 3.8.3)
* **Encoder-Only Architecture (ä»…ç¼–ç å™¨æ¶æ„):**
  åªä½¿ç”¨ Transformer ç¼–ç å™¨éƒ¨åˆ†çš„æ¨¡å‹æ¶æ„ï¼ˆå¦‚ BERTï¼‰ï¼Œæ“…é•¿è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ã€‚ (è§ 3.8.1)
* **Epoch (è½®æ¬¡):**
  åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•´ä¸ªè®­ç»ƒæ•°æ®é›†è¢«å®Œæ•´åœ°éå†ä¸€éã€‚
* **Evaluate (åº“):**
  Hugging Face å¼€å‘çš„ç”¨äºåŠ è½½å’Œè®¡ç®—æœºå™¨å­¦ä¹ è¯„ä¼°æŒ‡æ ‡çš„åº“ã€‚ (è§ é™„å½• C.5)
* **Evaluation (è¯„ä¼°):**
  è¡¡é‡æ¨¡å‹æ€§èƒ½ã€èƒ½åŠ›å’Œå±€é™æ€§çš„è¿‡ç¨‹ã€‚ (è§ ç¬¬ 9 ç« )
* **FAISS (Facebook AI Similarity Search):**
  ä¸€ä¸ªç”¨äºé«˜æ•ˆå‘é‡ç›¸ä¼¼åº¦æœç´¢å’Œèšç±»çš„åº“ã€‚ (è§ 11.3.2)
* **Fairness (å…¬å¹³æ€§):**
  æŒ‡ AI ç³»ç»Ÿåœ¨ä¸åŒç¾¤ä½“ä¹‹é—´ä¸åº”è¡¨ç°å‡ºä¸åˆç†çš„åè§æˆ–æ­§è§†ã€‚ (è§ 13.2.1)
* **Few-shot Learning (å°‘æ ·æœ¬å­¦ä¹ ):**
  æ¨¡å‹ä»…é€šè¿‡åœ¨æç¤ºä¸­æä¾›å°‘é‡ä»»åŠ¡ç¤ºä¾‹å°±èƒ½æ‰§è¡Œæ–°ä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ˜¯ä¸Šä¸‹æ–‡å­¦ä¹  (ICL) çš„ä¸€ç§å½¢å¼ã€‚ (è§ 1.1, 8.2.2)
* **FFN (Feed-Forward Network):**
  è§ Position-wise Feed-Forward Networkã€‚
* **Fine-tuning (å¾®è°ƒ):**
  åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨ä»»åŠ¡ç›¸å…³çš„æ ‡æ³¨æ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒä»¥é€‚åº”ç‰¹å®šä»»åŠ¡çš„è¿‡ç¨‹ã€‚ (è§ ç¬¬ 6 ç« )
* **FLAN (Fine-tuned Language Net):**
  Google æå‡ºçš„é€šè¿‡åœ¨å¤§é‡æŒ‡ä»¤æ ¼å¼çš„ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒæ¥æå‡æ¨¡å‹æŒ‡ä»¤éµå¾ªå’Œæ³›åŒ–èƒ½åŠ›çš„æ–¹æ³•/æ¨¡å‹ç³»åˆ—ã€‚ (è§ 6.4.2)
* **FLOPS (Floating Point Operations Per Second):**
  æ¯ç§’æµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼Œè¡¡é‡è®¡ç®—è®¾å¤‡æ€§èƒ½çš„å¸¸ç”¨æŒ‡æ ‡ã€‚è®­ç»ƒå¤§æ¨¡å‹é€šå¸¸éœ€è¦ PetaFLOPS (10^15) ç”šè‡³ ExaFLOPS (10^18) çº§åˆ«çš„ç®—åŠ›ã€‚
* **Foundation Model (åŸºç¡€æ¨¡å‹):**
  ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œèƒ½å¤Ÿé€‚åº”å¤šç§ä¸‹æ¸¸ä»»åŠ¡çš„æ¨¡å‹ã€‚ (è§ 1.1)
* **FSDP (Fully Sharded Data Parallel):**
  PyTorch å®˜æ–¹æä¾›çš„åˆ†å¸ƒå¼è®­ç»ƒæ–¹æ¡ˆï¼Œç±»ä¼¼ ZeRO Stage 3ï¼Œå°†æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€éƒ½åˆ†ç‰‡ã€‚ (è§ 4.4.5, 4.5.3)
* **Function Calling / Tool Use (å‡½æ•°è°ƒç”¨/å·¥å…·ä½¿ç”¨):**
  è®© LLM èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨ API æˆ–å·¥å…·æ¥è·å–ä¿¡æ¯æˆ–æ‰§è¡ŒåŠ¨ä½œçš„èƒ½åŠ›ã€‚ (è§ 12.3.2)
* **GeLU (Gaussian Error Linear Unit):**
  ä¸€ç§å¸¸ç”¨çš„å¹³æ»‘æ¿€æ´»å‡½æ•°ï¼Œåœ¨ Transformer æ¨¡å‹ä¸­å¹¿æ³›ä½¿ç”¨ã€‚ (è§ 2.1.3)
* **Generalization (æ³›åŒ–):**
  æ¨¡å‹åœ¨è®­ç»ƒæ—¶æœªè§è¿‡çš„æ–°æ•°æ®ä¸Šçš„è¡¨ç°èƒ½åŠ›ã€‚
* **Generative AI (ç”Ÿæˆå¼ AI):**
  èƒ½å¤Ÿåˆ›å»ºæ–°çš„ã€åŸåˆ›å†…å®¹ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€ä»£ç ï¼‰çš„äººå·¥æ™ºèƒ½ã€‚å¤§è¯­è¨€æ¨¡å‹æ˜¯ç”Ÿæˆå¼ AI çš„æ ¸å¿ƒã€‚
* **GPU (Graphics Processing Unit):**
  å›¾å½¢å¤„ç†å•å…ƒï¼Œå› å…¶å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—èƒ½åŠ›è€Œè¢«å¹¿æ³›ç”¨äºåŠ é€Ÿæ·±åº¦å­¦ä¹ è®­ç»ƒå’Œæ¨ç†ã€‚ (è§ 4.3.1)
* **GPT (Generative Pre-trained Transformer):**
  OpenAI å¼€å‘çš„åŸºäº Transformer Decoder çš„ä¸€ç³»åˆ—å¼ºå¤§çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ã€‚ (è§ 1.2.6, 3.8.3)
* **Gradient (æ¢¯åº¦):**
  å¤šå…ƒå‡½æ•°åœ¨æŸç‚¹ç›¸å¯¹äºæ‰€æœ‰å˜é‡çš„åå¯¼æ•°ç»„æˆçš„å‘é‡ï¼ŒæŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘ã€‚ (è§ 2.1.1, A.2)
* **Gradient Accumulation (æ¢¯åº¦ç´¯ç§¯):**
  ä¸€ç§æŠ€æœ¯ï¼Œé€šè¿‡ç´¯ç§¯å¤šä¸ªå¾®æ‰¹æ¬¡çš„æ¢¯åº¦å†è¿›è¡Œä¸€æ¬¡å‚æ•°æ›´æ–°ï¼Œæ¥æ¨¡æ‹Ÿæ›´å¤§çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼Œä»¥å…‹æœæ˜¾å­˜é™åˆ¶ã€‚ (è§ 5.3.5)
* **Gradient Clipping (æ¢¯åº¦è£å‰ª):**
  ä¸€ç§é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„æŠ€æœ¯ï¼Œå°†æ¢¯åº¦çš„èŒƒæ•°é™åˆ¶åœ¨ä¸€ä¸ªé˜ˆå€¼å†…ã€‚ (è§ 5.3.3)
* **Hallucination (å¹»è§‰):**
  æ¨¡å‹ç”Ÿæˆçœ‹ä¼¼åˆç†ä½†å®é™…ä¸Šæ˜¯è™šå‡æˆ–æé€ çš„ä¿¡æ¯çš„ç°è±¡ã€‚ (è§ 7.1, 13.1.1)
* **HHH (Helpful, Honest, Harmless):**
  å¸¸ç”¨äºæè¿° AI å¯¹é½ç›®æ ‡çš„ä¸‰ä¸ªåŸåˆ™ï¼šæœ‰ç”¨ã€è¯šå®ã€æ— å®³ã€‚ (è§ 7.1)
* **Hyperparameter (è¶…å‚æ•°):**
  åœ¨è®­ç»ƒå¼€å§‹å‰è®¾ç½®çš„å‚æ•°ï¼Œä¸ç”±æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ å¾—åˆ°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€å±‚æ•°ã€éšè—ç»´åº¦ç­‰ã€‚éœ€è¦æ‰‹åŠ¨è°ƒæ•´æˆ–é€šè¿‡æœç´¢ç¡®å®šã€‚
* **ICL (In-context Learning):**
  è§ Context Learningã€‚
* **In-context Learning (ICL, ä¸Šä¸‹æ–‡å­¦ä¹ ):**
  å¤§æ¨¡å‹é€šè¿‡åœ¨æç¤º (Prompt) ä¸­æ¥æ”¶ä»»åŠ¡æè¿°å’Œå°‘é‡ç¤ºä¾‹ï¼Œè€Œæ— éœ€æ›´æ–°æ¨¡å‹å‚æ•°å°±èƒ½æ‰§è¡Œæ–°ä»»åŠ¡çš„èƒ½åŠ›ã€‚Few-shot Learning æ˜¯å…¶ä¸€ç§å½¢å¼ã€‚ (è§ 1.1, 8.2.2)
* **Instruction Following (æŒ‡ä»¤éµå¾ª):**
  æ¨¡å‹ç†è§£å¹¶æ‰§è¡Œè‡ªç„¶è¯­è¨€æŒ‡ä»¤çš„èƒ½åŠ›ã€‚ (è§ 1.1, 6.4)
* **Instruction Tuning (æŒ‡ä»¤å¾®è°ƒ):**
  ä¸€ç§å¾®è°ƒèŒƒå¼ï¼Œé€šè¿‡åœ¨åŒ…å«å¤§é‡ä¸åŒä»»åŠ¡æŒ‡ä»¤çš„æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæå‡æ¨¡å‹éµå¾ªæŒ‡ä»¤å’Œé›¶æ ·æœ¬æ³›åŒ–çš„èƒ½åŠ›ã€‚ (è§ 6.4)
* **INT8 / INT4:**
  8 ä½ / 4 ä½æ•´æ•°é‡åŒ–ï¼Œç”¨äºæ¨¡å‹å‹ç¼©ã€‚ (è§ 12.2.1)
* **Interpretability (å¯è§£é‡Šæ€§):**
  ç†è§£æ¨¡å‹ä¸ºä½•åšå‡ºç‰¹å®šé¢„æµ‹æˆ–å†³ç­–çš„èƒ½åŠ›ã€‚å¤§æ¨¡å‹é€šå¸¸ç¼ºä¹è‰¯å¥½çš„å¯è§£é‡Šæ€§ï¼ˆâ€œé»‘ç®±â€é—®é¢˜ï¼‰ã€‚ (è§ 13.2.5)
* **Knowledge Cutoff (çŸ¥è¯†æˆªæ­¢æ—¥æœŸ):**
  é¢„è®­ç»ƒæ¨¡å‹åŒ…å«çš„çŸ¥è¯†æ‰€æˆªæ­¢çš„æ—¶é—´ç‚¹ï¼Œé€šå¸¸æ˜¯å…¶è®­ç»ƒæ•°æ®çš„æœ€åæ”¶é›†æ—¥æœŸã€‚ (è§ 13.1.2)
* **Knowledge Distillation (KD, çŸ¥è¯†è’¸é¦):**
  ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œä½¿ç”¨å¤§å‹æ•™å¸ˆæ¨¡å‹æŒ‡å¯¼å°å‹å­¦ç”Ÿæ¨¡å‹çš„è®­ç»ƒã€‚ (è§ 12.2.1)
* **KL Divergence (KL æ•£åº¦):**
  è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æŒ‡æ ‡ã€‚åœ¨ RLHF ä¸­ç”¨äºæƒ©ç½šç­–ç•¥æ¨¡å‹åç¦» SFT æ¨¡å‹ã€‚ (è§ 7.2.2, A.4)
* **LangChain:**
  ä¸€ä¸ªæµè¡Œçš„ç”¨äºæ„å»º LLM åº”ç”¨çš„å¼€æºæ¡†æ¶ã€‚ (è§ 11.2.2, é™„å½• D.4)
* **Large Language Model (LLM, å¤§è¯­è¨€æ¨¡å‹):**
  å‚æ•°è§„æ¨¡å·¨å¤§ï¼ˆé€šå¸¸æ•°åäº¿ä»¥ä¸Šï¼‰çš„ã€åŸºäº Transformer æ¶æ„çš„ã€åœ¨æµ·é‡æ–‡æœ¬æ•°æ®ä¸Šé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚
* **Layer Normalization (LayerNorm, LN, å±‚å½’ä¸€åŒ–):**
  ä¸€ç§å½’ä¸€åŒ–æŠ€æœ¯ï¼Œåœ¨å•ä¸ªæ ·æœ¬çš„å±‚å†…å¯¹æ‰€æœ‰ç¥ç»å…ƒæ¿€æ´»å€¼è¿›è¡Œå½’ä¸€åŒ–ã€‚Transformer ä¸­å¹¿æ³›ä½¿ç”¨ã€‚ (è§ 2.1.4)
* **Learning Rate (LR, å­¦ä¹ ç‡):**
  ä¼˜åŒ–ç®—æ³•ï¼ˆå¦‚ SGD, Adamï¼‰ä¸­æ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿çš„è¶…å‚æ•°ã€‚ (è§ 2.1.2, 5.3.3)
* **Learning Rate Schedule (å­¦ä¹ ç‡è°ƒåº¦):**
  åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡çš„ç­–ç•¥ï¼Œå¦‚å¸¦é¢„çƒ­çš„çº¿æ€§/ä½™å¼¦è¡°å‡ã€‚ (è§ 5.3.3)
* **LLaMA:**
  Meta AI å¼€å‘çš„ç³»åˆ—å¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚ (è§ 1.1, 4.2.2)
* **LLM (Large Language Model):**
  è§ Large Language Modelã€‚
* **LlamaIndex:**
  ä¸€ä¸ªä¸“æ³¨äºå°† LLM ä¸å¤–éƒ¨æ•°æ®è¿æ¥ï¼ˆRAGï¼‰çš„å¼€æºæ¡†æ¶ã€‚ (è§ 11.2.2, é™„å½• D.4)
* **LM Head (Language Model Head):**
  åœ¨è¯­è¨€æ¨¡å‹ï¼ˆç‰¹åˆ«æ˜¯ Decoder-Only æ¨¡å‹ï¼‰é¡¶éƒ¨ç”¨äºé¢„æµ‹è¯æ±‡è¡¨æ¦‚ç‡åˆ†å¸ƒçš„è¾“å‡ºå±‚ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼‰ã€‚ (è§ 6.2.2)
* **LoRA (Low-Rank Adaptation):**
  ä¸€ç§æµè¡Œçš„å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT) æ–¹æ³•ï¼Œé€šè¿‡å­¦ä¹ æƒé‡æ›´æ–°çŸ©é˜µçš„ä½ç§©åˆ†è§£æ¥è¿›è¡Œå¾®è°ƒã€‚ (è§ 6.3.3)
* **Loss Function (æŸå¤±å‡½æ•°):**
  è¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´å·®è·çš„å‡½æ•°ï¼Œè®­ç»ƒçš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±ã€‚ (è§ 2.1.2)
* **Masked Language Modeling (MLM, æ©ç è¯­è¨€æ¨¡å‹):**
  ä¸€ç§é¢„è®­ç»ƒä»»åŠ¡ï¼Œéšæœºé®ç›–è¾“å…¥åºåˆ—ä¸­çš„ä¸€äº› Tokenï¼Œè®©æ¨¡å‹é¢„æµ‹è¢«é®ç›–çš„ Tokenã€‚BERT ä½¿ç”¨çš„ä¸»è¦é¢„è®­ç»ƒä»»åŠ¡ã€‚ (è§ 5.2.1)
* **Masking (æ©ç ):**
  åœ¨ Transformer ä¸­ç”¨äºå¿½ç•¥æŸäº›ä½ç½®ä¿¡æ¯çš„æŠ€æœ¯ã€‚å¸¸è§çš„æœ‰ Padding Maskï¼ˆå¿½ç•¥å¡«å……ä½ï¼‰å’Œ Sequence/Future Maskï¼ˆå¿½ç•¥æœªæ¥ä½ï¼Œç”¨äºè§£ç å™¨è‡ªæ³¨æ„åŠ›ï¼‰ã€‚ (è§ 3.6.2)
* **Megatron-LM:**
  NVIDIA å¼€å‘çš„ç”¨äºå®ç°é«˜æ•ˆå¼ é‡å¹¶è¡Œå’Œæµæ°´çº¿å¹¶è¡Œçš„åº“/æ¡†æ¶ã€‚ (è§ 4.4.2, 4.5.2)
* **Memory (æ˜¾å­˜):**
  æŒ‡ GPU ç­‰åŠ é€Ÿå™¨ä¸Šçš„é«˜é€Ÿå†…å­˜ (VRAM)ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹å‚æ•°ã€æ¿€æ´»å€¼ã€æ¢¯åº¦ç­‰ã€‚æ˜¯è®­ç»ƒå¤§æ¨¡å‹çš„å…³é”®ç“¶é¢ˆèµ„æºã€‚ (è§ 4.3.1)
* **Micro-batch Size:**
  åœ¨æ•°æ®å¹¶è¡Œæˆ–æµæ°´çº¿å¹¶è¡Œä¸­ï¼Œæ¯ä¸ªè®¾å¤‡å•æ¬¡å¤„ç†çš„æ•°æ®é‡ã€‚æ¢¯åº¦ç´¯ç§¯æˆ–æµæ°´çº¿é˜¶æ®µå¤„ç†çš„åŸºæœ¬å•ä½ã€‚
* **Mini-batch Size:**
  åœ¨ä¸€æ¬¡ä¼˜åŒ–å™¨æ›´æ–° (step) ä¸­å¤„ç†çš„æ€»æ•°æ®é‡ï¼ˆå¯èƒ½ç”±å¤šä¸ª Micro-batch ç´¯ç§¯è€Œæˆï¼‰ã€‚
* **Misinformation (è™šå‡ä¿¡æ¯):**
  é”™è¯¯æˆ–ä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œå¤§æ¨¡å‹å¯èƒ½ç”Ÿæˆæˆ–è¢«ç”¨äºä¼ æ’­è™šå‡ä¿¡æ¯ã€‚ (è§ 13.2.2)
* **Mixed Precision Training (æ··åˆç²¾åº¦è®­ç»ƒ):**
  åœ¨è®­ç»ƒä¸­ä½¿ç”¨è¾ƒä½ç²¾åº¦ï¼ˆå¦‚ FP16, BF16ï¼‰è¿›è¡Œå¤§éƒ¨åˆ†è®¡ç®—ï¼ŒåŒæ—¶ä¿æŒéƒ¨åˆ†è®¡ç®—ï¼ˆå¦‚å‚æ•°æ›´æ–°ï¼‰ä½¿ç”¨ FP32ï¼Œä»¥åŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘å†…å­˜å ç”¨ã€‚ (è§ 5.3.4)
* **MLM (Masked Language Model):**
  è§ Masked Language Modelingã€‚
* **MMLU (Massive Multitask Language Understanding):**
  ä¸€ä¸ªåŒ…å« 57 ä¸ªå­¦ç§‘é€‰æ‹©é¢˜ä»»åŠ¡çš„å¤§è§„æ¨¡åŸºå‡†ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ã€‚ (è§ 9.3.2)
* **MoE (Mixture-of-Experts, æ··åˆä¸“å®¶æ¨¡å‹):**
  ä¸€ç§æ¨¡å‹æ¶æ„ï¼ŒåŒ…å«å¤šä¸ªâ€œä¸“å®¶â€ï¼ˆé€šå¸¸æ˜¯ FFNï¼‰ï¼Œé€šè¿‡é—¨æ§ç½‘ç»œä¸ºæ¯ä¸ªè¾“å…¥ Token åŠ¨æ€é€‰æ‹©å°‘æ•°å‡ ä¸ªä¸“å®¶è¿›è¡Œè®¡ç®—ï¼Œä»¥ç¨€ç–æ¿€æ´»çš„æ–¹å¼æ‰©å±•æ¨¡å‹å®¹é‡ã€‚ (è§ 12.2.2)
* **Model Parallelism (MP, æ¨¡å‹å¹¶è¡Œ):**
  ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå°†å•ä¸ªæ¨¡å‹çš„ä¸åŒéƒ¨åˆ†ï¼ˆå±‚æˆ–å±‚å†…è®¡ç®—ï¼‰åˆ†å‰²åˆ°ä¸åŒè®¾å¤‡ä¸Šã€‚ (è§ 4.4.2)
* **Multimodal Large Model (MLLM / LMM, å¤šæ¨¡æ€å¤§æ¨¡å‹):**
  èƒ½å¤Ÿå¤„ç†å’Œç†è§£æ¥è‡ªå¤šç§æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰ä¿¡æ¯çš„å¤§æ¨¡å‹ã€‚ (è§ 12.1)
* **Multi-Head Attention (å¤šå¤´æ³¨æ„åŠ›):**
  Transformer ä¸­çš„ä¸€ç§æœºåˆ¶ï¼Œå¹¶è¡Œåœ°è¿è¡Œå¤šä¸ªè‡ªæ³¨æ„åŠ›â€œå¤´â€ï¼Œå…è®¸æ¨¡å‹åŒæ—¶å…³æ³¨æ¥è‡ªä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ã€‚ (è§ 3.3.3)
* **Natural Language Processing (NLP, è‡ªç„¶è¯­è¨€å¤„ç†):**
  äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®©è®¡ç®—æœºç†è§£ã€å¤„ç†å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
* **NCCL (NVIDIA Collective Communications Library):**
  NVIDIA å¼€å‘çš„ç”¨äºåœ¨å¤šä¸ª GPU ä¹‹é—´è¿›è¡Œé«˜æ•ˆé›†ä½“é€šä¿¡ï¼ˆå¦‚ All-Reduce, Broadcastï¼‰çš„åº“ï¼Œæ˜¯ PyTorch DDP åœ¨ NVIDIA GPU ä¸Šçš„å¸¸ç”¨åç«¯ã€‚ (è§ 4.4.1)
* **Neural Network (ç¥ç»ç½‘ç»œ):**
  å—ç”Ÿç‰©ç¥ç»ç³»ç»Ÿå¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ï¼ˆç¥ç»å…ƒï¼‰ç»„æˆï¼Œç”¨äºå­¦ä¹ æ•°æ®ä¸­çš„å¤æ‚æ¨¡å¼ã€‚ (è§ 2.1.1)
* **NLG (Natural Language Generation, è‡ªç„¶è¯­è¨€ç”Ÿæˆ):**
  è®©è®¡ç®—æœºç”Ÿæˆè‡ªç„¶è¯­è¨€æ–‡æœ¬çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ã€æœºå™¨ç¿»è¯‘ã€å¯¹è¯ã€‚
* **NLU (Natural Language Understanding, è‡ªç„¶è¯­è¨€ç†è§£):**
  è®©è®¡ç®—æœºç†è§£è‡ªç„¶è¯­è¨€å«ä¹‰çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€‚
* **Norm (èŒƒæ•°):**
  è§ A.1ã€‚
* **Normalization (å½’ä¸€åŒ–):**
  å°†æ•°æ®è°ƒæ•´åˆ°ç‰¹å®šèŒƒå›´æˆ–åˆ†å¸ƒçš„è¿‡ç¨‹ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚å¸¸è§çš„æœ‰ Layer Normalization, Batch Normalizationã€‚ (è§ 2.1.4)
* **Optimizer (ä¼˜åŒ–å™¨):**
  åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°çš„ç®—æ³•ï¼Œå¦‚ SGD, Adam, AdamWã€‚ (è§ 2.1.2)
* **Overfitting (è¿‡æ‹Ÿåˆ):**
  æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æœªè§è¿‡çš„æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°å·®çš„ç°è±¡ï¼Œå³æ³›åŒ–èƒ½åŠ›å·®ã€‚ (è§ 2.1.4)
* **Padding (å¡«å……):**
  å°†åŒä¸€æ‰¹æ¬¡ä¸­ä¸åŒé•¿åº¦çš„åºåˆ—é€šè¿‡æ·»åŠ ç‰¹æ®Š `[PAD]` Token å¡«å……åˆ°ç›¸åŒé•¿åº¦ï¼Œä»¥ä¾¿è¿›è¡Œæ‰¹å¤„ç†è®¡ç®—ã€‚ (è§ 2.2.4)
* **Padding Mask (å¡«å……æ©ç ):**
  ç”¨äºæŒ‡ç¤ºæ¨¡å‹åœ¨è®¡ç®—ï¼ˆå¦‚æ³¨æ„åŠ›ï¼‰æ—¶å¿½ç•¥å¡«å……ä½ç½®çš„æ©ç ã€‚ (è§ 3.6.2)
* **Parameter (å‚æ•°):**
  æ¨¡å‹ä¸­å¯å­¦ä¹ çš„å˜é‡ï¼ˆä¸»è¦æ˜¯ç¥ç»ç½‘ç»œçš„æƒé‡å’Œåç½®ï¼‰ï¼Œæ¨¡å‹é€šè¿‡è®­ç»ƒæ•°æ®è°ƒæ•´è¿™äº›å‚æ•°æ¥å­¦ä¹ ã€‚ (è§ 1.1, 4.2)
* **Parameter-Efficient Fine-tuning (PEFT, å‚æ•°é«˜æ•ˆå¾®è°ƒ):**
  ä¸€ç±»å¾®è°ƒæ–¹æ³•ï¼Œåªè®­ç»ƒæ¨¡å‹å‚æ•°çš„ä¸€å°éƒ¨åˆ†ï¼ˆæˆ–å¼•å…¥å°‘é‡é¢å¤–å‚æ•°ï¼‰ï¼ŒåŒæ—¶å†»ç»“å¤§éƒ¨åˆ†é¢„è®­ç»ƒå‚æ•°ï¼Œä»¥é™ä½å¾®è°ƒæˆæœ¬ã€‚ (è§ 6.3)
* **PEFT (Parameter-Efficient Fine-tuning):**
  è§ Parameter-Efficient Fine-tuningã€‚
* **Perplexity (PPL, å›°æƒ‘åº¦):**
  è¡¡é‡è¯­è¨€æ¨¡å‹é¢„æµ‹èƒ½åŠ›çš„æŒ‡æ ‡ï¼Œæ˜¯äº¤å‰ç†µæŸå¤±çš„æŒ‡æ•°ã€‚å€¼è¶Šä½è¶Šå¥½ã€‚ (è§ 9.2.1)
* **Pipeline Parallelism (PP, æµæ°´çº¿å¹¶è¡Œ):**
  ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥ï¼Œå°†æ¨¡å‹çš„ä¸åŒå±‚ï¼ˆStageï¼‰åˆ†å¸ƒåˆ°ä¸åŒè®¾å¤‡ä¸Šï¼Œå¹¶ä½¿ç”¨å¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰ä»¥æµæ°´çº¿æ–¹å¼å¤„ç†æ•°æ®ï¼Œä»¥æé«˜å±‚é—´å¹¶è¡Œæ•ˆç‡ã€‚ (è§ 4.4.3)
* **Position-wise Feed-Forward Network (FFN, ä½ç½®ç›¸å…³å‰é¦ˆç½‘ç»œ):**
  Transformer å±‚ä¸­çš„ä¸€ä¸ªå­å±‚ï¼ŒåŒ…å«ä¸¤ä¸ªçº¿æ€§å˜æ¢å’Œä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œç‹¬ç«‹åœ°ä½œç”¨äºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ã€‚ (è§ 3.5)
* **Positional Encoding (PE, ä½ç½®ç¼–ç ):**
  å‘ Transformer æ¨¡å‹æ³¨å…¥åºåˆ—ä¸­å…ƒç´ ä½ç½®ä¿¡æ¯çš„æ–¹æ³•ï¼Œå› ä¸ºè‡ªæ³¨æ„åŠ›æœ¬èº«ä¸æ„ŸçŸ¥é¡ºåºã€‚å¯ä»¥æ˜¯å›ºå®šçš„ï¼ˆå¦‚æ­£å¼¦/ä½™å¼¦ï¼‰æˆ–å¯å­¦ä¹ çš„ã€‚ (è§ 3.4)
* **PPO (Proximal Policy Optimization):**
  ä¸€ç§å¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œç”¨äº RLHF ä¸­å¾®è°ƒè¯­è¨€æ¨¡å‹ã€‚ (è§ 7.2.2)
* **Pre-training (é¢„è®­ç»ƒ):**
  åœ¨è¶…å¤§è§„æ¨¡ã€é€šå¸¸æ— æ ‡æ³¨çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ä»¥å­¦ä¹ é€šç”¨çŸ¥è¯†å’Œè¡¨ç¤ºçš„è¿‡ç¨‹ï¼Œæ˜¯æ„å»ºåŸºç¡€æ¨¡å‹çš„ç¬¬ä¸€æ­¥ã€‚ (è§ ç¬¬ 5 ç« )
* **Prefix Tuning:**
  ä¸€ç§ PEFT æ–¹æ³•ï¼Œé€šè¿‡åœ¨ Transformer æ¯ä¸€å±‚çš„ K å’Œ V å‰æ·»åŠ å¯è®­ç»ƒçš„å‰ç¼€å‘é‡æ¥å¼•å¯¼æ¨¡å‹ã€‚ (è§ 6.3.4)
* **Prompt:**
  æä¾›ç»™å¤§è¯­è¨€æ¨¡å‹çš„è¾“å…¥æ–‡æœ¬ï¼Œç”¨äºå¼•å¯¼å…¶ç”Ÿæˆæ‰€éœ€çš„è¾“å‡ºã€‚ (è§ ç¬¬ 8 ç« )
* **Prompt Engineering (æç¤ºå·¥ç¨‹):**
  è®¾è®¡ã€ä¼˜åŒ–å’Œè¿­ä»£ Prompt ä»¥æœ‰æ•ˆå¼•å¯¼å¤§æ¨¡å‹å®Œæˆä»»åŠ¡çš„è‰ºæœ¯å’Œç§‘å­¦ã€‚ (è§ ç¬¬ 8 ç« )
* **Prompt Tuning:**
  ä¸€ç§å‚æ•°æ•ˆç‡æé«˜çš„ PEFT æ–¹æ³•ï¼Œåªåœ¨è¾“å…¥åµŒå…¥å±‚æ·»åŠ å¯è®­ç»ƒçš„è¿ç»­æç¤ºå‘é‡ã€‚ (è§ 6.3.5)
* **Pruning (å‰ªæ):**
  ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œç§»é™¤æ¨¡å‹ä¸­å†—ä½™çš„å‚æ•°æˆ–è¿æ¥ã€‚ (è§ 12.2.1)
* **P-Tuning:**
  ä¸€ç§ PEFT æ–¹æ³•ï¼Œé€šè¿‡åœ¨è¾“å…¥å±‚ï¼ˆv1ï¼‰æˆ–æ¯ä¸€å±‚ï¼ˆv2ï¼‰æ·»åŠ å¯è®­ç»ƒçš„è™šæ‹Ÿ Token/Prompt å‘é‡æ¥å¼•å¯¼æ¨¡å‹ã€‚ (è§ 6.3.4)
* **PyTorch:**
  ä¸€ä¸ªæµè¡Œçš„å¼€æºæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚ (è§ é™„å½• B.2)
* **Q, K, V (Query, Key, Value):**
  è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œç”±è¾“å…¥å‘é‡é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°çš„ä¸‰ä¸ªå‘é‡ï¼Œåˆ†åˆ«ä»£è¡¨æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚ (è§ 3.3.1)
* **Quantization (é‡åŒ–):**
  ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œä½¿ç”¨ä½ä½å®½æ•´æ•°è¡¨ç¤ºæ¨¡å‹å‚æ•°å’Œ/æˆ–æ¿€æ´»å€¼ã€‚ (è§ 12.2.1)
* **RAG (Retrieval-Augmented Generation, æ£€ç´¢å¢å¼ºç”Ÿæˆ):**
  ä¸€ç§ç»“åˆä¿¡æ¯æ£€ç´¢å’Œè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æŠ€æœ¯ï¼Œè®©æ¨¡å‹èƒ½å¤ŸåŸºäºå¤–éƒ¨çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚ (è§ 10.2, ç¬¬ 11 ç« )
* **ReAct (Reason + Act):**
  ä¸€ç§ Agent æ¡†æ¶ï¼Œé€šè¿‡è®© LLM äº¤é”™ç”Ÿæˆâ€œæ€è€ƒâ€å’Œâ€œè¡ŒåŠ¨â€ï¼ˆè°ƒç”¨å·¥å…·ï¼‰æ­¥éª¤æ¥å®Œæˆéœ€è¦å¤–éƒ¨çŸ¥è¯†çš„ä»»åŠ¡ã€‚ (è§ 12.3.1)
* **Recall (å¬å›ç‡):**
  è¯„ä¼°æŒ‡æ ‡ï¼Œè¡¡é‡æ‰€æœ‰çœŸæ­£ä¾‹ä¸­è¢«æ­£ç¡®è¯†åˆ«å‡ºæ¥çš„æ¯”ä¾‹ã€‚ (è§ 9.2.3)
* **Recursive Character Text Splitting (é€’å½’å­—ç¬¦æ–‡æœ¬åˆ†å‰²):**
  ä¸€ç§æ–‡æ¡£åˆ‡å—ç­–ç•¥ï¼Œå°è¯•æŒ‰ä¸€ç»„åˆ†éš”ç¬¦é€’å½’åˆ‡åˆ†ï¼Œä»¥åœ¨æ§åˆ¶å—å¤§å°çš„åŒæ—¶ä¿æŒè¯­ä¹‰ã€‚ (è§ 11.2.2)
* **Regularization (æ­£åˆ™åŒ–):**
  ç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ï¼Œå¦‚ L1/L2 æ­£åˆ™åŒ–ã€Dropoutã€‚ (è§ 2.1.4)
* **Reinforcement Learning (RL, å¼ºåŒ–å­¦ä¹ ):**
  æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ã€æ¥æ”¶å¥–åŠ±æˆ–æƒ©ç½šæ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚ (è§ 7.2.2)
* **ReLU (Rectified Linear Unit):**
  ä¸€ç§å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œ`f(x) = max(0, x)`ã€‚ (è§ 2.1.3)
* **Residual Connection / Skip Connection (æ®‹å·®è¿æ¥ / è·³è·ƒè¿æ¥):**
  å°†æŸå±‚çš„è¾“å…¥ç›´æ¥åŠ åˆ°å…¶è¾“å‡ºä¸Šçš„è¿æ¥æ–¹å¼ï¼Œæœ‰åŠ©äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼Œè®­ç»ƒæ›´æ·±çš„ç½‘ç»œã€‚Transformer ä¸­å¹¿æ³›ä½¿ç”¨ã€‚ (è§ 3.5)
* **Retrieval (æ£€ç´¢):**
  åœ¨ RAG ä¸­ï¼Œæ ¹æ®ç”¨æˆ·æŸ¥è¯¢ä»æ–‡æ¡£åº“ä¸­æŸ¥æ‰¾æœ€ç›¸å…³ä¿¡æ¯çš„è¿‡ç¨‹ã€‚ (è§ 11.4)
* **Reward Model (RM, å¥–åŠ±æ¨¡å‹):**
  åœ¨ RLHF ä¸­ï¼Œç”¨äºé¢„æµ‹äººç±»å¯¹æ¨¡å‹ç”Ÿæˆå†…å®¹åå¥½ç¨‹åº¦çš„æ¨¡å‹ï¼Œå…¶è¾“å‡ºä½œä¸º RL çš„å¥–åŠ±ä¿¡å·ã€‚ (è§ 7.2.1)
* **RLHF (Reinforcement Learning from Human Feedback):**
  ä¸€ç§å¯¹é½æŠ€æœ¯ï¼Œä½¿ç”¨äººç±»åå¥½æ•°æ®è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç„¶åé€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚ PPOï¼‰ä¼˜åŒ–è¯­è¨€æ¨¡å‹ã€‚ (è§ 7.2)
* **RNN (Recurrent Neural Network, å¾ªç¯ç¥ç»ç½‘ç»œ):**
  ä¸€ç±»èƒ½å¤Ÿå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œé€šè¿‡å†…éƒ¨å¾ªç¯è¿æ¥ä¼ é€’ä¿¡æ¯ã€‚ (è§ 1.2.3)
* **RoBERTa (Robustly Optimized BERT Pretraining Approach):**
  BERT çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ï¼Œé€šè¿‡ä¼˜åŒ–é¢„è®­ç»ƒç­–ç•¥ï¼ˆå¦‚ä½¿ç”¨æ›´å¤§æ‰¹æ¬¡ã€æ›´å¤šæ•°æ®ã€åŠ¨æ€ MLM æ©ç ï¼‰æå‡äº†æ€§èƒ½ã€‚ (è§ 3.8.1)
* **Robustness (é²æ£’æ€§):**
  æ¨¡å‹åœ¨é¢å¯¹è¾“å…¥æ‰°åŠ¨æˆ–å™ªå£°æ—¶ä¿æŒæ€§èƒ½ç¨³å®šçš„èƒ½åŠ›ã€‚ (è§ 13.1.3)
* **RoPE (Rotary Positional Embedding, æ—‹è½¬ä½ç½®ç¼–ç ):**
  ä¸€ç§æ”¹è¿›çš„ä½ç½®ç¼–ç æ–¹æ³•ï¼Œå°†ä½ç½®ä¿¡æ¯ä¹˜æ€§åœ°èå…¥ Q å’Œ Kï¼Œåœ¨é•¿åºåˆ—å»ºæ¨¡ä¸Šè¡¨ç°è¾ƒå¥½ã€‚ (è§ 12.5)
* **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):**
  ç”¨äºè¯„ä¼°æ–‡æœ¬æ‘˜è¦è´¨é‡çš„æŒ‡æ ‡ï¼ŒåŸºäº n-gramã€LCS æˆ– Skip-gram çš„å¬å›ç‡ã€‚ (è§ 9.2.2)
* **Safety (å®‰å…¨æ€§):**
  æŒ‡ AI ç³»ç»Ÿä¸åº”äº§ç”Ÿæœ‰å®³ã€å±é™©æˆ–ä¸é“å¾·çš„è¾“å‡ºã€‚ (è§ 7.1, 13.2)
* **Scaling Laws (æ‰©å±•å®šå¾‹):**
  æè¿°æ¨¡å‹æ€§èƒ½ä¸å…¶è§„æ¨¡ï¼ˆå‚æ•°é‡ã€æ•°æ®é‡ã€è®¡ç®—é‡ï¼‰ä¹‹é—´å…³ç³»çš„ç»éªŒæ€§å®šå¾‹ï¼Œé€šå¸¸å‘ˆå¹‚å¾‹å…³ç³»ã€‚ (è§ 4.2.2)
* **Self-Attention (è‡ªæ³¨æ„åŠ›):**
  æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ç§å½¢å¼ï¼Œè®¡ç®—åºåˆ—å†…éƒ¨å„å…ƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œå…è®¸æ¨¡å‹åœ¨å¤„ç†æ¯ä¸ªå…ƒç´ æ—¶å…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–å…ƒç´ ã€‚Transformer çš„æ ¸å¿ƒã€‚ (è§ 3.3)
* **Self-Consistency (è‡ªæˆ‘ä¸€è‡´æ€§):**
  ä¸€ç§å¢å¼º CoT æ•ˆæœçš„æŠ€æœ¯ï¼Œé€šè¿‡å¤šæ¬¡é‡‡æ ·ç”Ÿæˆä¸åŒæ¨ç†è·¯å¾„ï¼Œå¹¶é€‰æ‹©æœ€ä¸€è‡´çš„ç­”æ¡ˆã€‚ (è§ 8.3.2)
* **Self-Instruct:**
  ä¸€ç§è‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤å¾®è°ƒæ•°æ®çš„æ–¹æ³•ï¼Œåˆ©ç”¨å¼ºå¤§çš„æ•™å¸ˆæ¨¡å‹ç”ŸæˆæŒ‡ä»¤ã€è¾“å…¥å’Œè¾“å‡ºã€‚ (è§ 6.4.2)
* **Self-supervised Learning (SSL, è‡ªç›‘ç£å­¦ä¹ ):**
  ä¸€ç§æœºå™¨å­¦ä¹ èŒƒå¼ï¼Œæ¨¡å‹ä»æ•°æ®æœ¬èº«ç”Ÿæˆçš„â€œä¼ªæ ‡ç­¾â€ä¸­å­¦ä¹ ï¼Œè€Œæ— éœ€äººå·¥æ ‡æ³¨ã€‚MLM å’Œ CLM æ˜¯å…¸å‹çš„è‡ªç›‘ç£ä»»åŠ¡ã€‚ (è§ 5.1)
* **Semantic Search (è¯­ä¹‰æœç´¢):**
  æ ¹æ®æŸ¥è¯¢çš„è¯­ä¹‰å«ä¹‰è€Œéå…³é”®è¯åŒ¹é…æ¥æœç´¢ç›¸å…³æ–‡æ¡£çš„æŠ€æœ¯ï¼Œé€šå¸¸åŸºäºå‘é‡åµŒå…¥ã€‚ (è§ 10.8.1)
* **Sentence Embedding (å¥å­åµŒå…¥):**
  å°†æ•´ä¸ªå¥å­æ˜ å°„ä¸ºä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡è¡¨ç¤ºã€‚
* **Sentence Transformer:**
  ä¸€ä¸ªæµè¡Œçš„ Python åº“ï¼Œæä¾›äº†å¤§é‡é¢„è®­ç»ƒæ¨¡å‹ç”¨äºç”Ÿæˆé«˜è´¨é‡çš„å¥å­å’Œæ®µè½åµŒå…¥ã€‚ (è§ 10.8, 11.3.1)
* **Sequence Mask / Future Mask / Look-ahead Mask (åºåˆ—æ©ç  / æœªæ¥æ©ç ):**
  ç”¨äº Transformer è§£ç å™¨è‡ªæ³¨æ„åŠ›å±‚çš„ä¸€ç§æ©ç ï¼Œç¡®ä¿åœ¨é¢„æµ‹å½“å‰ä½ç½®æ—¶åªèƒ½å…³æ³¨ä¹‹å‰çš„ä½ç½®ï¼Œä¸èƒ½â€œçœ‹åˆ°â€æœªæ¥ã€‚ (è§ 3.6.2)
* **Sequence-to-Sequence (Seq2Seq, åºåˆ—åˆ°åºåˆ—):**
  ä¸€ç±»ä»»åŠ¡ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºæ˜¯å¦ä¸€ä¸ªåºåˆ—ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€‚ (è§ 1.2.4, 3.7)
* **SGD (Stochastic Gradient Descent, éšæœºæ¢¯åº¦ä¸‹é™):**
  ä¸€ç§åŸºæœ¬çš„ä¼˜åŒ–ç®—æ³•ï¼Œä½¿ç”¨å°æ‰¹é‡æ•°æ®è®¡ç®—æ¢¯åº¦æ¥æ›´æ–°å‚æ•°ã€‚ (è§ 2.1.2)
* **Softmax:**
  å°†ä¸€ä¸ªåˆ†æ•°å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒçš„å‡½æ•°ã€‚ (è§ A.5)
* **Sparse Attention (ç¨€ç–æ³¨æ„åŠ›):**
  é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ç§ï¼Œé€šè¿‡é™åˆ¶æ¯ä¸ª Token åªå…³æ³¨éƒ¨åˆ†å…¶ä»– Token æ¥é™ä½è‡ªæ³¨æ„åŠ›çš„äºŒæ¬¡å¤æ‚åº¦ã€‚ (è§ 12.5)
* **Sparsity (ç¨€ç–æ€§):**
  æŒ‡æ¨¡å‹ä¸­å¤§éƒ¨åˆ†å‚æ•°ä¸ºé›¶çš„ç‰¹æ€§ï¼ˆå¦‚å‰ªæåçš„æ¨¡å‹ï¼‰æˆ–è®¡ç®—è¿‡ç¨‹ä¸­å¤§éƒ¨åˆ†å•å…ƒæœªè¢«æ¿€æ´»çš„ç‰¹æ€§ï¼ˆå¦‚ MoE æ¨¡å‹ï¼‰ã€‚
* **State Dict (çŠ¶æ€å­—å…¸):**
  åœ¨ PyTorch ä¸­ï¼ŒåŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°å’ŒæŒä¹…åŒ–ç¼“å†²åŒºçš„ Python å­—å…¸å¯¹è±¡ï¼Œç”¨äºä¿å­˜å’ŒåŠ è½½æ¨¡å‹ã€‚ (è§ B.2.3)
* **Streaming (æµå¼å¤„ç†):**
  å¤„ç†æ— æ³•å®Œå…¨åŠ è½½åˆ°å†…å­˜çš„è¶…å¤§æ•°æ®é›†æ—¶ï¼Œé€æ‰¹åŠ è½½å’Œå¤„ç†æ•°æ®çš„æ–¹å¼ã€‚ (è§ 5.3.1)
* **Subword Tokenization (å­è¯åˆ†è¯):**
  ä¸€ç§åˆ†è¯ç­–ç•¥ï¼Œå°†è¯è¯­åˆ‡åˆ†æˆæ›´å°çš„ã€æœ‰æ„ä¹‰çš„å­è¯å•å…ƒï¼Œä»¥å¤„ç† OOV é—®é¢˜å¹¶æ§åˆ¶è¯æ±‡è¡¨å¤§å°ã€‚BPE, WordPiece, SentencePiece æ˜¯å¸¸ç”¨ç®—æ³•ã€‚ (è§ 2.2.2)
* **Supervised Fine-tuning (SFT, ç›‘ç£å¾®è°ƒ):**
  ä½¿ç”¨å¸¦æœ‰æ˜ç¡®æ ‡ç­¾çš„æ ‡æ³¨æ•°æ®å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒçš„è¿‡ç¨‹ã€‚æŒ‡ä»¤å¾®è°ƒæ˜¯ SFT çš„ä¸€ç§å½¢å¼ã€‚
* **Supervised Learning (ç›‘ç£å­¦ä¹ ):**
  æœºå™¨å­¦ä¹ çš„ä¸€ç§èŒƒå¼ï¼Œæ¨¡å‹ä»å¸¦æœ‰æ ‡ç­¾ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰çš„æ•°æ®ä¸­å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚
* **T5 (Text-to-Text Transfer Transformer):**
  ä¸€ç§åŸºäº Transformer Encoder-Decoder æ¶æ„çš„æ¨¡å‹ï¼Œå°†æ‰€æœ‰ NLP ä»»åŠ¡ç»Ÿä¸€ä¸ºæ–‡æœ¬åˆ°æ–‡æœ¬æ ¼å¼ï¼Œå¹¶é€šè¿‡æ–‡æœ¬ç‰‡æ®µæŸåä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚ (è§ 3.8.3, 5.2.3)
* **Tensor (å¼ é‡):**
  è§ A.1, B.2.1ã€‚
* **Tensor Parallelism (TP, å¼ é‡å¹¶è¡Œ):**
  ä¸€ç§æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œå°†å•å±‚å†…éƒ¨çš„è®¡ç®—ï¼ˆå¦‚æƒé‡çŸ©é˜µï¼‰åˆ†å‰²åˆ°å¤šä¸ªè®¾å¤‡ä¸Šã€‚ (è§ 4.4.2)
* **Token:**
  æ–‡æœ¬ç»è¿‡åˆ†è¯å™¨å¤„ç†åå¾—åˆ°çš„åŸºæœ¬å•å…ƒï¼ˆå¯èƒ½æ˜¯è¯ã€å­è¯æˆ–å­—ç¬¦ï¼‰ã€‚ (è§ 2.2.2)
* **Tokenization (åˆ†è¯):**
  å°†åŸå§‹æ–‡æœ¬å­—ç¬¦ä¸²åˆ‡åˆ†æˆ Token åºåˆ—çš„è¿‡ç¨‹ã€‚ (è§ 2.2.2)
* **Tokenizer (åˆ†è¯å™¨):**
  æ‰§è¡Œåˆ†è¯æ“ä½œçš„å·¥å…·æˆ–ç®—æ³•ã€‚
* **TPU (Tensor Processing Unit):**
  Google å¼€å‘çš„ä¸“ç”¨äºåŠ é€Ÿå¼ é‡è¿ç®—çš„ ASIC èŠ¯ç‰‡ã€‚ (è§ 4.3.1)
* **Trainer (Hugging Face):**
  `transformers` åº“ä¸­æä¾›çš„ç”¨äºç®€åŒ–æ¨¡å‹è®­ç»ƒå’Œå¾®è°ƒçš„ç±»ã€‚ (è§ é™„å½• C.2)
* **Transformer:**
  ä¸€ç§å®Œå…¨åŸºäºæ³¨æ„åŠ›æœºåˆ¶ï¼ˆç‰¹åˆ«æ˜¯è‡ªæ³¨æ„åŠ›ï¼‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ¶æ„ï¼Œæ˜¯ç°ä»£å¤§æ¨¡å‹çš„åŸºç¡€ã€‚ (è§ ç¬¬ 3 ç« )
* **Transfer Learning (è¿ç§»å­¦ä¹ ):**
  å°†åœ¨ä¸€ä¸ªä»»åŠ¡ï¼ˆæºä»»åŠ¡ï¼Œå¦‚é¢„è®­ç»ƒï¼‰ä¸Šå­¦åˆ°çš„çŸ¥è¯†åº”ç”¨äºå¦ä¸€ä¸ªç›¸å…³ä»»åŠ¡ï¼ˆç›®æ ‡ä»»åŠ¡ï¼Œå¦‚å¾®è°ƒï¼‰çš„æœºå™¨å­¦ä¹ èŒƒå¼ã€‚
* **TRL (Transformer Reinforcement Learning):**
  Hugging Face å¼€å‘çš„ç”¨äºç®€åŒ–ä½¿ç”¨ RL å¾®è°ƒ Transformer æ¨¡å‹çš„åº“ã€‚ (è§ é™„å½• C.7)
* **Unsupervised Learning (æ— ç›‘ç£å­¦ä¹ ):**
  æœºå™¨å­¦ä¹ çš„ä¸€ç§èŒƒå¼ï¼Œæ¨¡å‹ä»æœªæ ‡æ³¨çš„æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼å’Œç»“æ„ã€‚é¢„è®­ç»ƒé€šå¸¸å±äºè‡ªç›‘ç£å­¦ä¹ ï¼ˆæ— ç›‘ç£çš„ä¸€ç§å½¢å¼ï¼‰ã€‚
* **Validation Set (éªŒè¯é›†):**
  åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç”¨äºç›‘æ§æ¨¡å‹æ€§èƒ½ã€è°ƒæ•´è¶…å‚æ•°å’Œé€‰æ‹©æœ€ä½³æ¨¡å‹æ£€æŸ¥ç‚¹çš„æ•°æ®é›†ï¼Œæ¨¡å‹ä¸ç›´æ¥åœ¨è¯¥æ•°æ®é›†ä¸Šè¿›è¡Œæ¢¯åº¦æ›´æ–°ã€‚
* **Vector Database (å‘é‡æ•°æ®åº“):**
  ä¸“é—¨ç”¨äºå­˜å‚¨å’Œé«˜æ•ˆæ£€ç´¢é«˜ç»´å‘é‡ï¼ˆå¦‚åµŒå…¥ï¼‰çš„æ•°æ®åº“ç³»ç»Ÿã€‚ (è§ 10.8.1, 11.3.2)
* **ViT (Vision Transformer):**
  å°† Transformer æ¶æ„æˆåŠŸåº”ç”¨äºè®¡ç®—æœºè§†è§‰ï¼ˆå›¾åƒåˆ†ç±»ï¼‰çš„æ¨¡å‹ã€‚ (è§ 12.1.2)
* **VLM (Vision-Language Model):**
  èƒ½å¤Ÿå¤„ç†å›¾åƒå’Œæ–‡æœ¬ä¸¤ç§æ¨¡æ€çš„å¤šæ¨¡æ€æ¨¡å‹ã€‚ (è§ 12.1)
* **Warmup (é¢„çƒ­):**
  å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥çš„ä¸€éƒ¨åˆ†ï¼Œåœ¨è®­ç»ƒåˆæœŸå°†å­¦ä¹ ç‡ä»ä¸€ä¸ªå¾ˆå°çš„å€¼é€æ¸å¢åŠ åˆ°å³°å€¼ã€‚ (è§ 5.3.3)
* **Weight (æƒé‡):**
  ç¥ç»ç½‘ç»œè¿æ¥ä¸Šçš„å¯å­¦ä¹ å‚æ•° `W`ï¼Œè¡¨ç¤ºè¿æ¥å¼ºåº¦ã€‚
* **Weight Decay (æƒé‡è¡°å‡):**
  ä¸€ç§ L2 æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­å¯¹è¾ƒå¤§çš„æƒé‡è¿›è¡Œæƒ©ç½šï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚ (è§ 2.1.4, 5.3.3)
* **Word Embedding (è¯åµŒå…¥):**
  å°†è¯è¯­æ˜ å°„ä¸ºä½ç»´ç¨ å¯†å‘é‡çš„è¡¨ç¤ºæ–¹æ³•ï¼ˆå¦‚ Word2Vec, GloVeï¼‰ã€‚ (è§ 1.2.2)
* **WordPiece:**
  ä¸€ç§å­è¯åˆ†è¯ç®—æ³•ï¼ŒåŸºäºæœ€å¤§åŒ–æ•°æ®ä¼¼ç„¶åº¦æ¥åˆå¹¶å­—ç¬¦æˆ–å­è¯ã€‚BERT ä½¿ç”¨ã€‚ (è§ 2.2.2)
* **Zero-shot Learning (é›¶æ ·æœ¬å­¦ä¹ ):**
  æ¨¡å‹åœ¨æ²¡æœ‰çœ‹åˆ°ä»»ä½•ç‰¹å®šä»»åŠ¡ç¤ºä¾‹çš„æƒ…å†µä¸‹ï¼Œä»…å‡­ä»»åŠ¡æè¿°ï¼ˆé€šè¿‡ Promptï¼‰å°±èƒ½æ‰§è¡Œè¯¥ä»»åŠ¡çš„èƒ½åŠ›ã€‚ (è§ 1.1, 8.2.1)
* **ZeRO (Zero Redundancy Optimizer):**
  ä¸€ç§åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–æŠ€æœ¯ï¼ˆç”± DeepSpeed æå‡ºï¼‰ï¼Œé€šè¿‡åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ã€æ¢¯åº¦å’Œå‚æ•°æ¥æ˜¾è‘—å‡å°‘å†…å­˜å†—ä½™ã€‚ (è§ 4.4.5)

è¿™ä¸ªæœ¯è¯­è¡¨å¸Œæœ›èƒ½å¸®åŠ©ä½ å·©å›ºå¯¹å¤§æ¨¡å‹é¢†åŸŸæ ¸å¿ƒæ¦‚å¿µçš„ç†è§£ã€‚
