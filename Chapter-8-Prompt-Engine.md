**第五部分：使用与评估大模型**

# 第8章：提示工程：与大模型高效交互的艺术

经过前面章节的学习，我们已经了解了大模型的内部构造（Transformer）、如何通过预训练赋予其通用知识（Pre-training）、以及如何通过微调和对齐使其更适应特定任务并符合人类期望（Fine-tuning & Alignment）。现在，我们拥有了一个强大的工具。但是，如何才能高效地运用这个工具，让它准确理解我们的意图并产生期望的输出呢？这就是**提示工程 (Prompt Engineering)** 发挥作用的地方。

对于现代大语言模型（尤其是 Decoder-Only 架构如 GPT 系列），**提示 (Prompt)** 是我们与模型交互的主要（有时是唯一）接口。提示是我们提供给模型的输入文本，它包含了任务的描述、相关的背景信息、期望的输出格式，甚至是一些示例。模型会基于这个提示，利用其在预训练和微调（如果进行过）阶段学到的知识和模式，来预测接下来最可能的 Token 序列，从而生成响应。

可以说，提示就是我们用来“编程”或“引导”大模型的自然语言指令。设计出好的提示，往往能极大地释放模型的潜力，让模型在没有经过特定任务微调的情况下也能完成复杂的任务；而设计不佳的提示则可能导致模型输出混乱、不相关甚至错误的内容。因此，提示工程已经发展成为一门重要的技能，甚至可以说是一门“艺术”。

本章将深入探讨提示工程的核心概念、基本技巧和高级策略。我们将学习如何从零样本、少样本提示开始，逐步掌握思维链、角色扮演等更复杂的技巧，并讨论如何设计、迭代和评估提示的效果。理解提示工程，将使你能够更有效地与大模型协作，充分发挥它们的能力。

**8.1 理解提示（Prompt）的作用机制**

要理解提示工程，首先要回顾一下大语言模型（特别是 Decoder-Only 自回归模型）的工作原理：**预测下一个 Token**。

当模型接收到一个提示时，它会将提示文本编码成 Token ID 序列。然后，模型内部的 Transformer 网络（主要是带掩码的自注意力层和前馈网络层）会处理这个序列，计算出在给定提示序列条件下，词汇表中每个 Token 作为**下一个** Token 出现的概率分布。模型随后会根据这个概率分布选择一个 Token（通过贪心解码、束搜索、或者更常用的采样方法如 Top-k/Top-p 采样）添加到序列末尾。这个新生成的 Token 会成为下一步预测的输入的一部分，模型继续预测再下一个 Token，如此循环往复，直到生成一个结束符 `[EOS]` 或达到预设的最大长度。

**提示的作用就在于为这个“下一个 Token 预测”过程提供初始的、强有力的上下文引导 (Contextual Guidance)。** 提示中的每一个词、每一个标点、每一个换行，都在影响模型内部的注意力分布和最终的概率输出。一个好的提示能够：

1. **激活相关知识:** 提示中的关键词和概念能够“唤醒”模型在预训练时学到的相关信息。
2. **明确任务意图:** 清晰的指令让模型知道你希望它做什么（翻译、总结、分类、生成代码等）。
3. **塑造输出格式:** 通过示例或明确说明，引导模型按照特定的结构或格式（如 JSON、列表、Markdown）生成响应。
4. **建立上下文学习模式:** 在少样本提示中，提供的示例向模型展示了输入和期望输出之间的映射关系，模型会尝试模仿这种模式。
5. **约束生成空间:** 提示可以隐式或显式地限制模型生成内容的主题、风格或范围。

可以把提示想象成给一个非常聪明但需要明确引导的学生布置作业：作业要求越清晰、背景知识给得越足、范例越明确，学生完成作业的质量就越高。

**8.2 基本提示技巧**

掌握一些基本的提示技巧是进行有效交互的起点。

**8.2.1 零样本（Zero-shot）提示**

这是最简单的提示方式，直接向模型描述任务，不提供任何完成任务的示例。模型需要完全依赖其预训练时学到的知识和指令遵循能力来完成任务。

**特点:**

* 简单直接。
* 测试模型真正的通用理解和泛化能力。
* 对于模型没有明确训练过的、或者需要特定格式的任务，效果可能不佳。

**案例:**

* **任务: 情感分类**
  * 提示: `"请判断以下句子的情感是积极、消极还是中性： '这部电影真是太棒了！' 情感："`
  * 期望模型输出: `"积极"`
* **任务: 翻译**
  * 提示: `"将以下英文翻译成西班牙语： 'Hello, how are you?' 西班牙语："`
  * 期望模型输出: `"Hola, ¿cómo estás?"`
* **任务: 简单问答**
  * 提示: `"法国的首都是哪里？ 答案："`
  * 期望模型输出: `"巴黎"`

**要点:**

* **指令清晰:** 使用明确的动词（如“判断”、“翻译”、“总结”、“生成”）。
* **界定输入输出:** 明确指出哪部分是输入，并提示模型开始生成输出（如使用 "情感："、"西班牙语："、"答案：" 或换行）。

零样本提示的效果很大程度上取决于基础模型的规模和指令微调的程度。对于经过良好指令微调的大模型，零样本提示在很多常见任务上已经能取得不错的效果。

**8.2.2 少样本（Few-shot）提示 / 上下文学习（In-context Learning, ICL）**

当零样本提示效果不佳时，可以在提示中**提供几个完整的任务示例 (Demonstrations / Examples)**，让模型在“上下文中学习 (In-context Learning, ICL)”任务的模式。模型并不会在这些示例上更新参数（不是微调），而是将这些示例作为当前预测的上下文信息，模仿示例的格式和逻辑来处理最后那个需要它完成的新实例。

**特点:**

* 显著提升模型在特定任务上的表现，特别是对于需要特定格式或模型不太熟悉的新任务。
* 效果很大程度上依赖于**示例的质量、数量和顺序**。
* 仍然受限于模型的上下文窗口大小。

**案例:**

* **任务: 将非正式句子转为正式句子 (1-shot 示例)**
  * 提示:
    ```text
    将句子改写得更正式。

    例子：
    非正式: 这东西真没用。
    正式: 该物品未能展现出预期的实用性。

    任务：
    非正式: 老板说我们得快点搞定。
    正式:
    ```
  * 期望模型输出: `"经理指示我们需要尽快完成该项工作。"` (或其他类似的正式表达)
* **任务: 为新产品生成营销口号 (2-shot 示例)**
  * 提示:
    ```text
    为以下产品类别生成一句吸引人的营销口号。

    产品: 智能手表
    口号: 掌控时间，智联未来。

    产品: 有机咖啡
    口号: 品味纯粹，唤醒活力。

    产品: 降噪耳机
    口号:
    ```
  * 期望模型输出: `"静享世界，沉浸纯音。"` (或其他相关的口号)
* **任务: 简单的代码生成 (根据注释生成代码, 1-shot)**
  * 提示:
    ```python
    # Python 代码

    # 示例：
    # 函数：计算两个数的和
    def add(a, b):
      """返回 a 和 b 的和"""
      return a + b

    # 任务：
    # 函数：计算一个数的平方
    def square(n):
    ```
  * 期望模型输出:
    ```python
      """返回 n 的平方"""
      return n * n
    ```

**要点:**

* **示例格式一致:** 确保所有示例的格式（包括标签、换行等）与最终需要模型完成的任务实例格式保持一致。
* **示例相关且清晰:** 选择与目标任务高度相关、且输入输出关系明确的示例。
* **示例数量:** 通常 1 到 5 个示例（1-shot 到 5-shot）就足够。过多示例可能超出上下文长度，或引入噪声。有时甚至一个精心挑选的示例（1-shot）效果就很好。
* **示例顺序:** 示例的排列顺序有时也会影响性能，可以尝试不同的顺序。
* **分隔清晰:** 使用明确的分隔符（如换行、特定标记）区分不同的示例以及示例和最终任务。

少样本提示是利用大模型强大模式匹配能力的一种非常有效的技巧。

**8.2.3 清晰明确的指令 (Clear and Specific Instructions)**

无论采用零样本还是少样本，指令本身的清晰度和明确性都至关重要。模糊、歧义或过于复杂的指令容易让模型产生误解。

**原则:**

* **使用明确的动词:** 避免使用模糊的词语，如“处理”、“分析”，使用更具体的词，如“分类”、“总结”、“提取”、“翻译”、“改写”、“生成列表”。
* **指定角色 (Persona):** 如果希望模型扮演特定角色（如“你是一位经验丰富的 Python 程序员”，“假设你是一位莎士比亚戏剧评论家”），在提示开头明确指出。
* **明确输出格式:** 如果需要特定的输出格式（如 JSON、Markdown 列表、特定风格），在指令中明确说明或通过示例展示。
* **提供必要的上下文:** 如果任务需要背景知识，简要地在提示中提供。
* **分解复杂任务:** 如果任务包含多个步骤，可以考虑将提示分解成多个更简单的子任务提示，或者使用后面介绍的思维链方法。
* **使用分隔符:** 使用 `###`、`---`、引号、XML 标签（如 `<input>`, `<output>`）等清晰地分隔指令、上下文、示例和输入。

**反例 (模糊指令):** "告诉我关于法国的事情。"
**良例 (明确指令):** "请总结法国的历史，重点关注其在 18 世纪的关键事件，并以三个要点的形式列出。"

**8.3 高级提示策略**

掌握了基础技巧后，可以探索一些更高级的策略来应对更复杂的任务，特别是那些需要推理或精确控制的任务。

**8.3.1 思维链（Chain-of-Thought, CoT）提示**

思维链提示 (Wei et al., 2022) 是一种旨在**提升大模型在复杂推理任务（如数学问题、常识推理、逻辑谜题）上表现**的强大技术。其核心思想是：**在提示的示例中，不仅给出最终答案，还要展示得出答案的中间推理步骤（思考过程）。**

**原理:**

* 标准少样本提示只展示 (输入, 输出) 对，模型需要直接从输入跳到输出。
* CoT 提示展示 (输入, **推理步骤**, 输出) 对。通过模仿这些推理步骤，模型在处理新问题时，也被引导着先生成一步步的思考过程，然后再给出最终答案。
* 这种显式的、分解的推理过程似乎更符合大模型的自回归生成机制，使其能够将复杂问题分解为更容易处理的中间步骤，从而提高最终答案的准确性。

**案例：解决数学应用题 (CoT 少样本)**

* **标准少样本提示:**

  ```text
  问：罗杰有 5 个网球。他又买了 2 罐网球。每罐有 3 个网球。他现在有多少个网球？
  答：11

  问：自助餐厅有 23 个苹果。如果他们用了 20 个做午餐，又买了 6 个，他们现在有多少个苹果？
  答：
  ```

  * 模型可能直接输出: `9` (错误)
* **思维链 (CoT) 少样本提示:**

  ```text
  问：罗杰有 5 个网球。他又买了 2 罐网球。每罐有 3 个网球。他现在有多少个网球？
  答：罗杰开始时有 5 个球。2 罐网球，每罐 3 个，所以他买了 2 * 3 = 6 个网球。他现在总共有 5 + 6 = 11 个网球。最终答案是 11。

  问：自助餐厅有 23 个苹果。如果他们用了 20 个做午餐，又买了 6 个，他们现在有多少个苹果？
  答：
  ```

  * 期望模型输出 (模仿 CoT 格式):
    `自助餐厅开始时有 23 个苹果。他们用了 20 个，所以剩下 23 - 20 = 3 个。他们又买了 6 个，所以现在有 3 + 6 = 9 个苹果。最终答案是 9。` (即使最终答案仍可能错，但推理过程更清晰，更容易诊断问题)

**零样本 CoT (Zero-shot CoT):** (Kojima et al., 2022)

有趣的是，研究发现，即使不提供包含推理步骤的示例，只需在提示中加入一句简单的“**让我们一步一步地思考 (Let's think step by step)**”，也能显著提升大模型在推理任务上的表现。模型似乎能理解这个元指令，并自发地生成中间步骤。

* **零样本 CoT 提示:**

  ```text
  问：自助餐厅有 23 个苹果。如果他们用了 20 个做午餐，又买了 6 个，他们现在有多少个苹果？

  答：让我们一步一步地思考。
  ```

  * 期望模型输出 (自发生成步骤):
    `首先，餐厅有 23 个苹果。然后他们用了 20 个，剩下 23 - 20 = 3 个苹果。接着他们又买了 6 个苹果。所以现在总共有 3 + 6 = 9 个苹果。最终答案是 9。`

**要点:**

* CoT 对模型规模非常敏感，通常在非常大的模型（如 >100B 参数）上效果才显著。
* 推理步骤的格式和详细程度会影响效果。
* 适用于需要多步逻辑、计算或规划的任务。

**8.3.2 自我一致性（Self-Consistency）**

自我一致性 (Wang et al., 2022) 是一种**增强 CoT 效果**的技术，尤其适用于答案唯一的推理任务（如数学题、选择题）。

* **原理:** 与 CoT 生成单一的推理路径不同，自我一致性通过**多次采样**（使用带有随机性的解码策略，如设置 `temperature > 0`）来生成**多个不同的推理路径 (Multiple Chains-of-Thought)**。然后，观察这些不同路径最终得出的答案，选择**最一致的答案**（即出现次数最多的那个答案）作为最终输出。
* **直觉:** 对于复杂问题，可能有多条不同的推理路径都能得到正确答案。如果一个答案能通过多种不同的方式被推导出来，那么它很可能是正确的。这种方法利用了“条条大路通罗马”的思想，通过聚合多个独立思考过程来提高结果的鲁棒性。

**流程:**

1. 使用 CoT 提示（零样本或少样本）。
2. 设置采样解码参数（如 `temperature=0.7`, `top_p=0.9`）。
3. 对同一个问题，**独立生成 K 次** 完整的推理路径和答案（例如 K=10 或 40）。
4. 提取每次生成的最终答案。
5. 统计所有 K 个答案的频率。
6. 选择**出现频率最高的答案**作为最终输出。

**优点:** 显著提高 CoT 在算术、常识和符号推理任务上的性能。
**缺点:** 需要进行多次生成，计算成本是原来的 K 倍。

**8.3.3 角色扮演提示 (Role-Playing Prompts)**

通过赋予模型一个特定的**角色 (Persona)**，可以引导其生成特定风格、口吻或具备特定领域知识的回答。

**案例:**

* **扮演程序员:**
  * 提示: `"你是一位拥有 10 年经验的资深 Python 工程师，擅长编写简洁、高效且符合 PEP8 规范的代码。请帮我优化以下 Python 函数，使其更具可读性和效率：\n\n[代码片段]\n\n优化后的代码："`
* **扮演历史学家:**
  * 提示: `"假设你是一位专门研究古埃及历史的历史学家。请用通俗易懂的语言，解释一下图坦卡蒙法老在历史上的重要性及其陵墓被发现的意义。"`
* **扮演创意作家:**
  * 提示: `"你是一位想象力丰富的科幻小说作家。请根据以下设定，撰写一个关于人类首次接触外星智慧生命的短篇故事的开头：设定：时间是 2242 年，地点是木星的卫星欧罗巴冰层下的海洋。"`

**要点:**

* 在提示开头清晰地描述角色身份、专业领域、性格特点或说话风格。
* 角色设定越具体，引导效果可能越好。

**8.3.4 控制输出格式（JSON, Markdown等）**

在很多应用场景中，我们需要模型以机器可读的格式（如 JSON）或特定标记语言（如 Markdown）输出结果。可以通过指令和/或示例来强制模型遵循格式要求。

**案例:**

* **输出 JSON:**
  * 提示:
    ```text
    请从以下文本中提取关键信息，并以 JSON 格式输出。JSON 应包含 "姓名", "职位", "公司", 和 "邮箱地址" 字段。如果信息不存在，请将对应值设为 null。

    文本：
    "会议记录显示，张三（项目经理，未来科技公司，邮箱 zhangsan@example.com）介绍了项目进展。李四（工程师，未来科技公司）提出了技术问题。"

    JSON 输出:
    ```
  * 期望模型输出:
    ```json
    [
      {
        "姓名": "张三",
        "职位": "项目经理",
        "公司": "未来科技公司",
        "邮箱地址": "zhangsan@example.com"
      },
      {
        "姓名": "李四",
        "职位": "工程师",
        "公司": "未来科技公司",
        "邮箱地址": null
      }
    ]
    ```
* **输出 Markdown 列表:**
  * 提示: `"请列出学习大语言模型的三个主要挑战，使用 Markdown 无序列表格式输出。"`
  * 期望模型输出:
    ```markdown
    *   **高昂的计算成本和资源需求:** 训练和运行大型模型需要大量的 GPU/TPU 算力和能源。
    *   **数据质量和偏见问题:** 海量训练数据可能包含噪声、错误信息和有害偏见，影响模型性能和安全性。
    *   **模型的可解释性和可控性:** 理解大模型决策过程和精确控制其行为仍然是挑战。
    ```

**要点:**

* 在指令中明确要求输出格式（"以 JSON 格式输出", "使用 Markdown 列表"）。
* 如果格式复杂，最好提供一个**完整格式的少样本示例**。
* 对于 JSON，有时需要强调转义特殊字符或确保有效的 JSON 结构。

**8.4 提示的设计、迭代与评估**

设计出完美的提示往往不是一蹴而就的，它通常是一个**迭代优化**的过程。

**8.4.1 如何构建好的提示？**

1. **明确目标:** 首先清晰地定义你希望模型完成什么任务，达到什么效果。
2. **选择合适的技巧:** 根据任务复杂度和模型能力，选择零样本、少样本、CoT 或其他策略。
3. **从简单开始:** 先尝试简单的零样本提示，如果效果不好，再逐步增加复杂性（如添加示例、CoT、角色设定）。
4. **指令要清晰简洁:** 避免歧义和冗余信息。
5. **提供上下文和约束:** 给出必要的背景信息，明确格式、风格、长度等要求。
6. **利用分隔符:** 清晰地组织提示结构。
7. **考虑模型的“知识边界”:** 避免问模型不知道或不擅长的问题（除非目标是测试其“诚实性”）。

**8.4.2 A/B测试与效果评估**

当你对同一个任务设计了多个不同的提示版本时，如何知道哪个更好？

* **定性评估:**
  * 直接观察模型对不同提示生成的输出。
  * 判断哪个版本的输出更准确、更相关、更流畅、更符合要求。
  * 注意模型是否产生了幻觉、偏见或其他不良内容。
* **定量评估 (如果可能):**
  * **自动化指标:** 如果任务有明确的评估指标（如分类任务的准确率/F1，摘要任务的 ROUGE 分数），可以在一个小的测试集上运行不同提示，比较指标得分。
  * **人工评估 (A/B 测试):** 准备一组测试用例，让模型分别使用提示 A 和提示 B 生成结果。然后让**人类评估者**对两组结果进行**盲评 (Blind Evaluation)**（不知道哪个结果来自哪个提示），判断哪个更好，或者进行打分。收集足够多的评估结果进行统计分析。
  * **用户反馈:** 如果应用已经上线，收集真实用户的反馈（如点赞/点踩、用户报告）也是评估提示效果的重要途径。

**迭代优化:**

根据评估结果，不断调整和改进提示：

* 修改指令措辞。
* 更换或调整少样本示例。
* 改变提示结构或分隔符。
* 尝试不同的高级技巧（CoT, 角色扮演等）。
* 调整解码参数（temperature, top_p, top_k）也可能影响输出质量。

提示工程是一个需要耐心、细致和不断实验的过程。

**8.5 自动化提示工程简介**

手动设计和优化提示既耗时又依赖经验。研究界也在探索自动化提示工程的方法：

* **Prompt Tuning (Revisited):** 从 PEFT 的角度看，Prompt Tuning 可以被视为一种**自动化学习“软提示”**的方法。模型通过梯度下降自动学习最优的连续提示向量，而不是由人手动设计离散的文本提示。
* **Instruction Prompt Tuning:** 结合指令微调和 Prompt Tuning，只训练添加到输入指令前的软提示向量。
* **Automatic Prompt Engineer (APE):** (Zhou et al., 2022) 提出一种框架，让一个大型 LLM **自动搜索**能够最好地引导另一个（或其自身）LLM 完成任务的**离散文本提示**。它通过生成候选指令、使用 LLM 评估指令效果、并基于评估结果进行迭代搜索来找到最优指令。
* **基于梯度的提示搜索:** 尝试使用梯度下降等优化方法直接在离散的 Token 空间中搜索最优提示，但这非常具有挑战性。

自动化提示工程是一个活跃的研究领域，目标是降低提示设计的门槛，并可能找到超越人类设计的更优提示。

**8.6 本章小结：提示是释放模型能力的关键**

本章我们深入探讨了与大模型高效交互的关键——提示工程。

* **提示机制:** 提示通过提供上下文，引导大模型的**下一个 Token 预测**过程，从而塑造其输出。
* **基本技巧:**
  * **零样本提示:** 直接描述任务。
  * **少样本提示 (ICL):** 提供示例让模型模仿。
  * **清晰指令:** 明确任务、角色、格式、上下文。
* **高级策略:**
  * **思维链 (CoT):** 通过展示或引导推理步骤提升复杂任务性能。
  * **自我一致性:** 通过多次采样和投票增强 CoT 的鲁棒性。
  * **角色扮演:** 引导特定风格或知识。
  * **格式控制:** 强制输出 JSON、Markdown 等。
* **设计与迭代:** 提示工程是一个迭代过程，需要明确目标、从简到繁、并通过定性/定量评估（A/B测试）不断优化。
* **自动化提示:** Prompt Tuning、APE 等方法试图自动化提示的设计过程。

掌握提示工程，就像学会了如何与一位才华横溢但需要精确指令的伙伴有效沟通。它是充分利用大模型能力、将其应用于实际问题的必备技能。

在了解了如何“使用”大模型之后，我们还需要知道如何科学地“衡量”它们。下一章 **第9章：评估大模型：衡量能力与局限** 将探讨评估这些复杂系统的挑战、方法和常用基准。

---
